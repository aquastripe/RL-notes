<!doctype html><html lang=en class=color-toggle-hidden><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=description content="A k-armed Bandit Problem Action-value Methods Sample-average method Greedy $\varepsilon$-greedy The 10-armed Testbed Incremental Implementation Tracking a Nonstationary Problem 可變長度的步長 (step-size) Optimistic Initial Value Upper-Confidence-Bound Action Selection Gradient Bandit Algorithms Associative Search (Contextual Bandits) Summary A k-armed Bandit Problem k 臂吃角子老虎機問題：你需要從 k 個不同 選項 或是 動作"><title>Multi-Armed Bandits | Reinforcement Learning Notes</title><link rel=icon href=/RL-notes/favicon/favicon-32x32.png type=image/x-icon><script src=/RL-notes/js/darkmode-ce906ea916.min.js></script><link rel=preload as=font href=/RL-notes/fonts/Metropolis.woff2 type=font/woff2 crossorigin=anonymous><link rel=preload as=font href=/RL-notes/fonts/LiberationSans.woff2 type=font/woff2 crossorigin=anonymous><link rel=preload href=/RL-notes/main-5caf214477.min.css as=style><link rel=stylesheet href=/RL-notes/main-5caf214477.min.css media=all><link rel=preload href=/RL-notes/mobile-249b801e7d.min.css as=style><link rel=stylesheet href=/RL-notes/mobile-249b801e7d.min.css media="screen and (max-width: 45rem)"><link rel=preload href=/RL-notes/print-f79fc3e5d7.min.css as=style><link rel=stylesheet href=/RL-notes/print-f79fc3e5d7.min.css media=print><link rel=preload href=/RL-notes/custom.css as=style><link rel=stylesheet href=/RL-notes/custom.css media=all><link rel=alternate type=application/rss+xml href=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/index.xml title="Reinforcement Learning Notes"></head><body itemscope itemtype=https://schema.org/WebPage><svg class="svg-sprite" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_back" xmlns="http://www.w3.org/2000/svg"><path d="M24 10.526v2.947H5.755l8.351 8.421-2.105 2.105-12-12 12-12 2.105 2.105-8.351 8.421H24z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_left_alt" xmlns="http://www.w3.org/2000/svg"><path d="M5.965 10.526V6.035L0 12l5.965 5.965v-4.491H24v-2.947H5.965z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_right_alt" xmlns="http://www.w3.org/2000/svg"><path d="M18.035 10.526V6.035L24 12l-5.965 5.965v-4.491H0v-2.947h18.035z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_bitbucket" xmlns="http://www.w3.org/2000/svg"><path d="M15.905 13.355c.189 1.444-1.564 2.578-2.784 1.839-1.375-.602-1.375-2.784-.034-3.403 1.151-.705 2.818.223 2.818 1.564zm1.907-.361c-.309-2.44-3.076-4.056-5.328-3.042-1.426.636-2.389 2.148-2.32 3.747.086 2.097 2.08 3.815 4.176 3.626s3.729-2.234 3.472-4.331zm4.108-9.315c-.756-.997-2.045-1.169-3.179-1.358-3.214-.516-6.513-.533-9.727.034-1.066.172-2.269.361-2.939 1.323 1.1 1.031 2.664 1.186 4.073 1.358 2.544.327 5.156.344 7.699.017 1.426-.172 3.008-.309 4.073-1.375zm.979 17.788c-.481 1.684-.206 3.953-1.994 4.932-3.076 1.701-6.806 1.89-10.191 1.289-1.787-.327-3.884-.894-4.864-2.578-.43-1.65-.705-3.334-.98-5.018l.103-.275.309-.155c5.121 3.386 12.288 3.386 17.427.0.808.241.206 1.22.189 1.805zM26.01 4.951c-.584 3.764-1.255 7.51-1.908 11.257-.189 1.1-1.255 1.719-2.148 2.183-3.214 1.615-6.96 1.89-10.483 1.512-2.389-.258-4.829-.894-6.771-2.389-.911-.705-.911-1.908-1.083-2.922-.602-3.523-1.289-7.046-1.719-10.604.206-1.547 1.942-2.217 3.231-2.698C6.848.654 8.686.362 10.508.19c3.884-.378 7.854-.241 11.618.859 1.341.395 2.784.945 3.695 2.097.412.533.275 1.203.189 1.805z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_bookmark" xmlns="http://www.w3.org/2000/svg"><path d="M15.268 4.392q.868.0 1.532.638t.664 1.506v17.463l-7.659-3.268-7.608 3.268V6.536q0-.868.664-1.506t1.532-.638h10.876zm4.34 14.144V4.392q0-.868-.638-1.532t-1.506-.664H6.537q0-.868.664-1.532T8.733.0h10.876q.868.0 1.532.664t.664 1.532v17.412z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_auto" xmlns="http://www.w3.org/2000/svg"><path d="M16.846 18.938h2.382L15.22 7.785h-2.44L8.772 18.938h2.382l.871-2.44h3.95zm7.087-9.062L27.999 14l-4.066 4.124v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809zm-11.385 4.937L14 10.282l1.452 4.531h-2.904z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_dark" xmlns="http://www.w3.org/2000/svg"><path d="M14 21.435q3.079.0 5.257-2.178T21.435 14t-2.178-5.257T14 6.565q-1.51.0-3.079.697 1.917.871 3.108 2.701T15.22 14t-1.191 4.037-3.108 2.701q1.568.697 3.079.697zm9.933-11.559L27.999 14l-4.066 4.124v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_light" xmlns="http://www.w3.org/2000/svg"><path d="M14 21.435q3.079.0 5.257-2.178T21.435 14t-2.178-5.257T14 6.565 8.743 8.743 6.565 14t2.178 5.257T14 21.435zm9.933-3.311v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809L27.999 14z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_cloud_off" xmlns="http://www.w3.org/2000/svg"><path d="M9.023 10.5H7q-1.914.0-3.281 1.395t-1.367 3.309 1.367 3.281T7 19.852h11.375zM3.5 4.976l1.477-1.477L24.5 23.022l-1.477 1.477-2.352-2.297H6.999q-2.898.0-4.949-2.051t-2.051-4.949q0-2.844 1.969-4.867t4.758-2.133zm19.086 5.578q2.242.164 3.828 1.832T28 16.351q0 3.008-2.461 4.758l-1.695-1.695q1.805-.984 1.805-3.063.0-1.422-1.039-2.461t-2.461-1.039h-1.75v-.602q0-2.68-1.859-4.539t-4.539-1.859q-1.531.0-2.953.711l-1.75-1.695Q11.431 3.5 14.001 3.5q2.953.0 5.496 2.078t3.09 4.977z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_code" xmlns="http://www.w3.org/2000/svg"><path d="M9.917 24.5a1.75 1.75.0 10-3.501.001 1.75 1.75.0 003.501-.001zm0-21a1.75 1.75.0 10-3.501.001A1.75 1.75.0 009.917 3.5zm11.666 2.333a1.75 1.75.0 10-3.501.001 1.75 1.75.0 003.501-.001zm1.75.0a3.502 3.502.0 01-1.75 3.026c-.055 6.581-4.721 8.039-7.82 9.023-2.898.911-3.846 1.349-3.846 3.117v.474a3.502 3.502.0 011.75 3.026c0 1.932-1.568 3.5-3.5 3.5s-3.5-1.568-3.5-3.5c0-1.294.711-2.424 1.75-3.026V6.526A3.502 3.502.0 014.667 3.5c0-1.932 1.568-3.5 3.5-3.5s3.5 1.568 3.5 3.5a3.502 3.502.0 01-1.75 3.026v9.06c.93-.456 1.914-.766 2.807-1.039 3.391-1.075 5.323-1.878 5.359-5.687a3.502 3.502.0 01-1.75-3.026c0-1.932 1.568-3.5 3.5-3.5s3.5 1.568 3.5 3.5z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_date" xmlns="http://www.w3.org/2000/svg"><path d="M27.192 28.844V11.192H4.808v17.652h22.384zm0-25.689q1.277.0 2.253.976t.976 2.253v22.459q0 1.277-.976 2.216t-2.253.939H4.808q-1.352.0-2.291-.901t-.939-2.253V6.385q0-1.277.939-2.253t2.291-.976h1.577V.001h3.23v3.155h12.769V.001h3.23v3.155h1.577zm-3.155 11.267v3.155h-3.23v-3.155h3.23zm-6.46.0v3.155h-3.155v-3.155h3.155zm-6.384.0v3.155h-3.23v-3.155h3.23z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_download" xmlns="http://www.w3.org/2000/svg"><path d="M2.866 28.209h26.269v3.79H2.866v-3.79zm26.268-16.925L16 24.418 2.866 11.284h7.493V.001h11.283v11.283h7.493z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_email" xmlns="http://www.w3.org/2000/svg"><path d="M28.845 9.615v-3.23L16 14.422 3.155 6.385v3.23L16 17.577zm0-6.46q1.277.0 2.216.977T32 6.385v19.23q0 1.277-.939 2.253t-2.216.977H3.155q-1.277.0-2.216-.977T0 25.615V6.385q0-1.277.939-2.253t2.216-.977h25.69z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_git" xmlns="http://www.w3.org/2000/svg"><path d="M27.472 12.753 15.247.529a1.803 1.803.0 00-2.55.0l-2.84 2.84 2.137 2.137a2.625 2.625.0 013.501 3.501l3.499 3.499a2.625 2.625.0 11-1.237 1.237l-3.499-3.499c-.083.04-.169.075-.257.106v7.3a2.626 2.626.0 11-1.75.0v-7.3a2.626 2.626.0 01-1.494-3.607L8.62 4.606l-8.09 8.09a1.805 1.805.0 000 2.551l12.225 12.224a1.803 1.803.0 002.55.0l12.168-12.168a1.805 1.805.0 000-2.551z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_github" xmlns="http://www.w3.org/2000/svg"><path d="M16 .394c8.833.0 15.999 7.166 15.999 15.999.0 7.062-4.583 13.062-10.937 15.187-.813.146-1.104-.354-1.104-.771.0-.521.021-2.25.021-4.396.0-1.5-.5-2.458-1.083-2.958 3.562-.396 7.312-1.75 7.312-7.896.0-1.75-.625-3.167-1.646-4.291.167-.417.708-2.042-.167-4.25-1.333-.417-4.396 1.646-4.396 1.646a15.032 15.032.0 00-8 0S8.937 6.602 7.603 7.018c-.875 2.208-.333 3.833-.167 4.25-1.021 1.125-1.646 2.542-1.646 4.291.0 6.125 3.729 7.5 7.291 7.896-.458.417-.875 1.125-1.021 2.146-.917.417-3.25 1.125-4.646-1.333-.875-1.521-2.458-1.646-2.458-1.646-1.562-.021-.104.979-.104.979 1.042.479 1.771 2.333 1.771 2.333.938 2.854 5.396 1.896 5.396 1.896.0 1.333.021 2.583.021 2.979.0.417-.292.917-1.104.771C4.582 29.455-.001 23.455-.001 16.393-.001 7.56 7.165.394 15.998.394zM6.063 23.372c.042-.083-.021-.187-.146-.25-.125-.042-.229-.021-.271.042-.042.083.021.187.146.25.104.062.229.042.271-.042zm.646.709c.083-.062.062-.208-.042-.333-.104-.104-.25-.146-.333-.062-.083.062-.062.208.042.333.104.104.25.146.333.062zm.625.937c.104-.083.104-.25.0-.396-.083-.146-.25-.208-.354-.125-.104.062-.104.229.0.375s.271.208.354.146zm.875.875c.083-.083.042-.271-.083-.396-.146-.146-.333-.167-.417-.062-.104.083-.062.271.083.396.146.146.333.167.417.062zm1.187.521c.042-.125-.083-.271-.271-.333-.167-.042-.354.021-.396.146s.083.271.271.312c.167.062.354.0.396-.125zm1.313.104c0-.146-.167-.25-.354-.229-.187.0-.333.104-.333.229.0.146.146.25.354.229.187.0.333-.104.333-.229zm1.208-.208c-.021-.125-.187-.208-.375-.187-.187.042-.312.167-.292.312.021.125.187.208.375.167s.312-.167.292-.292z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_gitlab" xmlns="http://www.w3.org/2000/svg"><path d="M1.629 11.034 14 26.888.442 17.048a1.09 1.09.0 01-.39-1.203l1.578-4.811zm7.217.0h10.309l-5.154 15.854zM5.753 1.475l3.093 9.559H1.63l3.093-9.559a.548.548.0 011.031.0zm20.618 9.559 1.578 4.811c.141.437-.016.922-.39 1.203l-13.558 9.84 12.371-15.854zm0 0h-7.216l3.093-9.559a.548.548.0 011.031.0z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_heart" xmlns="http://www.w3.org/2000/svg"><path d="M16 29.714a1.11 1.11.0 01-.786-.321L4.072 18.643c-.143-.125-4.071-3.714-4.071-8 0-5.232 3.196-8.357 8.535-8.357 3.125.0 6.053 2.464 7.464 3.857 1.411-1.393 4.339-3.857 7.464-3.857 5.339.0 8.535 3.125 8.535 8.357.0 4.286-3.928 7.875-4.089 8.035L16.785 29.392c-.214.214-.5.321-.786.321z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_keyborad_arrow_down" xmlns="http://www.w3.org/2000/svg"><path d="M3.281 5.36 14 16.079 24.719 5.36 28 8.641l-14 14-14-14z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_keyborad_arrow_left" xmlns="http://www.w3.org/2000/svg"><path d="M25.875 28.25 22.125 32 6.126 16.001 22.125.002l3.75 3.75-12.25 12.25z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_keyborad_arrow_right" xmlns="http://www.w3.org/2000/svg"><path d="M6.125 28.25 18.375 16 6.125 3.75 9.875.0l15.999 15.999L9.875 31.998z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_link" xmlns="http://www.w3.org/2000/svg"><path d="M24.037 7.963q3.305.0 5.634 2.366T32 16t-2.329 5.671-5.634 2.366h-6.46v-3.08h6.46q2.028.0 3.493-1.465t1.465-3.493-1.465-3.493-3.493-1.465h-6.46v-3.08h6.46zM9.615 17.578v-3.155h12.77v3.155H9.615zM3.005 16q0 2.028 1.465 3.493t3.493 1.465h6.46v3.08h-6.46q-3.305.0-5.634-2.366T0 16.001t2.329-5.671 5.634-2.366h6.46v3.08h-6.46q-2.028.0-3.493 1.465t-1.465 3.493z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_menu" xmlns="http://www.w3.org/2000/svg"><path d="M.001 5.334h31.998v3.583H.001V5.334zm0 12.416v-3.5h31.998v3.5H.001zm0 8.916v-3.583h31.998v3.583H.001z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_notification" xmlns="http://www.w3.org/2000/svg"><path d="M22.615 19.384l2.894 2.894v1.413H2.49v-1.413l2.894-2.894V12.25q0-3.365 1.716-5.856t4.745-3.231v-1.01q0-.875.606-1.514T13.999.0t1.548.639.606 1.514v1.01q3.029.74 4.745 3.231t1.716 5.856v7.134zM14 27.999q-1.211.0-2.053-.808t-.841-2.019h5.788q0 1.144-.875 1.986T14 27.999z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_path" xmlns="http://www.w3.org/2000/svg"><path d="M28 12.62h-9.793V8.414h-2.826v11.173h2.826v-4.206H28V26.62h-9.793v-4.206H12.62v-14H9.794v4.206H.001V1.381h9.793v4.206h8.413V1.381H28V12.62z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_person" xmlns="http://www.w3.org/2000/svg"><path d="M16 20.023q5.052.0 10.526 2.199t5.473 5.754v4.023H0v-4.023q0-3.555 5.473-5.754t10.526-2.199zM16 16q-3.275.0-5.614-2.339T8.047 8.047t2.339-5.661T16 0t5.614 2.386 2.339 5.661-2.339 5.614T16 16z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_search" xmlns="http://www.w3.org/2000/svg"><path d="M11.925 20.161q3.432.0 5.834-2.402t2.402-5.834-2.402-5.834-5.834-2.402-5.834 2.402-2.402 5.834 2.402 5.834 5.834 2.402zm10.981.0L32 29.255 29.255 32l-9.094-9.094v-1.458l-.515-.515q-3.26 2.831-7.721 2.831-4.976.0-8.45-3.432T.001 11.925t3.474-8.45 8.45-3.474 8.407 3.474 3.432 8.45q0 1.802-.858 4.075t-1.973 3.646l.515.515h1.458z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_shield" xmlns="http://www.w3.org/2000/svg"><path d="M22.167 15.166V3.5h-8.166v20.726c.93-.492 2.424-1.349 3.883-2.497 1.95-1.531 4.284-3.919 4.284-6.562zm3.499-13.999v14c0 7.674-10.737 12.523-11.192 12.724-.146.073-.31.109-.474.109s-.328-.036-.474-.109c-.456-.201-11.192-5.049-11.192-12.724v-14C2.334.529 2.863.0 3.501.0H24.5c.638.0 1.167.529 1.167 1.167z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_tags" xmlns="http://www.w3.org/2000/svg"><path d="M6.606 7.549c0-1.047-.84-1.887-1.887-1.887s-1.887.84-1.887 1.887.84 1.887 1.887 1.887 1.887-.84 1.887-1.887zm15.732 8.493c0 .501-.206.988-.546 1.327l-7.239 7.254c-.354.339-.84.546-1.342.546s-.988-.206-1.327-.546L1.342 14.066C.59 13.329.0 11.899.0 10.852V4.718a1.9 1.9.0 011.887-1.887h6.134c1.047.0 2.477.59 3.229 1.342L21.792 14.7c.339.354.546.84.546 1.342zm5.661.0c0 .501-.206.988-.546 1.327l-7.239 7.254c-.354.339-.84.546-1.342.546-.767.0-1.15-.354-1.651-.87l6.93-6.93c.339-.339.546-.826.546-1.327s-.206-.988-.546-1.342L13.609 4.173c-.752-.752-2.182-1.342-3.229-1.342h3.303c1.047.0 2.477.59 3.229 1.342L27.454 14.7c.339.354.546.84.546 1.342z"/></symbol></svg><div class=wrapper><input type=checkbox class=hidden id=menu-control><header class=gdoc-header><div class="container flex align-center justify-between"><label for=menu-control class=gdoc-nav__control><svg class="icon gdoc_menu"><use xlink:href="#gdoc_menu"/></svg><svg class="icon gdoc_arrow_back"><use xlink:href="#gdoc_arrow_back"/></svg></label><a class=gdoc-header__link href=https://aquastripe.github.io/RL-notes/><span class="gdoc-brand flex align-center"><img class=gdoc-brand__img src=/RL-notes/brand.svg alt>
Reinforcement Learning Notes</span></a>
<span id=gdoc-dark-mode><svg class="icon gdoc_brightness_dark"><use xlink:href="#gdoc_brightness_dark"/></svg><svg class="icon gdoc_brightness_light"><use xlink:href="#gdoc_brightness_light"/></svg><svg class="icon gdoc_brightness_auto"><use xlink:href="#gdoc_brightness_auto"/></svg></span></div></header><main class="container flex flex-even"><aside class=gdoc-nav><nav><div class=gdoc-search><svg class="icon gdoc_search"><use xlink:href="#gdoc_search"/></svg><input type=text id=gdoc-search-input class=gdoc-search__input placeholder=Search... aria-label=Search maxlength=64><div class="gdoc-search__spinner spinner hidden"></div><ul id=gdoc-search-results class=gdoc-search__list></ul></div><section class=gdoc-nav--main><h2>Navigation</h2><ul class=gdoc-nav__list><li><span class=flex>Reinforcement Learning: An Introduction</span><ul class=gdoc-nav__list><li><span class=flex><a href=/RL-notes/Reinforcement-Learning-An-Introduction/introduction/ class=gdoc-nav__entry>Introduction</a></span></li><li><span class=flex><a href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/ class=gdoc-nav__entry>Tabular Solution Methods</a></span><ul class=gdoc-nav__list><li><span class=flex><a href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/ class="gdoc-nav__entry is-active">Multi-Armed Bandits</a></span></li></ul></li></ul></li></ul></section><section class=gdoc-nav--more></section></nav></aside><div class=gdoc-page><div class="gdoc-page__header flex flex-wrap
justify-between
hidden-mobile" itemprop=breadcrumb><div><svg class="icon gdoc_path hidden-mobile"><use xlink:href="#gdoc_path"/></svg><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemscope itemtype=https://schema.org/WebPage itemprop=item itemid=/RL-notes/ href=/RL-notes/><span itemprop=name>Reinforcement Learning Notes</span></a><meta itemprop=position content="2"></li><li>/</li><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemscope itemtype=https://schema.org/WebPage itemprop=item itemid=/RL-notes/reinforcement-learning-an-introduction/ href=/RL-notes/reinforcement-learning-an-introduction/><span itemprop=name>Reinforcement Learning: An Introduction</span></a><meta itemprop=position content="3"></li><li>/</li><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemscope itemtype=https://schema.org/WebPage itemprop=item itemid=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/ href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/><span itemprop=name>Tabular Solution Methods</span></a><meta itemprop=position content="4"></li><li>/</li><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Multi-Armed Bandits</span><meta itemprop=position content="5"></li></ol></div></div><script>MathJax={tex:{inlineMath:[["$","$"]]},displayMath:[["$$","$$"],["[[","]]"]],svg:{fontCache:"global"}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><article class="gdoc-markdown gdoc-markdown__align--left"><h1>Multi-Armed Bandits</h1><div class="gdoc-toc gdoc-toc__level--6"><nav id=TableOfContents><ul><li><a href=#a-k-armed-bandit-problem>A k-armed Bandit Problem</a></li><li><a href=#action-value-methods>Action-value Methods</a><ul><li><a href=#sample-average-method>Sample-average method</a></li><li><a href=#greedy>Greedy</a></li><li><a href=#varepsilon-greedy>$\varepsilon$-greedy</a></li></ul></li><li><a href=#the-10-armed-testbed>The 10-armed Testbed</a></li><li><a href=#incremental-implementation>Incremental Implementation</a></li><li><a href=#tracking-a-nonstationary-problem>Tracking a Nonstationary Problem</a></li><li><a href=#可變長度的步長-step-size>可變長度的步長 (step-size)</a></li><li><a href=#optimistic-initial-value>Optimistic Initial Value</a></li><li><a href=#upper-confidence-bound-action-selection>Upper-Confidence-Bound Action Selection</a></li><li><a href=#gradient-bandit-algorithms>Gradient Bandit Algorithms</a></li><li><a href=#associative-search-contextual-bandits>Associative Search (Contextual Bandits)</a></li><li><a href=#summary>Summary</a></li></ul></nav><hr></div><div class=gdoc-page__anchorwrap><h2 id=a-k-armed-bandit-problem>A k-armed Bandit Problem
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/#a-k-armed-bandit-problem class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor A k-armed Bandit Problem" href=#a-k-armed-bandit-problem><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg></a></h2></div><p><em>k</em> 臂吃角子老虎機問題：你需要從 <em>k</em> 個不同 選項 或是 動作 中做出選擇，每個選擇都會帶來獎勵，獎勵多寡來自固定的機率分佈。你的目標是要在經過一段時間後得到最大的累積獎勵值。</p><p>在這個問題中，每個動作會有期望或平均的獎勵值，讓我們稱之為該動作的 價值。我們把在時間點 $t$ 選擇的動作表示為 $A_t$，對應的獎勵表示為 $R_t$。那麼：任意動作 $a$ 的價值 $q_{*}(a)$ 是選擇該動作的期望值：</p><p>$$q_{*}(a) \doteq \mathbb{E}[R_t | A_t = a].$$</p><p>我們把 在時間點 $t$ 選擇動作 $a$ 的估計價值表示為 $Q_t(a)$，我們想要此估計值要接近 $q_{*}(a)$。</p><div class=gdoc-page__anchorwrap><h2 id=action-value-methods>Action-value Methods
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/#action-value-methods class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Action-value Methods" href=#action-value-methods><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg></a></h2></div><p>估計動作的價值，作為根據來做決策，這類的方法通稱為 <em>action-value methods</em>。</p><div class=gdoc-page__anchorwrap><h3 id=sample-average-method>Sample-average method
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/#sample-average-method class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Sample-average method" href=#sample-average-method><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg></a></h3></div><p>一個自然的方式是計算平均得到多少的獎勵來當作估計值：</p><p>$$Q_t(a) \doteq \dfrac{\text{sum of rewards when } a \text{ taken prior to }t}{\text{number of times } a \text{ taken prior to }t} = \dfrac{\sum_{i=1}^{t-1}{R_i \cdot \mathbb{1}_{A_i=a}}}{\sum_{i=1}^{t-1}{\mathbb{1}_{A_i=a}}}$$</p><p>其中 $\mathbb{1}_{predicate}$ 表示 $predicate$ 為真時設定為 $1$，否則為 $0$。</p><ul><li>當分母趨近於 $0$ 的時候，把 $Q_{t}(a)$ 設定為預設值 (例如 $0$)。</li><li>當分母趨近於無限大時，根據大數法則，$Q_t(a)$ 會收斂於 $q_*(a)$。</li></ul><div class=gdoc-page__anchorwrap><h3 id=greedy>Greedy
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/#greedy class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Greedy" href=#greedy><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg></a></h3></div><p>最簡單的方法是選擇 最大的估計價值，如下：</p><p>$$A_t \doteq \arg \max_{a} Q_t (a)$$</p><p>這個方法完全的利用已知的資訊，不採樣任何較差的動作來進行任何探索。</p><div class=gdoc-page__anchorwrap><h3 id=varepsilon-greedy>$\varepsilon$-greedy
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/#varepsilon-greedy class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor $\varepsilon$-greedy" href=#varepsilon-greedy><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg></a></h3></div><p>一種替代方案是大部分時間都用貪心法，以機率為 $\varepsilon$ 採樣其他的動作來進行少量的探索。</p><div class=gdoc-page__anchorwrap><h2 id=the-10-armed-testbed>The 10-armed Testbed
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/#the-10-armed-testbed class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor The 10-armed Testbed" href=#the-10-armed-testbed><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg></a></h2></div><p><img src=2.1.png alt></p><p>Figure 2.1 為 $k=10$ 的實例，每個都是 $\mathcal{N}(0,1)$ 的常態分佈。</p><p><img src=2.2.png alt></p><p>$\varepsilon$-greedy 比 greedy 還要看情況決定</p><ul><li>如果獎勵的變異數是 10 而非 1，那麼要花費更多的探索才會找到最佳解。</li><li>如果獎勵的變異數是 0，那麼 greedy 可能會找到最佳解而不需要再探索。</li><li>如果吃角子老虎機是確定性 (deterministic) 但不穩定的 (nonstationarity)，例如：會隨著時間改變，那麼 $\varepsilon$-greedy 還是比較好。</li></ul><div class=gdoc-page__anchorwrap><h2 id=incremental-implementation>Incremental Implementation
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/#incremental-implementation class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Incremental Implementation" href=#incremental-implementation><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg></a></h2></div><p>以上都是以 平均獎勵值 來估計 價值。以下討論如何以更有效率的方式計算。</p><p>設 $R_{i}$ 為當前動作中第 $i$ 個選擇接收的獎勵值，$Q_n$ 為這個動作第 $n-1$ 次選擇後的估計價值，可以簡寫如下：</p><p>$$Q_n \doteq \dfrac{R_1 + R_2 + \ldots + R_{n-1}}{n-1}$$</p><p>計算複雜度會隨著次數越來越多次以後變得更多。可以改寫為遞迴式：</p><p>$$
\begin{aligned}
Q_{n+1}
&= \frac{1}{n} \sum_{i=1}^{n} R_{i}\newline
&= \frac{1}{n}\left(R_{n}+\sum_{i=1}^{n-1} R_{i}\right)\newline
&=\frac{1}{n}\left(R_{n}+(n-1) \frac{1}{n-1} \sum_{i=1}^{n-1} R_{i}\right)\newline
&=\frac{1}{n}\left(R_{n}+(n-1) Q_{n}\right)\newline
&=\frac{1}{n}\left(R_{n}+n Q_{n}-Q_{n}\right)\newline
&=Q_{n}+\frac{1}{n}\left[R_{n}-Q_{n}\right],\newline
\end{aligned}
$$</p><p>通用形式如下：</p><p>$$\text{NewEstimate} \leftarrow \text{OldEstimate} + \text{StepSize}[\text{Target} - \text{OldEstimate}]$$</p><p>其中 $\text{Target} - \text{OldEstimate}$ 為 <strong>誤差</strong>，$\text{StepSize}$ 為 <strong>步長</strong>，通常以符號 $\alpha$ 或 $\alpha_{t}(a)$ 表示。</p><p>增量法的完整演算法如下：</p><p><img src=a-simple-bandit-algorithm.png alt></p><div class=gdoc-page__anchorwrap><h2 id=tracking-a-nonstationary-problem>Tracking a Nonstationary Problem
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/#tracking-a-nonstationary-problem class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Tracking a Nonstationary Problem" href=#tracking-a-nonstationary-problem><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg></a></h2></div><p>以上方法適用於固定的 (stationary) 吃角子老虎機問題，也就是獲得的 獎勵值 不會隨著時間改變。為了計算不固定 (nonstationary) 的問題，通常會為每一步給個 <strong>權重值</strong>，可以經由 $\text{StepSize}$ $\alpha$ 給定。</p><p>例如：原增量法的計算式如下：</p><p>$$Q_{n+1} \doteq Q_{n}+\alpha\left[R_{n}-Q_{n}\right]$$</p><p>其中 $\alpha \in (0, 1]$ 為常數。那麼展開後的 價值 為過去 獎勵值 的 <strong>加權平均</strong> (weighted average)，結果計算如下 (2.6)：</p><p>$$\begin{aligned}
Q_{n+1} &=Q_{n}+\alpha\left[R_{n}-Q_{n}\right] \newline
&=\alpha R_{n}+(1-\alpha) Q_{n} \newline
&=\alpha R_{n}+(1-\alpha)\left[\alpha R_{n-1}+(1-\alpha) Q_{n-1}\right] \newline
&=\alpha R_{n}+(1-\alpha) \alpha R_{n-1}+(1-\alpha)^{2} Q_{n-1} \newline
&=\alpha R_{n}+(1-\alpha) \alpha R_{n-1}+(1-\alpha)^{2} \alpha R_{n-2}+\newline
& \quad \cdots+(1-\alpha)^{n-1} \alpha R_{1}+(1-\alpha)^{n} Q_{1} \newline
&=(1-\alpha)^{n} Q_{1}+\sum_{i=1}^{n} \alpha(1-\alpha)^{n-i} R_{i}
\end{aligned}$$</p><p>稱為加權平均是因為 $(1-\alpha)^{n}+\sum_{i=1}^{n} \alpha(1-\alpha)^{n-i}=1$。</p><ul><li>$(1 - \alpha) &lt; 1$: 所以給予過去探索的獎勵值 $R_i$ 權重會越來越小</li><li>當 $(1 - \alpha) &lt; 0$: 稱為 exponential recency-weighted average</li><li>當 $(1 - \alpha) = 0$: 所有權重都會給最近的獎勵值。</li></ul><div class=gdoc-page__anchorwrap><h2 id=可變長度的步長-step-size>可變長度的步長 (step-size)
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/#可變長度的步長-step-size class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor 可變長度的步長 (step-size)" href=#%e5%8f%af%e8%ae%8a%e9%95%b7%e5%ba%a6%e7%9a%84%e6%ad%a5%e9%95%b7-step-size><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg></a></h2></div><p>設 在第 $n$ 個選擇了動作 $a$ 之後的步長 (step-size) 為 $\alpha_n (a)$</p><ul><li>$\alpha_n (a) = \dfrac{1}{n}$: 採樣平均法 (sample-average method)，根據大數法則 (the law of large numbers) 保證收斂到真實的動作價值。</li><li>$\sum_{n=1}^\infty \alpha_n (a) = \infty \text{ and } \sum_{n=1}^\infty \alpha_n^2 (a) &lt; \infty$: 根據隨機近似理論 (stochastic approximation theory) 符合條件保證收斂到機率為 $1$。<ul><li>第一個條件保證步長總和大到可以克服任意初始條件和隨機波動。</li><li>第二個條件保證最後步長足夠小到可以收斂。</li></ul></li></ul><p>問題：</p><ul><li>雖然保證收斂，但是收斂速度緩慢，或是要經過調整才能得到良好的收斂率。</li><li>理論上可行，實務上很少使用。</li></ul><div class=gdoc-page__anchorwrap><h2 id=optimistic-initial-value>Optimistic Initial Value
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/#optimistic-initial-value class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Optimistic Initial Value" href=#optimistic-initial-value><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg></a></h2></div><p>以上的方法都依賴於初始的動作價值 (initial action-value) $Q_1(a)$。在統計學裡，這稱為 <strong>偏差</strong> (<em>biased</em> by their initial estimates)。</p><ul><li>在採樣平均法中，偏差會在所有動作至少採樣過一次以後消失。</li><li>在所有 $\alpha$ 為常數的方法，根據 2.6 式，偏差會隨著時間削減但永久存在。<ul><li>壞處：初始值成為一種使用者設定的參數，除非把它們設定為 $0$。</li><li>好處：以一種簡單的方式提供某種先驗的知識，告訴我們預期可以得到什麼程度的獎勵值。</li></ul></li></ul><p>初始動作價值也可以用來 <strong>鼓勵探索</strong>。假設不把初始值設定為 $0$，在 10-armed testbed 中把所有都設定成 $+5$，而 $q_*(a)$ 在先前設定為平均為 $0$ 變異數為 $1$ 的常態分佈。</p><ol><li>這種情況下，初始的估計值都是 $+5$ 是樂觀的 (optimistic)，這種樂觀會鼓勵 動作價值法 去探索。</li><li>不論選擇哪個動作，獎勵值都會少於估計值，學習器 (learner) 會對獎勵值失望 (disappointed)。</li><li>所有動作在嘗試幾次以後收斂。</li></ol><p>下圖為 10-armed testbed 比較兩種設定 ($\alpha = 0.1$)：</p><ul><li>greedy: $Q_1(a) = +5$</li><li>$\varepsilon$-greedy: $Q_1(a) = 0$</li></ul><p><img src=2.3.png alt></p><p>它是一個簡單的技巧，但是不適用於不固定問題 (nonstationary problems)。</p><ul><li>當任務改變時，需要開啟新的探索，這個方法就會無效。</li></ul><div class=gdoc-page__anchorwrap><h2 id=upper-confidence-bound-action-selection>Upper-Confidence-Bound Action Selection
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/#upper-confidence-bound-action-selection class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Upper-Confidence-Bound Action Selection" href=#upper-confidence-bound-action-selection><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg></a></h2></div><p>因為不確定 動作價值 (action-value) 估計值的準確度，所以需要 <strong>探索</strong>。問題是如何選擇可能最好的動作？以下提供一個有效的方式：</p><p>$$A_{t} \doteq \underset{a}{\arg \max }\left[Q_{t}(a)+c \sqrt{\frac{\ln t}{N_{t}(a)}}\right],$$</p><ul><li>$N_t(a)$ 表示動作 $a$ 隨著時間 $t$ 經過後採樣的次數。</li><li>$c > 0$ 用來控制探索的程度。</li><li>平方根內的值用來測量動作 $a$ 的不確定性 (uncertainty) 或 變異數 (variance)。</li></ul><p>下圖為 10-armed testbed 比較兩種設定：</p><ul><li>UCB: $c = 2$</li><li>$\varepsilon$-greedy: $\varepsilon = 0.1$</li></ul><p><img src=2.4.png alt></p><p>解決更通用的 RL 問題的困難：</p><ul><li>處理不固定問題 (nonstationary problems) 方法更複雜。</li><li>處理更大的狀態空間 (state spaces)。</li></ul><div class=gdoc-page__anchorwrap><h2 id=gradient-bandit-algorithms>Gradient Bandit Algorithms
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/#gradient-bandit-algorithms class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Gradient Bandit Algorithms" href=#gradient-bandit-algorithms><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg></a></h2></div><p>先前的作法是估計動作價值之後根據這些估計值來選擇動作。</p><p>這節的作法是學習一個動作的 <strong>偏好</strong> (<em>preference</em>)，表示為 $H_t (a) \in \mathbb{R}$。</p><ul><li>偏好跟獎勵值無關。</li><li>偏好是相對的。如果對所有偏好都增加 $1000$，根據 <em>soft-max</em> 將不影響計算結果：</li></ul><p>$$
\operatorname{Pr}\{A_{t}=a \} \doteq \frac{e^{H_{t}(a)}}{\sum_{b=1}^{k} e^{H_{t}(b)}} \doteq \pi_{t}(a)
$$</p><p>使用 stochastic gradient ascent 來更新偏好值：</p><p>$$\begin{aligned}
H_{t+1}\left(A_{t}\right) & \doteq H_{t}\left(A_{t}\right)+\alpha\left(R_{t}-\bar{R}_{t}\right)\left(1-\pi_{t}\left(A_{t}\right)\right), & & \text { and } \newline
H_{t+1}(a) & \doteq H_{t}(a)-\alpha\left(R_{t}-\bar{R}_{t}\right) \pi_{t}(a), & & \text { for all } a \neq A_{t},
\end{aligned}$$</p><ul><li>$\alpha > 0$: 步長 (step-size) 參數。</li><li>$\bar{R}_t \in \mathbb{R}$: 平均但不包含時間點 $t$ 的獎勵值，通常當作比較基準 (baseline)。<ul><li>如果獎勵比基準大，未來選擇動作 $A_t$ 的機率會增加；否則就降低。</li></ul></li><li>原本應該要計算的梯度: $\dfrac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x)(\mathbb{1}_{a=x} - \pi_t(a))$ (證明略)</li></ul><p>下圖為 10-armed testbed $\mathcal{N}(4,1)$ 比較兩種設定：</p><ul><li>包含 baseline</li><li>不含 baseline ($\bar{R}_t = 0$)</li></ul><p><img src=2.5.png alt></p><div class=gdoc-page__anchorwrap><h2 id=associative-search-contextual-bandits>Associative Search (Contextual Bandits)
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/#associative-search-contextual-bandits class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Associative Search (Contextual Bandits)" href=#associative-search-contextual-bandits><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg></a></h2></div><p>目前為止考慮的都是非結合式的任務 (nonassociative tasks)，不需要在不同情況下結合不同的動作。更通用的 RL 任務是學習一個策略 (policy)：從一個狀態映射到一個最好的動作選項 (a mapping from situations to the actions that are best in those situations)。</p><p>舉例：</p><ul><li>假設有幾個不同的 k-armed 吃角子老虎機任務。</li><li>每一步都是隨機的。</li><li>假設機器是可以辨識的，例如當他改變動作價值 (action values) 時你會知道顏色。</li><li>目標是要學習策略來結合每個任務，根據你看到的顏色，選擇當前任務最好的動作。</li></ul><p>在文獻中也稱為 <em>contextual bandits</em>。</p><div class=gdoc-page__anchorwrap><h2 id=summary>Summary
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/#summary class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Summary" href=#summary><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg></a></h2></div><ul><li><em>k</em>-armed Bandit Problem<ul><li>$\varepsilon$-greedy 隨機性的探索不同動作</li><li>greedy with optimistic initialization 利用初始值來當作探索策略</li><li>UCB 確定性的選擇，並且計算採樣過的動作的不確定性來取得平衡</li><li>Gradient bandit algorithm 估計動作偏好 (action preference) 而非動作價值</li></ul></li></ul><p>下圖為各個方法在控制不同參數過後平均取得的獎勵值：</p><p><img src=2.6.png alt></p></article><div class="gdoc-page__footer flex flex-wrap justify-between"></div></div></main><footer class=gdoc-footer><div class="container flex flex-wrap"><span class=gdoc-footer__item>Built with <a href=https://gohugo.io/ class=gdoc-footer__link>Hugo</a> and<svg class="icon gdoc_heart"><use xlink:href="#gdoc_heart"/></svg></span></div></footer></div><script defer src=/RL-notes/js/en.search.min.191d955031f082e3b2dc0bee21c9b0726710674ac895a139ac85aee536ac9039.js></script><script defer src=/RL-notes/js/clipboard-27784b7376.min.js></script><script defer src=/RL-notes/js/clipboard-loader-f0b5fbd5f6.min.js></script></body></html>