<!doctype html><html lang=zh class=color-toggle-hidden>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=color-scheme content="light dark">
<meta name=description content="Temporal-difference (TD) learning 是一個 RL 新穎且重要的觀念，它結合 Monte Carlo 和 dynamic programming (DP) 的想法。 Monte Carlo: 類似於 Monte Carlo，TD 法可以直接從原始經驗學習，而不需要環境模型。 DP: 類似於 D">
<title>Temporal Difference Learning | Reinforcement Learning Notes</title>
<link rel=icon href=/RL-notes/favicon/favicon-32x32.png type=image/x-icon>
<script src=/RL-notes/js/darkmode-ce906ea916.min.js></script>
<link rel=preload as=font href=/RL-notes/fonts/Metropolis.woff2 type=font/woff2 crossorigin=anonymous>
<link rel=preload as=font href=/RL-notes/fonts/LiberationSans.woff2 type=font/woff2 crossorigin=anonymous>
<link rel=preload href=/RL-notes/main-1be624d457.min.css as=style>
<link rel=stylesheet href=/RL-notes/main-1be624d457.min.css media=all>
<link rel=preload href=/RL-notes/mobile-3fc330242c.min.css as=style>
<link rel=stylesheet href=/RL-notes/mobile-3fc330242c.min.css media="screen and (max-width: 45rem)">
<link rel=preload href=/RL-notes/print-f79fc3e5d7.min.css as=style>
<link rel=stylesheet href=/RL-notes/print-f79fc3e5d7.min.css media=print>
<link rel=preload href=/RL-notes/custom.css as=style>
<link rel=stylesheet href=/RL-notes/custom.css media=all>
</head>
<body itemscope itemtype=https://schema.org/WebPage><svg class="svg-sprite" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_back" xmlns="http://www.w3.org/2000/svg"><path d="M24 10.526v2.947H5.755l8.351 8.421-2.105 2.105-12-12 12-12 2.105 2.105-8.351 8.421H24z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_left_alt" xmlns="http://www.w3.org/2000/svg"><path d="M5.965 10.526V6.035L0 12l5.965 5.965v-4.491H24v-2.947H5.965z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_right_alt" xmlns="http://www.w3.org/2000/svg"><path d="M18.035 10.526V6.035L24 12l-5.965 5.965v-4.491H0v-2.947h18.035z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_bitbucket" xmlns="http://www.w3.org/2000/svg"><path d="M15.905 13.355c.189 1.444-1.564 2.578-2.784 1.839-1.375-.602-1.375-2.784-.034-3.403 1.151-.705 2.818.223 2.818 1.564zm1.907-.361c-.309-2.44-3.076-4.056-5.328-3.042-1.426.636-2.389 2.148-2.32 3.747.086 2.097 2.08 3.815 4.176 3.626s3.729-2.234 3.472-4.331zm4.108-9.315c-.756-.997-2.045-1.169-3.179-1.358-3.214-.516-6.513-.533-9.727.034-1.066.172-2.269.361-2.939 1.323 1.1 1.031 2.664 1.186 4.073 1.358 2.544.327 5.156.344 7.699.017 1.426-.172 3.008-.309 4.073-1.375zm.979 17.788c-.481 1.684-.206 3.953-1.994 4.932-3.076 1.701-6.806 1.89-10.191 1.289-1.787-.327-3.884-.894-4.864-2.578-.43-1.65-.705-3.334-.98-5.018l.103-.275.309-.155c5.121 3.386 12.288 3.386 17.427.0.808.241.206 1.22.189 1.805zM26.01 4.951c-.584 3.764-1.255 7.51-1.908 11.257-.189 1.1-1.255 1.719-2.148 2.183-3.214 1.615-6.96 1.89-10.483 1.512-2.389-.258-4.829-.894-6.771-2.389-.911-.705-.911-1.908-1.083-2.922-.602-3.523-1.289-7.046-1.719-10.604.206-1.547 1.942-2.217 3.231-2.698C6.848.654 8.686.362 10.508.19c3.884-.378 7.854-.241 11.618.859 1.341.395 2.784.945 3.695 2.097.412.533.275 1.203.189 1.805z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_bookmark" xmlns="http://www.w3.org/2000/svg"><path d="M15.268 4.392q.868.0 1.532.638t.664 1.506v17.463l-7.659-3.268-7.608 3.268V6.536q0-.868.664-1.506t1.532-.638h10.876zm4.34 14.144V4.392q0-.868-.638-1.532t-1.506-.664H6.537q0-.868.664-1.532T8.733.0h10.876q.868.0 1.532.664t.664 1.532v17.412z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_auto" xmlns="http://www.w3.org/2000/svg"><path d="M16.846 18.938h2.382L15.22 7.785h-2.44L8.772 18.938h2.382l.871-2.44h3.95zm7.087-9.062L27.999 14l-4.066 4.124v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809zm-11.385 4.937L14 10.282l1.452 4.531h-2.904z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_dark" xmlns="http://www.w3.org/2000/svg"><path d="M14 21.435q3.079.0 5.257-2.178T21.435 14t-2.178-5.257T14 6.565q-1.51.0-3.079.697 1.917.871 3.108 2.701T15.22 14t-1.191 4.037-3.108 2.701q1.568.697 3.079.697zm9.933-11.559L27.999 14l-4.066 4.124v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_light" xmlns="http://www.w3.org/2000/svg"><path d="M14 21.435q3.079.0 5.257-2.178T21.435 14t-2.178-5.257T14 6.565 8.743 8.743 6.565 14t2.178 5.257T14 21.435zm9.933-3.311v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809L27.999 14z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_cloud_off" xmlns="http://www.w3.org/2000/svg"><path d="M9.023 10.5H7q-1.914.0-3.281 1.395t-1.367 3.309 1.367 3.281T7 19.852h11.375zM3.5 4.976l1.477-1.477L24.5 23.022l-1.477 1.477-2.352-2.297H6.999q-2.898.0-4.949-2.051t-2.051-4.949q0-2.844 1.969-4.867t4.758-2.133zm19.086 5.578q2.242.164 3.828 1.832T28 16.351q0 3.008-2.461 4.758l-1.695-1.695q1.805-.984 1.805-3.063.0-1.422-1.039-2.461t-2.461-1.039h-1.75v-.602q0-2.68-1.859-4.539t-4.539-1.859q-1.531.0-2.953.711l-1.75-1.695Q11.431 3.5 14.001 3.5q2.953.0 5.496 2.078t3.09 4.977z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_code" xmlns="http://www.w3.org/2000/svg"><path d="M9.917 24.5a1.75 1.75.0 10-3.501.001 1.75 1.75.0 003.501-.001zm0-21a1.75 1.75.0 10-3.501.001A1.75 1.75.0 009.917 3.5zm11.666 2.333a1.75 1.75.0 10-3.501.001 1.75 1.75.0 003.501-.001zm1.75.0a3.502 3.502.0 01-1.75 3.026c-.055 6.581-4.721 8.039-7.82 9.023-2.898.911-3.846 1.349-3.846 3.117v.474a3.502 3.502.0 011.75 3.026c0 1.932-1.568 3.5-3.5 3.5s-3.5-1.568-3.5-3.5c0-1.294.711-2.424 1.75-3.026V6.526A3.502 3.502.0 014.667 3.5c0-1.932 1.568-3.5 3.5-3.5s3.5 1.568 3.5 3.5a3.502 3.502.0 01-1.75 3.026v9.06c.93-.456 1.914-.766 2.807-1.039 3.391-1.075 5.323-1.878 5.359-5.687a3.502 3.502.0 01-1.75-3.026c0-1.932 1.568-3.5 3.5-3.5s3.5 1.568 3.5 3.5z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_date" xmlns="http://www.w3.org/2000/svg"><path d="M27.192 28.844V11.192H4.808v17.652h22.384zm0-25.689q1.277.0 2.253.976t.976 2.253v22.459q0 1.277-.976 2.216t-2.253.939H4.808q-1.352.0-2.291-.901t-.939-2.253V6.385q0-1.277.939-2.253t2.291-.976h1.577V.001h3.23v3.155h12.769V.001h3.23v3.155h1.577zm-3.155 11.267v3.155h-3.23v-3.155h3.23zm-6.46.0v3.155h-3.155v-3.155h3.155zm-6.384.0v3.155h-3.23v-3.155h3.23z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_download" xmlns="http://www.w3.org/2000/svg"><path d="M2.866 28.209h26.269v3.79H2.866v-3.79zm26.268-16.925L16 24.418 2.866 11.284h7.493V.001h11.283v11.283h7.493z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_email" xmlns="http://www.w3.org/2000/svg"><path d="M28.845 9.615v-3.23L16 14.422 3.155 6.385v3.23L16 17.577zm0-6.46q1.277.0 2.216.977T32 6.385v19.23q0 1.277-.939 2.253t-2.216.977H3.155q-1.277.0-2.216-.977T0 25.615V6.385q0-1.277.939-2.253t2.216-.977h25.69z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_git" xmlns="http://www.w3.org/2000/svg"><path d="M27.472 12.753 15.247.529a1.803 1.803.0 00-2.55.0l-2.84 2.84 2.137 2.137a2.625 2.625.0 013.501 3.501l3.499 3.499a2.625 2.625.0 11-1.237 1.237l-3.499-3.499c-.083.04-.169.075-.257.106v7.3a2.626 2.626.0 11-1.75.0v-7.3a2.626 2.626.0 01-1.494-3.607L8.62 4.606l-8.09 8.09a1.805 1.805.0 000 2.551l12.225 12.224a1.803 1.803.0 002.55.0l12.168-12.168a1.805 1.805.0 000-2.551z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_github" xmlns="http://www.w3.org/2000/svg"><path d="M16 .394c8.833.0 15.999 7.166 15.999 15.999.0 7.062-4.583 13.062-10.937 15.187-.813.146-1.104-.354-1.104-.771.0-.521.021-2.25.021-4.396.0-1.5-.5-2.458-1.083-2.958 3.562-.396 7.312-1.75 7.312-7.896.0-1.75-.625-3.167-1.646-4.291.167-.417.708-2.042-.167-4.25-1.333-.417-4.396 1.646-4.396 1.646a15.032 15.032.0 00-8 0S8.937 6.602 7.603 7.018c-.875 2.208-.333 3.833-.167 4.25-1.021 1.125-1.646 2.542-1.646 4.291.0 6.125 3.729 7.5 7.291 7.896-.458.417-.875 1.125-1.021 2.146-.917.417-3.25 1.125-4.646-1.333-.875-1.521-2.458-1.646-2.458-1.646-1.562-.021-.104.979-.104.979 1.042.479 1.771 2.333 1.771 2.333.938 2.854 5.396 1.896 5.396 1.896.0 1.333.021 2.583.021 2.979.0.417-.292.917-1.104.771C4.582 29.455-.001 23.455-.001 16.393-.001 7.56 7.165.394 15.998.394zM6.063 23.372c.042-.083-.021-.187-.146-.25-.125-.042-.229-.021-.271.042-.042.083.021.187.146.25.104.062.229.042.271-.042zm.646.709c.083-.062.062-.208-.042-.333-.104-.104-.25-.146-.333-.062-.083.062-.062.208.042.333.104.104.25.146.333.062zm.625.937c.104-.083.104-.25.0-.396-.083-.146-.25-.208-.354-.125-.104.062-.104.229.0.375s.271.208.354.146zm.875.875c.083-.083.042-.271-.083-.396-.146-.146-.333-.167-.417-.062-.104.083-.062.271.083.396.146.146.333.167.417.062zm1.187.521c.042-.125-.083-.271-.271-.333-.167-.042-.354.021-.396.146s.083.271.271.312c.167.062.354.0.396-.125zm1.313.104c0-.146-.167-.25-.354-.229-.187.0-.333.104-.333.229.0.146.146.25.354.229.187.0.333-.104.333-.229zm1.208-.208c-.021-.125-.187-.208-.375-.187-.187.042-.312.167-.292.312.021.125.187.208.375.167s.312-.167.292-.292z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_gitlab" xmlns="http://www.w3.org/2000/svg"><path d="M1.629 11.034 14 26.888.442 17.048a1.09 1.09.0 01-.39-1.203l1.578-4.811zm7.217.0h10.309l-5.154 15.854zM5.753 1.475l3.093 9.559H1.63l3.093-9.559a.548.548.0 011.031.0zm20.618 9.559 1.578 4.811c.141.437-.016.922-.39 1.203l-13.558 9.84 12.371-15.854zm0 0h-7.216l3.093-9.559a.548.548.0 011.031.0z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_heart" xmlns="http://www.w3.org/2000/svg"><path d="M16 29.714a1.11 1.11.0 01-.786-.321L4.072 18.643c-.143-.125-4.071-3.714-4.071-8 0-5.232 3.196-8.357 8.535-8.357 3.125.0 6.053 2.464 7.464 3.857 1.411-1.393 4.339-3.857 7.464-3.857 5.339.0 8.535 3.125 8.535 8.357.0 4.286-3.928 7.875-4.089 8.035L16.785 29.392c-.214.214-.5.321-.786.321z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_home" xmlns="http://www.w3.org/2000/svg"><path d="M24.003 15.695v8.336c0 .608-.504 1.111-1.111 1.111h-6.669v-6.669h-4.446v6.669H5.108a1.119 1.119.0 01-1.111-1.111v-8.336c0-.035.017-.069.017-.104L14 7.359l9.986 8.232a.224.224.0 01.017.104zm3.873-1.198-1.077 1.285a.578.578.0 01-.365.191h-.052a.547.547.0 01-.365-.122L14 5.831 1.983 15.851a.594.594.0 01-.417.122.578.578.0 01-.365-.191L.124 14.497a.57.57.0 01.069-.781L12.679 3.314c.729-.608 1.91-.608 2.64.0l4.237 3.543V3.471c0-.313.243-.556.556-.556h3.334c.313.0.556.243.556.556v7.085l3.803 3.161c.226.191.26.556.069.781z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_keyborad_arrow_down" xmlns="http://www.w3.org/2000/svg"><path d="M3.281 5.36 14 16.079 24.719 5.36 28 8.641l-14 14-14-14z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_keyborad_arrow_left" xmlns="http://www.w3.org/2000/svg"><path d="M25.875 28.25 22.125 32 6.126 16.001 22.125.002l3.75 3.75-12.25 12.25z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_keyborad_arrow_right" xmlns="http://www.w3.org/2000/svg"><path d="M6.125 28.25 18.375 16 6.125 3.75 9.875.0l15.999 15.999L9.875 31.998z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_keyborad_arrow_up" xmlns="http://www.w3.org/2000/svg"><path d="M24.719 22.64 14 11.921 3.281 22.64.0 19.359l14-14 14 14z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_link" xmlns="http://www.w3.org/2000/svg"><path d="M24.037 7.963q3.305.0 5.634 2.366T32 16t-2.329 5.671-5.634 2.366h-6.46v-3.08h6.46q2.028.0 3.493-1.465t1.465-3.493-1.465-3.493-3.493-1.465h-6.46v-3.08h6.46zM9.615 17.578v-3.155h12.77v3.155H9.615zM3.005 16q0 2.028 1.465 3.493t3.493 1.465h6.46v3.08h-6.46q-3.305.0-5.634-2.366T0 16.001t2.329-5.671 5.634-2.366h6.46v3.08h-6.46q-2.028.0-3.493 1.465t-1.465 3.493z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_menu" xmlns="http://www.w3.org/2000/svg"><path d="M.001 5.334h31.998v3.583H.001V5.334zm0 12.416v-3.5h31.998v3.5H.001zm0 8.916v-3.583h31.998v3.583H.001z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_notification" xmlns="http://www.w3.org/2000/svg"><path d="M22.615 19.384l2.894 2.894v1.413H2.49v-1.413l2.894-2.894V12.25q0-3.365 1.716-5.856t4.745-3.231v-1.01q0-.875.606-1.514T13.999.0t1.548.639.606 1.514v1.01q3.029.74 4.745 3.231t1.716 5.856v7.134zM14 27.999q-1.211.0-2.053-.808t-.841-2.019h5.788q0 1.144-.875 1.986T14 27.999z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_path" xmlns="http://www.w3.org/2000/svg"><path d="M28 12.62h-9.793V8.414h-2.826v11.173h2.826v-4.206H28V26.62h-9.793v-4.206H12.62v-14H9.794v4.206H.001V1.381h9.793v4.206h8.413V1.381H28V12.62z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_person" xmlns="http://www.w3.org/2000/svg"><path d="M16 20.023q5.052.0 10.526 2.199t5.473 5.754v4.023H0v-4.023q0-3.555 5.473-5.754t10.526-2.199zM16 16q-3.275.0-5.614-2.339T8.047 8.047t2.339-5.661T16 0t5.614 2.386 2.339 5.661-2.339 5.614T16 16z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_search" xmlns="http://www.w3.org/2000/svg"><path d="M11.925 20.161q3.432.0 5.834-2.402t2.402-5.834-2.402-5.834-5.834-2.402-5.834 2.402-2.402 5.834 2.402 5.834 5.834 2.402zm10.981.0L32 29.255 29.255 32l-9.094-9.094v-1.458l-.515-.515q-3.26 2.831-7.721 2.831-4.976.0-8.45-3.432T.001 11.925t3.474-8.45 8.45-3.474 8.407 3.474 3.432 8.45q0 1.802-.858 4.075t-1.973 3.646l.515.515h1.458z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_shield" xmlns="http://www.w3.org/2000/svg"><path d="M22.167 15.166V3.5h-8.166v20.726c.93-.492 2.424-1.349 3.883-2.497 1.95-1.531 4.284-3.919 4.284-6.562zm3.499-13.999v14c0 7.674-10.737 12.523-11.192 12.724-.146.073-.31.109-.474.109s-.328-.036-.474-.109c-.456-.201-11.192-5.049-11.192-12.724v-14C2.334.529 2.863.0 3.501.0H24.5c.638.0 1.167.529 1.167 1.167z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_tags" xmlns="http://www.w3.org/2000/svg"><path d="M6.606 7.549c0-1.047-.84-1.887-1.887-1.887s-1.887.84-1.887 1.887.84 1.887 1.887 1.887 1.887-.84 1.887-1.887zm15.732 8.493c0 .501-.206.988-.546 1.327l-7.239 7.254c-.354.339-.84.546-1.342.546s-.988-.206-1.327-.546L1.342 14.066C.59 13.329.0 11.899.0 10.852V4.718a1.9 1.9.0 011.887-1.887h6.134c1.047.0 2.477.59 3.229 1.342L21.792 14.7c.339.354.546.84.546 1.342zm5.661.0c0 .501-.206.988-.546 1.327l-7.239 7.254c-.354.339-.84.546-1.342.546-.767.0-1.15-.354-1.651-.87l6.93-6.93c.339-.339.546-.826.546-1.327s-.206-.988-.546-1.342L13.609 4.173c-.752-.752-2.182-1.342-3.229-1.342h3.303c1.047.0 2.477.59 3.229 1.342L27.454 14.7c.339.354.546.84.546 1.342z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_timer" xmlns="http://www.w3.org/2000/svg"><path d="M16 29q4.428.0 7.536-3.143t3.107-7.571-3.107-7.536T16 7.643 8.464 10.75t-3.107 7.536 3.107 7.571T16 29zM26.714 9.786q1.214 1.571 2.107 4.036t.893 4.464q0 5.643-4 9.678T16 32t-9.714-4.036-4-9.678 4-9.678T16 4.572q1.929.0 4.464.929t4.107 2.143l2.143-2.214q1.143.929 2.143 2.143zM14.5 19.857v-9.143h3v9.143h-3zM20.571.001v3.071h-9.143V.001h9.143z"/></symbol></svg>
<div class=wrapper>
<input type=checkbox class=hidden id=menu-control>
<input type=checkbox class=hidden id=menu-header-control>
<header class=gdoc-header>
<div class="container flex align-center justify-between">
<label for=menu-control class=gdoc-nav__control><svg class="icon gdoc_menu"><title>Open Nav Menu</title><use xlink:href="#gdoc_menu"/></svg><svg class="icon gdoc_arrow_back"><title>Close Nav Menu</title><use xlink:href="#gdoc_arrow_back"/></svg>
</label>
<a class=gdoc-header__link href=https://aquastripe.github.io/RL-notes/>
<span class="gdoc-brand flex align-center">
<img class=gdoc-brand__img src=/RL-notes/brand.svg alt>
<span class=gdoc-brand__title>Reinforcement Learning Notes</span>
</span>
</a>
<div class=gdoc-menu-header>
<span id=gdoc-dark-mode><svg class="icon gdoc_brightness_dark"><title>Toggle Dark/Light/Auto mode</title><use xlink:href="#gdoc_brightness_dark"/></svg><svg class="icon gdoc_brightness_light"><title>Toggle Dark/Light/Auto mode</title><use xlink:href="#gdoc_brightness_light"/></svg><svg class="icon gdoc_brightness_auto"><title>Toggle Dark/Light/Auto mode</title><use xlink:href="#gdoc_brightness_auto"/></svg>
</span>
</div>
</div>
</header>
<main class="container flex flex-even">
<aside class=gdoc-nav>
<nav>
<div class=gdoc-search><svg class="icon gdoc_search"><use xlink:href="#gdoc_search"/></svg>
<input type=text id=gdoc-search-input class=gdoc-search__input placeholder=Search... aria-label=Search maxlength=64>
<div class="gdoc-search__spinner spinner hidden"></div>
<ul id=gdoc-search-results class=gdoc-search__list></ul>
</div>
<section class=gdoc-nav--main>
<h2>Navigation</h2>
<ul class=gdoc-nav__list>
<li>
<span class=flex>Reinforcement Learning: An Introduction</span>
<ul class=gdoc-nav__list>
<li>
<span class=flex>
<a href=/RL-notes/Reinforcement-Learning-An-Introduction/introduction/ class=gdoc-nav__entry>
Ch 1. Introduction
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/ class=gdoc-nav__entry>
Part I: Tabular Solution Methods
</a>
</span>
<ul class=gdoc-nav__list>
<li>
<span class=flex>
<a href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/ class=gdoc-nav__entry>
Ch 2. Multi-Armed Bandits
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/finite-markov-decision-processes/ class=gdoc-nav__entry>
Ch 3. Finite Markov Decision Processes
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/dynamic-programming/ class=gdoc-nav__entry>
Ch 4. Dynamic Programming
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/monte-carlo-methods/ class=gdoc-nav__entry>
Ch 5. Monte Carlo Methods
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/temporal-difference-learning/ class="gdoc-nav__entry is-active">
Ch 6. Temporal-Difference Learning
</a>
</span>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section class=gdoc-nav--more>
<h2>More</h2>
<ul class=gdoc-nav__list>
<li>
<span class=flex><svg class="icon gdoc_github"><use xlink:href="#gdoc_github"/></svg>
<a href=https://github.com/aquastripe/RL-notes class=gdoc-nav__entry>
View Source
</a>
</span>
</li>
</ul>
</section>
</nav>
</aside>
<div class=gdoc-page>
<div class="gdoc-page__header flex flex-wrap
justify-between
hidden-mobile" itemprop=breadcrumb>
<div><svg class="icon gdoc_path hidden-mobile"><use xlink:href="#gdoc_path"/></svg>
<ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList>
<li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemscope itemtype=https://schema.org/WebPage itemprop=item itemid=/RL-notes/ href=/RL-notes/><span itemprop=name>Reinforcement Learning Notes</span></a><meta itemprop=position content="2"></li><li> / </li><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemscope itemtype=https://schema.org/WebPage itemprop=item itemid=/RL-notes/reinforcement-learning-an-introduction/ href=/RL-notes/reinforcement-learning-an-introduction/><span itemprop=name>Reinforcement Learning: An Introduction</span></a><meta itemprop=position content="3"></li><li> / </li><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemscope itemtype=https://schema.org/WebPage itemprop=item itemid=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/ href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/><span itemprop=name>Part I: Tabular Solution Methods</span></a><meta itemprop=position content="4"></li><li> / </li><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Temporal Difference Learning</span><meta itemprop=position content="5"></li>
</ol>
</div>
</div>
<script>MathJax={tex:{inlineMath:[["$","$"]]},displayMath:[["$$","$$"],["[[","]]"]],svg:{fontCache:"global"}}</script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<article class="gdoc-markdown gdoc-markdown__align--left">
<h1>Temporal Difference Learning</h1>
<p>Temporal-difference (TD) learning 是一個 RL 新穎且重要的觀念，它結合 Monte Carlo 和 dynamic programming (DP) 的想法。</p>
<ul>
<li>Monte Carlo: 類似於 Monte Carlo，TD 法可以直接從原始經驗學習，而不需要環境模型。</li>
<li>DP: 類似於 DP，TD 法可以從另一個已學習的估計來更新估計值，而不用等最後結果。</li>
</ul>
<div class="gdoc-toc gdoc-toc__level--6"><nav id=TableOfContents><ul>
<li><a href=#td-prediction>TD Prediction</a></li>
<li><a href=#advantages-of-td-prediction-methods>Advantages of TD Prediction Methods</a></li>
<li><a href=#optimality-of-td0>Optimality of TD(0)</a>
<ul>
<li><a href=#batch-updating>Batch updating</a></li>
</ul>
</li>
<li><a href=#sarsa-on-policy-td-control>Sarsa: On-policy TD Control</a></li>
<li><a href=#q-learning-off-policy-td-control>Q-learning: Off-policy TD Control</a>
<ul>
<li><a href=#example-66-cliff-walking>Example 6.6: Cliff Walking</a></li>
</ul>
</li>
<li><a href=#expected-sarsa>Expected Sarsa</a></li>
</ul></nav><hr></div>
<div class=gdoc-page__anchorwrap>
<h2 id=td-prediction>
TD Prediction
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/temporal-difference-learning/#td-prediction class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor TD Prediction" href=#td-prediction><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>一個 every-visit Monte Carlo 適用於 nonstationary 的環境，value function 更新如下：</p>
<p>$$
V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left[G_{t}-V\left(S_{t}\right)\right]
$$</p>
<ul>
<li>$G_t$: return following $t$</li>
<li>$\alpha$: step-size</li>
</ul>
<p>這個稱為 constant-$\alpha$ MC。Monte Carlo 法必須等到 episode 結束以後才能更新 $V(S_t)$。因此，TD 法概念是希望可以在每個 time step 都能進行更新。最簡單的 TD 形式如下：</p>
<p>$$
V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left[R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)\right]
$$</p>
<p>稱為 TD(0) 或 one-step TD。因為 TD(0) 基於現有的估計值進行更新，所以它是一種 bootstrapping 法，像是 DP。</p>
<p>比較：</p>
<ul>
<li>Monte Carlo: 更新的目標是 $G_t$</li>
<li>TD: 更新的目標是 $R_{t+1} + \gamma V(S_{t+1})$</li>
</ul>
<p><img src=td-0.png alt></p>
<p>$$
\begin{aligned}
v_{\pi}(s) & \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right] \newline
&=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1} \mid S_{t}=s\right] \newline
&=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right]
\end{aligned}
$$</p>
<ul>
<li>Monte Carlo 使用第一式的估計值當作目標。因為</li>
<li>DP 使用第三式的估計值當作目標</li>
</ul>
<p>&mldr; (?)</p>
<p>TD(0) 更新值是一種誤差，這個值稱為 TD error:</p>
<p>$$
\delta_{t} \doteq R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)
$$</p>
<ul>
<li>TD error 每個時刻的當下產生。但 TD error 相依於下次的 state 和 reward，它要到下一個 time step 才會存在。</li>
<li>如果 $V$ 在 episode 期間不會改變，那麼 Monte Carlo error 可以寫成 TD error 的和：</li>
</ul>
<p>$$
\begin{aligned}
G_{t}-V\left(S_{t}\right) &=R_{t+1}+\gamma G_{t+1}-V\left(S_{t}\right)+\gamma V\left(S_{t+1}\right)-\gamma V\left(S_{t+1}\right) \quad(\text { from }(3.9)) \newline
&=\delta_{t}+\gamma\left(G_{t+1}-V\left(S_{t+1}\right)\right) \newline
&=\delta_{t}+\gamma \delta_{t+1}+\gamma^{2}\left(G_{t+2}-V\left(S_{t+2}\right)\right) \newline
&=\delta_{t}+\gamma \delta_{t+1}+\gamma^{2} \delta_{t+2}+\cdots+\gamma^{T-t-1} \delta_{T-1}+\gamma^{T-t}\left(G_{T}-V\left(S_{T}\right)\right) \newline
&=\delta_{t}+\gamma \delta_{t+1}+\gamma^{2} \delta_{t+2}+\cdots+\gamma^{T-t-1} \delta_{T-1}+\gamma^{T-t}(0-0) \newline
&=\sum_{k=t}^{T-1} \gamma^{k-t} \delta_{k}
\end{aligned}
$$</p>
<ul>
<li>如果 $V$ 在 episode 期間會改變，這個等式就不精確，不過 step size 夠小的話還是很接近。</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h2 id=advantages-of-td-prediction-methods>
Advantages of TD Prediction Methods
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/temporal-difference-learning/#advantages-of-td-prediction-methods class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Advantages of TD Prediction Methods" href=#advantages-of-td-prediction-methods><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>TD 與 DP 相比的優勢：</p>
<ul>
<li>不需要環境模型 (model of environment)</li>
</ul>
<p>TD 與 Monte Carlo 相比的優勢：</p>
<ul>
<li>Monte Carlo 必須等待整個 episode 結束以後才能進行計算</li>
</ul>
<p>收歛性：對任意固定的 policy $\pi$，如果某個常數的 step-size 參數足夠的小，TD(0) 平均會收斂到 $v_{\pi}$。如果 step-size 參數根據以下 **隨機近似條件** (stochastic approximation conditions) 進行減少，則收斂的機率是 $1$：</p>
<p>$$
\sum_{n=1}^{\infty} \alpha_{n}(a)=\infty \quad \text { and } \quad \sum_{n=1}^{\infty} \alpha_{n}^{2}(a)&lt;\infty
$$</p>
<p>TD 和 Monte Carlo 都能保證收斂的情況下，哪個收斂得更快？</p>
<ul>
<li>這還是一個開放問題，目前沒有數學方法證明出來。</li>
<li>實務上，在隨機的任務上 TD 法通常比 constant-$\alpha$ MC 收斂得更快。</li>
</ul>
<p><img src=ex-6.2.png alt></p>
<div class=gdoc-page__anchorwrap>
<h2 id=optimality-of-td0>
Optimality of TD(0)
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/temporal-difference-learning/#optimality-of-td0 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Optimality of TD(0)" href=#optimality-of-td0><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><div class=gdoc-page__anchorwrap>
<h3 id=batch-updating>
Batch updating
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/temporal-difference-learning/#batch-updating class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Batch updating" href=#batch-updating><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p>假設在有限個 episodes。這種情況下，有一種常見的訓練策略：<br>
利用 (6.1) or (6.2) 的 value function 更新式先對每個 time step $t$ 進行增量計算，最後把整個結果加總後更新 value function 一次。更新完畢以後，再重複以上的過程直到收斂。這個方法稱為批次更新 (batch updating)。</p>
<ul>
<li>只要 step-size 足夠小，TD(0) 就會收斂到一個確定的解。</li>
<li>然而，constant-$\alpha$ MC 也會確定性的收斂，但有不同的解。</li>
</ul>
<p><img src=6.2.png alt></p>
<p>Figure 6.2 展示了利用 batch updating 進行 Example 6.2 的結果。TD 的結果總是比 MC 更好，因為 TD 更相關於預測 returns。</p>
<p><img src=ex-6.4.png alt></p>
<p>給定以上的 state-rewards，那麼 $V(A)$ 和 $V(B)$ 為何？</p>
<ul>
<li>有些人可能會認同 $V(B)=\frac{3}{4}$，因為 $\frac{6}{8}$ 個 returns 為 $1$，剩下的為 $0$。</li>
<li>$V(A)$ 則有兩種可能：
<ul>
<li>根據觀察，$A$ 總是轉移到 $B$，而 $V(B)=\frac{3}{4}$，因此 $V(A)=V(B)=\frac{3}{4}$。可以把它視為一種 Markov process，結果如下圖。這也是 batch TD(0) 計算的結果。</li>
<li>另一種方式，則是簡單的認為 $A$ 只看過一次且產生出 reward 為 $0$ 的結果，因此 $V(A)=0$。這是 batch MC 的結果。注意到這也會產生最小的訓練誤差，但我們會期望認為第一個答案會更好。如果這個是 Markov process，將會預期在未來的資料產生更小的誤差，而 MC 則會在已知的訓練資料產生較小的誤差。</li>
</ul>
</li>
</ul>
<p><img src=ex-6.4.2.png alt></p>
<p>更進一步說明兩種方法的差異：</p>
<ul>
<li>batch MC: 總是找到最小的訓練誤差。</li>
<li>batch TD(0): 總是找到正確的 <strong>maximum-likelihood</strong>。
<ul>
<li>Maximum-likelihood estimate 的參數，是會使得產生這些資料的機率為最大值的。</li>
<li>Maximum-likelihood estimate 是 Markov process 的模型：
<ul>
<li>預測 從 $i$ 到 $j$ 的轉移機率是觀測到所有從 $i$ 到 $j$ 的轉移的分數 (fraction)</li>
<li>對應的 reward 期望值是這些轉移所產生的 rewards 的平均值。</li>
</ul>
</li>
<li>在這個模型下，如果模型絕對正確，我們就可以計算絕對正確的 value function。</li>
<li>這稱為 <strong>certainty-equivalence estimate</strong> (確定等值估計)。</li>
<li>通常 batch TD(0) 會收練到這個 certainty-equivalence estimate。</li>
</ul>
</li>
</ul>
<p>複雜度：如果有 $n$ 個狀態，</p>
<ul>
<li>形成 maximum-likelihood estimate 需要 $O(n^2)$ 記憶體</li>
<li>計算 value function 需要 $O(n^3)$ 的複雜度。</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h2 id=sarsa-on-policy-td-control>
Sarsa: On-policy TD Control
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/temporal-difference-learning/#sarsa-on-policy-td-control class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Sarsa: On-policy TD Control" href=#sarsa-on-policy-td-control><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>現在來探討如何使用 TD prediction 來解決 control problem.</p>
<p><img src=sequence-of-states-and-state-action-pairs.png alt></p>
<p>確保收歛性的定理也適用於 action values，如下:</p>
<p>$$
Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)-Q\left(S_{t}, A_{t}\right)\right]
$$</p>
<p><img src=backup-diagram-sarsa.png alt></p>
<p>收歛性：</p>
<ul>
<li>只要所有 state-action pairs 都以無限次數經歷過，sarsa 會以 $1$ 的機率收斂到 optimal policy 和 action-value function，並且 policy 收斂到 greedy policy 的極限 (converges in the limit to the greedy policy)。</li>
</ul>
<p><img src=algorithm-sarsa.png alt></p>
<div class=gdoc-page__anchorwrap>
<h2 id=q-learning-off-policy-td-control>
Q-learning: Off-policy TD Control
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/temporal-difference-learning/#q-learning-off-policy-td-control class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Q-learning: Off-policy TD Control" href=#q-learning-off-policy-td-control><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>Q-learning 的更新式定義如下：</p>
<p>$$
Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a\right)-Q\left(S_{t}, A_{t}\right)\right]
$$</p>
<p>要被學習的 action-value function $Q$ 直接近似 optimal action-value function $q_*$，更新過程與 policy 無關。</p>
<ul>
<li>Policy 決定哪個 state-action pairs 要被經歷。</li>
<li>為了要正確的收斂，所有的 pairs 都要連續的被更新。</li>
</ul>
<p><img src=algorithm-q-learning.png alt></p>
<div class=gdoc-page__anchorwrap>
<h3 id=example-66-cliff-walking>
Example 6.6: Cliff Walking
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/temporal-difference-learning/#example-66-cliff-walking class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Example 6.6: Cliff Walking" href=#example-66-cliff-walking><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p><img src=ex-6.6.png alt></p>
<p>Reward:</p>
<ul>
<li>在 The Cliff 時: $-100$</li>
<li>其他: $-1$</li>
</ul>
<p>比較:</p>
<ul>
<li>Sarsa</li>
<li>Q-learning + $\varepsilon$-greedy with $\varepsilon = 0.1$</li>
</ul>
<p>結果：</p>
<ul>
<li>Sarsa: 走了較安全的路，儘管走了較長的路。</li>
<li>Q-learning: 學習到 optimal policy，但是 online performance 較差。</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h2 id=expected-sarsa>
Expected Sarsa
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/temporal-difference-learning/#expected-sarsa class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Expected Sarsa" href=#expected-sarsa><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>把尋找 <strong>最大</strong> 的 value 改為尋找其 <strong>期望值</strong>，如下：</p>
<p>$$
\begin{aligned}
Q\left(S_{t}, A_{t}\right) & \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma \mathbb{E}_{\pi}\left[Q\left(S_{t+1}, A_{t+1}\right) \mid S_{t+1}\right]-Q\left(S_{t}, A_{t}\right)\right] \newline
& \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma \sum_{a} \pi\left(a \mid S_{t+1}\right) Q\left(S_{t+1}, a\right)-Q\left(S_{t}, A_{t}\right)\right]
\end{aligned}
$$</p>
<p>這稱為 <strong>Expected Sarsa</strong>。</p>
<ul>
<li>計算量更大</li>
<li>消除選擇動作的變異性</li>
</ul>
<p><img src=6.3.png alt></p>
<ul>
<li>Asymptotic: $100000$ episodes 並實驗 10 次的平均</li>
<li>Interim: $100$ episodes 並實驗 50000 次的平均</li>
</ul>
<p>討論：</p>
<ul>
<li>在走懸崖的例子，狀態轉移都是確定性的，所有的隨機性來自選擇動作。</li>
<li>因此，Expected Sarsa 可以安全的設定為 $\alpha = 1$ 而不會減少任何效能。</li>
<li>相對來說，Sarsa 只能用在長期且很小的 $\alpha$，效果也比較差。</li>
</ul>
<p><img src=6.4.png alt></p>
</article>
<div class="gdoc-page__footer flex flex-wrap justify-between">
<span class=gdoc-page__nav>
<a class="gdoc-page__nav--prev flex align-center" href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/monte-carlo-methods/ title="Ch 5. Monte Carlo Methods"> Ch 5. Monte Carlo Methods</a>
</span>
<span class=gdoc-page__nav>
</span>
</div>
</div>
</main>
<footer class=gdoc-footer>
<div class="container flex">
<div class="flex flex-wrap">
<span class="gdoc-footer__item gdoc-footer__item--row">
Built with <a href=https://gohugo.io/ class=gdoc-footer__link>Hugo</a> and<svg class="icon gdoc_heart"><use xlink:href="#gdoc_heart"/></svg>
</span>
</div>
<div class="flex flex-25 justify-end">
<span class=gdoc-footer__item>
<a class="gdoc-footer__link fake-link" href=# aria-label="Back to top"><svg class="icon gdoc_keyborad_arrow_up"><use xlink:href="#gdoc_keyborad_arrow_up"/></svg> <span class=hidden-mobile>Back to top</span>
</a>
</span>
</div>
</div>
</footer>
</div>
<script defer src=/RL-notes/js/zh.search.min.62011e6f91033499983c850099ab3e1c2c53d1691525ece5783685e0ddd1c310.js></script>
<script defer src=/RL-notes/js/clipboard-27784b7376.min.js></script>
<script defer src=/RL-notes/js/clipboard-loader-f0b5fbd5f6.min.js></script>
</body>
</html>