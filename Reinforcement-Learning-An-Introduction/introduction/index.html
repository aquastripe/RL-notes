<!doctype html><html lang=zh class=color-toggle-hidden>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=color-scheme content="light dark">
<meta name=description content="Reinforcement Learning (增強式學習) Elements of Reinforcement Learning Policy (策略) Reward signal (獎勵訊號) Value function (價值函數) Model (模型) Limitations and Scope An Extended Example: Tic-Tac-Toe (井字遊戲) Minimax 動態規劃 演化式方法 使用 RL 並配合價值">
<title>Ch 1. Introduction | Reinforcement Learning Notes</title>
<link rel=icon href=/RL-notes/favicon/favicon-32x32.png type=image/x-icon>
<script src=/RL-notes/js/darkmode-ce906ea916.min.js></script>
<link rel=preload as=font href=/RL-notes/fonts/Metropolis.woff2 type=font/woff2 crossorigin=anonymous>
<link rel=preload as=font href=/RL-notes/fonts/LiberationSans.woff2 type=font/woff2 crossorigin=anonymous>
<link rel=preload href=/RL-notes/main-1be624d457.min.css as=style>
<link rel=stylesheet href=/RL-notes/main-1be624d457.min.css media=all>
<link rel=preload href=/RL-notes/mobile-3fc330242c.min.css as=style>
<link rel=stylesheet href=/RL-notes/mobile-3fc330242c.min.css media="screen and (max-width: 45rem)">
<link rel=preload href=/RL-notes/print-f79fc3e5d7.min.css as=style>
<link rel=stylesheet href=/RL-notes/print-f79fc3e5d7.min.css media=print>
<link rel=preload href=/RL-notes/custom.css as=style>
<link rel=stylesheet href=/RL-notes/custom.css media=all>
</head>
<body itemscope itemtype=https://schema.org/WebPage><svg class="svg-sprite" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_back" xmlns="http://www.w3.org/2000/svg"><path d="M24 10.526v2.947H5.755l8.351 8.421-2.105 2.105-12-12 12-12 2.105 2.105-8.351 8.421H24z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_left_alt" xmlns="http://www.w3.org/2000/svg"><path d="M5.965 10.526V6.035L0 12l5.965 5.965v-4.491H24v-2.947H5.965z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_right_alt" xmlns="http://www.w3.org/2000/svg"><path d="M18.035 10.526V6.035L24 12l-5.965 5.965v-4.491H0v-2.947h18.035z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_bitbucket" xmlns="http://www.w3.org/2000/svg"><path d="M15.905 13.355c.189 1.444-1.564 2.578-2.784 1.839-1.375-.602-1.375-2.784-.034-3.403 1.151-.705 2.818.223 2.818 1.564zm1.907-.361c-.309-2.44-3.076-4.056-5.328-3.042-1.426.636-2.389 2.148-2.32 3.747.086 2.097 2.08 3.815 4.176 3.626s3.729-2.234 3.472-4.331zm4.108-9.315c-.756-.997-2.045-1.169-3.179-1.358-3.214-.516-6.513-.533-9.727.034-1.066.172-2.269.361-2.939 1.323 1.1 1.031 2.664 1.186 4.073 1.358 2.544.327 5.156.344 7.699.017 1.426-.172 3.008-.309 4.073-1.375zm.979 17.788c-.481 1.684-.206 3.953-1.994 4.932-3.076 1.701-6.806 1.89-10.191 1.289-1.787-.327-3.884-.894-4.864-2.578-.43-1.65-.705-3.334-.98-5.018l.103-.275.309-.155c5.121 3.386 12.288 3.386 17.427.0.808.241.206 1.22.189 1.805zM26.01 4.951c-.584 3.764-1.255 7.51-1.908 11.257-.189 1.1-1.255 1.719-2.148 2.183-3.214 1.615-6.96 1.89-10.483 1.512-2.389-.258-4.829-.894-6.771-2.389-.911-.705-.911-1.908-1.083-2.922-.602-3.523-1.289-7.046-1.719-10.604.206-1.547 1.942-2.217 3.231-2.698C6.848.654 8.686.362 10.508.19c3.884-.378 7.854-.241 11.618.859 1.341.395 2.784.945 3.695 2.097.412.533.275 1.203.189 1.805z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_bookmark" xmlns="http://www.w3.org/2000/svg"><path d="M15.268 4.392q.868.0 1.532.638t.664 1.506v17.463l-7.659-3.268-7.608 3.268V6.536q0-.868.664-1.506t1.532-.638h10.876zm4.34 14.144V4.392q0-.868-.638-1.532t-1.506-.664H6.537q0-.868.664-1.532T8.733.0h10.876q.868.0 1.532.664t.664 1.532v17.412z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_auto" xmlns="http://www.w3.org/2000/svg"><path d="M16.846 18.938h2.382L15.22 7.785h-2.44L8.772 18.938h2.382l.871-2.44h3.95zm7.087-9.062L27.999 14l-4.066 4.124v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809zm-11.385 4.937L14 10.282l1.452 4.531h-2.904z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_dark" xmlns="http://www.w3.org/2000/svg"><path d="M14 21.435q3.079.0 5.257-2.178T21.435 14t-2.178-5.257T14 6.565q-1.51.0-3.079.697 1.917.871 3.108 2.701T15.22 14t-1.191 4.037-3.108 2.701q1.568.697 3.079.697zm9.933-11.559L27.999 14l-4.066 4.124v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_light" xmlns="http://www.w3.org/2000/svg"><path d="M14 21.435q3.079.0 5.257-2.178T21.435 14t-2.178-5.257T14 6.565 8.743 8.743 6.565 14t2.178 5.257T14 21.435zm9.933-3.311v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809L27.999 14z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_cloud_off" xmlns="http://www.w3.org/2000/svg"><path d="M9.023 10.5H7q-1.914.0-3.281 1.395t-1.367 3.309 1.367 3.281T7 19.852h11.375zM3.5 4.976l1.477-1.477L24.5 23.022l-1.477 1.477-2.352-2.297H6.999q-2.898.0-4.949-2.051t-2.051-4.949q0-2.844 1.969-4.867t4.758-2.133zm19.086 5.578q2.242.164 3.828 1.832T28 16.351q0 3.008-2.461 4.758l-1.695-1.695q1.805-.984 1.805-3.063.0-1.422-1.039-2.461t-2.461-1.039h-1.75v-.602q0-2.68-1.859-4.539t-4.539-1.859q-1.531.0-2.953.711l-1.75-1.695Q11.431 3.5 14.001 3.5q2.953.0 5.496 2.078t3.09 4.977z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_code" xmlns="http://www.w3.org/2000/svg"><path d="M9.917 24.5a1.75 1.75.0 10-3.501.001 1.75 1.75.0 003.501-.001zm0-21a1.75 1.75.0 10-3.501.001A1.75 1.75.0 009.917 3.5zm11.666 2.333a1.75 1.75.0 10-3.501.001 1.75 1.75.0 003.501-.001zm1.75.0a3.502 3.502.0 01-1.75 3.026c-.055 6.581-4.721 8.039-7.82 9.023-2.898.911-3.846 1.349-3.846 3.117v.474a3.502 3.502.0 011.75 3.026c0 1.932-1.568 3.5-3.5 3.5s-3.5-1.568-3.5-3.5c0-1.294.711-2.424 1.75-3.026V6.526A3.502 3.502.0 014.667 3.5c0-1.932 1.568-3.5 3.5-3.5s3.5 1.568 3.5 3.5a3.502 3.502.0 01-1.75 3.026v9.06c.93-.456 1.914-.766 2.807-1.039 3.391-1.075 5.323-1.878 5.359-5.687a3.502 3.502.0 01-1.75-3.026c0-1.932 1.568-3.5 3.5-3.5s3.5 1.568 3.5 3.5z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_date" xmlns="http://www.w3.org/2000/svg"><path d="M27.192 28.844V11.192H4.808v17.652h22.384zm0-25.689q1.277.0 2.253.976t.976 2.253v22.459q0 1.277-.976 2.216t-2.253.939H4.808q-1.352.0-2.291-.901t-.939-2.253V6.385q0-1.277.939-2.253t2.291-.976h1.577V.001h3.23v3.155h12.769V.001h3.23v3.155h1.577zm-3.155 11.267v3.155h-3.23v-3.155h3.23zm-6.46.0v3.155h-3.155v-3.155h3.155zm-6.384.0v3.155h-3.23v-3.155h3.23z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_download" xmlns="http://www.w3.org/2000/svg"><path d="M2.866 28.209h26.269v3.79H2.866v-3.79zm26.268-16.925L16 24.418 2.866 11.284h7.493V.001h11.283v11.283h7.493z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_email" xmlns="http://www.w3.org/2000/svg"><path d="M28.845 9.615v-3.23L16 14.422 3.155 6.385v3.23L16 17.577zm0-6.46q1.277.0 2.216.977T32 6.385v19.23q0 1.277-.939 2.253t-2.216.977H3.155q-1.277.0-2.216-.977T0 25.615V6.385q0-1.277.939-2.253t2.216-.977h25.69z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_git" xmlns="http://www.w3.org/2000/svg"><path d="M27.472 12.753 15.247.529a1.803 1.803.0 00-2.55.0l-2.84 2.84 2.137 2.137a2.625 2.625.0 013.501 3.501l3.499 3.499a2.625 2.625.0 11-1.237 1.237l-3.499-3.499c-.083.04-.169.075-.257.106v7.3a2.626 2.626.0 11-1.75.0v-7.3a2.626 2.626.0 01-1.494-3.607L8.62 4.606l-8.09 8.09a1.805 1.805.0 000 2.551l12.225 12.224a1.803 1.803.0 002.55.0l12.168-12.168a1.805 1.805.0 000-2.551z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_github" xmlns="http://www.w3.org/2000/svg"><path d="M16 .394c8.833.0 15.999 7.166 15.999 15.999.0 7.062-4.583 13.062-10.937 15.187-.813.146-1.104-.354-1.104-.771.0-.521.021-2.25.021-4.396.0-1.5-.5-2.458-1.083-2.958 3.562-.396 7.312-1.75 7.312-7.896.0-1.75-.625-3.167-1.646-4.291.167-.417.708-2.042-.167-4.25-1.333-.417-4.396 1.646-4.396 1.646a15.032 15.032.0 00-8 0S8.937 6.602 7.603 7.018c-.875 2.208-.333 3.833-.167 4.25-1.021 1.125-1.646 2.542-1.646 4.291.0 6.125 3.729 7.5 7.291 7.896-.458.417-.875 1.125-1.021 2.146-.917.417-3.25 1.125-4.646-1.333-.875-1.521-2.458-1.646-2.458-1.646-1.562-.021-.104.979-.104.979 1.042.479 1.771 2.333 1.771 2.333.938 2.854 5.396 1.896 5.396 1.896.0 1.333.021 2.583.021 2.979.0.417-.292.917-1.104.771C4.582 29.455-.001 23.455-.001 16.393-.001 7.56 7.165.394 15.998.394zM6.063 23.372c.042-.083-.021-.187-.146-.25-.125-.042-.229-.021-.271.042-.042.083.021.187.146.25.104.062.229.042.271-.042zm.646.709c.083-.062.062-.208-.042-.333-.104-.104-.25-.146-.333-.062-.083.062-.062.208.042.333.104.104.25.146.333.062zm.625.937c.104-.083.104-.25.0-.396-.083-.146-.25-.208-.354-.125-.104.062-.104.229.0.375s.271.208.354.146zm.875.875c.083-.083.042-.271-.083-.396-.146-.146-.333-.167-.417-.062-.104.083-.062.271.083.396.146.146.333.167.417.062zm1.187.521c.042-.125-.083-.271-.271-.333-.167-.042-.354.021-.396.146s.083.271.271.312c.167.062.354.0.396-.125zm1.313.104c0-.146-.167-.25-.354-.229-.187.0-.333.104-.333.229.0.146.146.25.354.229.187.0.333-.104.333-.229zm1.208-.208c-.021-.125-.187-.208-.375-.187-.187.042-.312.167-.292.312.021.125.187.208.375.167s.312-.167.292-.292z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_gitlab" xmlns="http://www.w3.org/2000/svg"><path d="M1.629 11.034 14 26.888.442 17.048a1.09 1.09.0 01-.39-1.203l1.578-4.811zm7.217.0h10.309l-5.154 15.854zM5.753 1.475l3.093 9.559H1.63l3.093-9.559a.548.548.0 011.031.0zm20.618 9.559 1.578 4.811c.141.437-.016.922-.39 1.203l-13.558 9.84 12.371-15.854zm0 0h-7.216l3.093-9.559a.548.548.0 011.031.0z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_heart" xmlns="http://www.w3.org/2000/svg"><path d="M16 29.714a1.11 1.11.0 01-.786-.321L4.072 18.643c-.143-.125-4.071-3.714-4.071-8 0-5.232 3.196-8.357 8.535-8.357 3.125.0 6.053 2.464 7.464 3.857 1.411-1.393 4.339-3.857 7.464-3.857 5.339.0 8.535 3.125 8.535 8.357.0 4.286-3.928 7.875-4.089 8.035L16.785 29.392c-.214.214-.5.321-.786.321z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_home" xmlns="http://www.w3.org/2000/svg"><path d="M24.003 15.695v8.336c0 .608-.504 1.111-1.111 1.111h-6.669v-6.669h-4.446v6.669H5.108a1.119 1.119.0 01-1.111-1.111v-8.336c0-.035.017-.069.017-.104L14 7.359l9.986 8.232a.224.224.0 01.017.104zm3.873-1.198-1.077 1.285a.578.578.0 01-.365.191h-.052a.547.547.0 01-.365-.122L14 5.831 1.983 15.851a.594.594.0 01-.417.122.578.578.0 01-.365-.191L.124 14.497a.57.57.0 01.069-.781L12.679 3.314c.729-.608 1.91-.608 2.64.0l4.237 3.543V3.471c0-.313.243-.556.556-.556h3.334c.313.0.556.243.556.556v7.085l3.803 3.161c.226.191.26.556.069.781z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_keyborad_arrow_down" xmlns="http://www.w3.org/2000/svg"><path d="M3.281 5.36 14 16.079 24.719 5.36 28 8.641l-14 14-14-14z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_keyborad_arrow_left" xmlns="http://www.w3.org/2000/svg"><path d="M25.875 28.25 22.125 32 6.126 16.001 22.125.002l3.75 3.75-12.25 12.25z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_keyborad_arrow_right" xmlns="http://www.w3.org/2000/svg"><path d="M6.125 28.25 18.375 16 6.125 3.75 9.875.0l15.999 15.999L9.875 31.998z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_keyborad_arrow_up" xmlns="http://www.w3.org/2000/svg"><path d="M24.719 22.64 14 11.921 3.281 22.64.0 19.359l14-14 14 14z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_link" xmlns="http://www.w3.org/2000/svg"><path d="M24.037 7.963q3.305.0 5.634 2.366T32 16t-2.329 5.671-5.634 2.366h-6.46v-3.08h6.46q2.028.0 3.493-1.465t1.465-3.493-1.465-3.493-3.493-1.465h-6.46v-3.08h6.46zM9.615 17.578v-3.155h12.77v3.155H9.615zM3.005 16q0 2.028 1.465 3.493t3.493 1.465h6.46v3.08h-6.46q-3.305.0-5.634-2.366T0 16.001t2.329-5.671 5.634-2.366h6.46v3.08h-6.46q-2.028.0-3.493 1.465t-1.465 3.493z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_menu" xmlns="http://www.w3.org/2000/svg"><path d="M.001 5.334h31.998v3.583H.001V5.334zm0 12.416v-3.5h31.998v3.5H.001zm0 8.916v-3.583h31.998v3.583H.001z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_notification" xmlns="http://www.w3.org/2000/svg"><path d="M22.615 19.384l2.894 2.894v1.413H2.49v-1.413l2.894-2.894V12.25q0-3.365 1.716-5.856t4.745-3.231v-1.01q0-.875.606-1.514T13.999.0t1.548.639.606 1.514v1.01q3.029.74 4.745 3.231t1.716 5.856v7.134zM14 27.999q-1.211.0-2.053-.808t-.841-2.019h5.788q0 1.144-.875 1.986T14 27.999z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_path" xmlns="http://www.w3.org/2000/svg"><path d="M28 12.62h-9.793V8.414h-2.826v11.173h2.826v-4.206H28V26.62h-9.793v-4.206H12.62v-14H9.794v4.206H.001V1.381h9.793v4.206h8.413V1.381H28V12.62z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_person" xmlns="http://www.w3.org/2000/svg"><path d="M16 20.023q5.052.0 10.526 2.199t5.473 5.754v4.023H0v-4.023q0-3.555 5.473-5.754t10.526-2.199zM16 16q-3.275.0-5.614-2.339T8.047 8.047t2.339-5.661T16 0t5.614 2.386 2.339 5.661-2.339 5.614T16 16z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_search" xmlns="http://www.w3.org/2000/svg"><path d="M11.925 20.161q3.432.0 5.834-2.402t2.402-5.834-2.402-5.834-5.834-2.402-5.834 2.402-2.402 5.834 2.402 5.834 5.834 2.402zm10.981.0L32 29.255 29.255 32l-9.094-9.094v-1.458l-.515-.515q-3.26 2.831-7.721 2.831-4.976.0-8.45-3.432T.001 11.925t3.474-8.45 8.45-3.474 8.407 3.474 3.432 8.45q0 1.802-.858 4.075t-1.973 3.646l.515.515h1.458z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_shield" xmlns="http://www.w3.org/2000/svg"><path d="M22.167 15.166V3.5h-8.166v20.726c.93-.492 2.424-1.349 3.883-2.497 1.95-1.531 4.284-3.919 4.284-6.562zm3.499-13.999v14c0 7.674-10.737 12.523-11.192 12.724-.146.073-.31.109-.474.109s-.328-.036-.474-.109c-.456-.201-11.192-5.049-11.192-12.724v-14C2.334.529 2.863.0 3.501.0H24.5c.638.0 1.167.529 1.167 1.167z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_tags" xmlns="http://www.w3.org/2000/svg"><path d="M6.606 7.549c0-1.047-.84-1.887-1.887-1.887s-1.887.84-1.887 1.887.84 1.887 1.887 1.887 1.887-.84 1.887-1.887zm15.732 8.493c0 .501-.206.988-.546 1.327l-7.239 7.254c-.354.339-.84.546-1.342.546s-.988-.206-1.327-.546L1.342 14.066C.59 13.329.0 11.899.0 10.852V4.718a1.9 1.9.0 011.887-1.887h6.134c1.047.0 2.477.59 3.229 1.342L21.792 14.7c.339.354.546.84.546 1.342zm5.661.0c0 .501-.206.988-.546 1.327l-7.239 7.254c-.354.339-.84.546-1.342.546-.767.0-1.15-.354-1.651-.87l6.93-6.93c.339-.339.546-.826.546-1.327s-.206-.988-.546-1.342L13.609 4.173c-.752-.752-2.182-1.342-3.229-1.342h3.303c1.047.0 2.477.59 3.229 1.342L27.454 14.7c.339.354.546.84.546 1.342z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_timer" xmlns="http://www.w3.org/2000/svg"><path d="M16 29q4.428.0 7.536-3.143t3.107-7.571-3.107-7.536T16 7.643 8.464 10.75t-3.107 7.536 3.107 7.571T16 29zM26.714 9.786q1.214 1.571 2.107 4.036t.893 4.464q0 5.643-4 9.678T16 32t-9.714-4.036-4-9.678 4-9.678T16 4.572q1.929.0 4.464.929t4.107 2.143l2.143-2.214q1.143.929 2.143 2.143zM14.5 19.857v-9.143h3v9.143h-3zM20.571.001v3.071h-9.143V.001h9.143z"/></symbol></svg>
<div class=wrapper>
<input type=checkbox class=hidden id=menu-control>
<input type=checkbox class=hidden id=menu-header-control>
<header class=gdoc-header>
<div class="container flex align-center justify-between">
<label for=menu-control class=gdoc-nav__control><svg class="icon gdoc_menu"><title>Open Nav Menu</title><use xlink:href="#gdoc_menu"/></svg><svg class="icon gdoc_arrow_back"><title>Close Nav Menu</title><use xlink:href="#gdoc_arrow_back"/></svg>
</label>
<a class=gdoc-header__link href=https://aquastripe.github.io/RL-notes/>
<span class="gdoc-brand flex align-center">
<img class=gdoc-brand__img src=/RL-notes/brand.svg alt>
<span class=gdoc-brand__title>Reinforcement Learning Notes</span>
</span>
</a>
<div class=gdoc-menu-header>
<span id=gdoc-dark-mode><svg class="icon gdoc_brightness_dark"><title>Toggle Dark/Light/Auto mode</title><use xlink:href="#gdoc_brightness_dark"/></svg><svg class="icon gdoc_brightness_light"><title>Toggle Dark/Light/Auto mode</title><use xlink:href="#gdoc_brightness_light"/></svg><svg class="icon gdoc_brightness_auto"><title>Toggle Dark/Light/Auto mode</title><use xlink:href="#gdoc_brightness_auto"/></svg>
</span>
</div>
</div>
</header>
<main class="container flex flex-even">
<aside class=gdoc-nav>
<nav>
<div class=gdoc-search><svg class="icon gdoc_search"><use xlink:href="#gdoc_search"/></svg>
<input type=text id=gdoc-search-input class=gdoc-search__input placeholder=Search... aria-label=Search maxlength=64>
<div class="gdoc-search__spinner spinner hidden"></div>
<ul id=gdoc-search-results class=gdoc-search__list></ul>
</div>
<section class=gdoc-nav--main>
<h2>Navigation</h2>
<ul class=gdoc-nav__list>
<li>
<span class=flex>Reinforcement Learning: An Introduction</span>
<ul class=gdoc-nav__list>
<li>
<span class=flex>
<a href=/RL-notes/Reinforcement-Learning-An-Introduction/introduction/ class="gdoc-nav__entry is-active">
Ch 1. Introduction
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/ class=gdoc-nav__entry>
Part I: Tabular Solution Methods
</a>
</span>
<ul class=gdoc-nav__list>
<li>
<span class=flex>
<a href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/ class=gdoc-nav__entry>
Ch 2. Multi-Armed Bandits
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/finite-markov-decision-processes/ class=gdoc-nav__entry>
Ch 3. Finite Markov Decision Processes
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/dynamic-programming/ class=gdoc-nav__entry>
Ch 4. Dynamic Programming
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/monte-carlo-methods/ class=gdoc-nav__entry>
Ch 5. Monte Carlo Methods
</a>
</span>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section class=gdoc-nav--more>
<h2>More</h2>
<ul class=gdoc-nav__list>
<li>
<span class=flex><svg class="icon gdoc_github"><use xlink:href="#gdoc_github"/></svg>
<a href=https://github.com/aquastripe/RL-notes class=gdoc-nav__entry>
View Source
</a>
</span>
</li>
</ul>
</section>
</nav>
</aside>
<div class=gdoc-page>
<div class="gdoc-page__header flex flex-wrap
justify-between
hidden-mobile" itemprop=breadcrumb>
<div><svg class="icon gdoc_path hidden-mobile"><use xlink:href="#gdoc_path"/></svg>
<ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList>
<li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemscope itemtype=https://schema.org/WebPage itemprop=item itemid=/RL-notes/ href=/RL-notes/><span itemprop=name>Reinforcement Learning Notes</span></a><meta itemprop=position content="2"></li><li> / </li><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemscope itemtype=https://schema.org/WebPage itemprop=item itemid=/RL-notes/reinforcement-learning-an-introduction/ href=/RL-notes/reinforcement-learning-an-introduction/><span itemprop=name>Reinforcement Learning: An Introduction</span></a><meta itemprop=position content="3"></li><li> / </li><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Ch 1. Introduction</span><meta itemprop=position content="4"></li>
</ol>
</div>
</div>
<script>MathJax={tex:{inlineMath:[["$","$"]]},displayMath:[["$$","$$"],["[[","]]"]],svg:{fontCache:"global"}}</script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<article class="gdoc-markdown gdoc-markdown__align--left">
<h1>Ch 1. Introduction</h1>
<div class="gdoc-toc gdoc-toc__level--6"><nav id=TableOfContents><ul>
<li><a href=#reinforcement-learning-增強式學習>Reinforcement Learning (增強式學習)</a></li>
<li><a href=#elements-of-reinforcement-learning>Elements of Reinforcement Learning</a>
<ul>
<li><a href=#policy-策略>Policy (策略)</a></li>
<li><a href=#reward-signal-獎勵訊號>Reward signal (獎勵訊號)</a></li>
<li><a href=#value-function-價值函數>Value function (價值函數)</a></li>
<li><a href=#model-模型>Model (模型)</a></li>
</ul>
</li>
<li><a href=#limitations-and-scope>Limitations and Scope</a></li>
<li><a href=#an-extended-example-tic-tac-toe-井字遊戲>An Extended Example: Tic-Tac-Toe (井字遊戲)</a>
<ul>
<li><a href=#minimax>Minimax</a></li>
<li><a href=#動態規劃>動態規劃</a></li>
<li><a href=#演化式方法>演化式方法</a></li>
<li><a href=#使用-rl-並配合價值函數>使用 RL 並配合價值函數</a></li>
</ul>
</li>
</ul></nav><hr></div>
<div class=gdoc-page__anchorwrap>
<h2 id=reinforcement-learning-增強式學習>
Reinforcement Learning (增強式學習)
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/introduction/#reinforcement-learning-增強式學習 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Reinforcement Learning (增強式學習)" href=#reinforcement-learning-%e5%a2%9e%e5%bc%b7%e5%bc%8f%e5%ad%b8%e7%bf%92><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>RL 的目標：學習怎麼在給定狀態下，輸出可以得到最大的獎勵 (reward) 的動作 (action)。</p>
<p>RL 最特別的性質：</p>
<ul>
<li>trial-and-error search</li>
<li>delayed reward</li>
</ul>
<p>Reinforcement learning 和其他 -ing 結尾的主題類似 (e.g. machine learning)，同時是個問題也是解決問題的方法。區分問題和解法在 RL 非常重要，搞不清楚時常會造成困惑。</p>
<p>RL 借用動態系統理論 (dynamical systems theroy) 的馬可夫決策過程 (Markov decision processes) 的觀念來形式化。基本觀念：一個 <strong>學習代理</strong> (learning agent) 會隨著 <strong>時間</strong> 與 <strong>環境</strong> (environment) 互動來達成一個 <strong>目標</strong> (goal)。學習代理必須可以：</p>
<ul>
<li>感測環境的 <strong>狀態</strong> (state)</li>
<li>採取 <strong>動作</strong> (action) 來影響環境的狀態</li>
</ul>
<p>馬可夫決策過程以三個觀點提供最簡化的形式涵蓋這個問題：</p>
<ul>
<li>sensation</li>
<li>action</li>
<li>goal</li>
</ul>
<p>機器學習三大類：</p>
<ul>
<li>supervised learning (監督式學習)</li>
<li>unsupervised learning (非監督式學習)</li>
<li>增強式學習</li>
</ul>
<p>不同之處：</p>
<ul>
<li>RL 與 supervised learning 的不同：（略）</li>
<li>RL 與 unsupervised learning 的不同：（略）</li>
</ul>
<p>RL 必須在 exploration (探索) 和 exploitation (利用) 之間取捨：</p>
<ul>
<li>利用：為了要獲得大量的獎勵，RL 代理必須偏好選擇「過去嘗試過最好的」動作</li>
<li>探索：為了做到這件事情，必須探索沒有嘗試過的動作</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h2 id=elements-of-reinforcement-learning>
Elements of Reinforcement Learning
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/introduction/#elements-of-reinforcement-learning class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Elements of Reinforcement Learning" href=#elements-of-reinforcement-learning><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>RL 的基本組成：</p>
<ul>
<li>the agent (智慧主體，主動進行動作並影響環境狀態的主體)</li>
<li>the environment (環境)</li>
<li>RL 系統的子元素:
<ul>
<li>a policy (策略)</li>
<li>a reward signal (獎勵訊號)</li>
<li>a value function (價值函數)</li>
<li>(optional) a model of the environment (環境的模型)</li>
</ul>
</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h3 id=policy-策略>
Policy (策略)
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/introduction/#policy-策略 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Policy (策略)" href=#policy-%e7%ad%96%e7%95%a5><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p>策略定義了智慧主體的行為。在給定一個時間點，智慧主體從環境接收狀態，並選擇動作來改變環境。而<strong>策略</strong>是狀態到動作的映射函數。</p>
<ul>
<li>可能是簡單的函數或是查表，也可能是會涉及複雜計算的演算法</li>
<li>可能是隨機性的，提供採取每個動作的機率</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h3 id=reward-signal-獎勵訊號>
Reward signal (獎勵訊號)
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/introduction/#reward-signal-獎勵訊號 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Reward signal (獎勵訊號)" href=#reward-signal-%e7%8d%8e%e5%8b%b5%e8%a8%8a%e8%99%9f><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p>獎勵訊號定義了 RL 問題的目標。在每個時間點，環境會傳送單一數值的訊號給 RL 智慧主體，稱為 <strong>獎勵</strong>。獎勵給學習代理定義事件的好壞。</p>
<ul>
<li>可能是一個隨機性的函數，根據環境的狀態和採取的動作</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h3 id=value-function-價值函數>
Value function (價值函數)
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/introduction/#value-function-價值函數 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Value function (價值函數)" href=#value-function-%e5%83%b9%e5%80%bc%e5%87%bd%e6%95%b8><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p>一個狀態的 <strong>價值</strong> 是智慧主體預期未來會在這個狀態下取得多少獎勵的總和。</p>
<ul>
<li>以最大的價值而非最大的獎勵來採取動作。</li>
<li>獎勵是立即的回饋，價值函數是長期的</li>
<li>獎勵是主要的，價值函數是次要的。
<ul>
<li>沒有獎勵就沒有價值，估計價值的唯一目的是獲得更多的獎勵。</li>
</ul>
</li>
<li>決定價值比獎勵更難
<ul>
<li>獎勵通常可以直接由環境取得</li>
<li>價值必須估計、以及來自一個生命週期觀測的結果來重新估計</li>
</ul>
</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h3 id=model-模型>
Model (模型)
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/introduction/#model-模型 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Model (模型)" href=#model-%e6%a8%a1%e5%9e%8b><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p><strong>模型</strong> 用來模擬環境的行為。例如：給定狀態和動作，模型要預測下個狀態和下個獎勵</p>
<p>模型是用來 <em>planning</em> (規劃)</p>
<ul>
<li>model-based</li>
<li>model-free
<ul>
<li>trial-and-error</li>
</ul>
</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h2 id=limitations-and-scope>
Limitations and Scope
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/introduction/#limitations-and-scope class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Limitations and Scope" href=#limitations-and-scope><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><ul>
<li>極度依賴狀態：作為輸入給策略、價值函數和模型，以及來自模型的輸出。
<ul>
<li>本書探討的問題是假設狀態已經被良好的處理過，可以直接使用。</li>
<li>為了專注在討論決策問題，不考慮如何設計狀態的訊號的問題。</li>
</ul>
</li>
<li>大部分 RL 的方法都圍繞在如何估計價值函數，但這非 RL 的必要條件。不涉及估計價值函數的方法的例子：
<ul>
<li>基因演算法 (genetic algorithms)</li>
<li>基因規劃 (genetic programming)</li>
<li>模擬退火法 (simulated annealing)</li>
<li>以上是演化式 (evolutionary) 方法，在生命週期中不學習，而是在下個世代產生具備能力的個體。</li>
<li>什麼情況下演化式方法會有優勢：
<ul>
<li>如果策略空間很小，或是容易被找到、有足夠多的時間搜尋</li>
<li>當無法從環境中感測出完整的狀態</li>
</ul>
</li>
</ul>
</li>
<li>本書專注在環境互動中學習的方法，不包含演化式方法。</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h2 id=an-extended-example-tic-tac-toe-井字遊戲>
An Extended Example: Tic-Tac-Toe (井字遊戲)
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/introduction/#an-extended-example-tic-tac-toe-井字遊戲 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor An Extended Example: Tic-Tac-Toe (井字遊戲)" href=#an-extended-example-tic-tac-toe-%e4%ba%95%e5%ad%97%e9%81%8a%e6%88%b2><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>井字遊戲是個簡單的問題，但沒有辦法由經典的演算法來適當的解決。以下舉幾個例子來說明：</p>
<ul>
<li>使用 minimax</li>
<li>使用動態規劃</li>
<li>使用演化式方法</li>
<li>使用 RL 並配合價值函數</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h3 id=minimax>
Minimax
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/introduction/#minimax class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Minimax" href=#minimax><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><ul>
<li>這個方法假設了對手的遊戲策略</li>
<li>讓玩家無法到達一個「可能會輸掉、但實際上對手可能會失誤而因此勝利」的狀態。</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h3 id=動態規劃>
動態規劃
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/introduction/#動態規劃 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor 動態規劃" href=#%e5%8b%95%e6%85%8b%e8%a6%8f%e5%8a%83><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><ul>
<li>需要關於對手的完整規格，包含在每個狀態下會以何種機率採取行動。通常這種資訊是不會先驗的 (a prior) 獲得，大部分實務也不會有。</li>
<li>有一種方式是學習模型來模擬對手的行為，再根據模型來計算動態規劃求出最佳解。最後，這個方法和某些 RL 的方法並無不同。</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h3 id=演化式方法>
演化式方法
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/introduction/#演化式方法 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor 演化式方法" href=#%e6%bc%94%e5%8c%96%e5%bc%8f%e6%96%b9%e6%b3%95><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><ul>
<li>直接搜尋所有可能的策略，找出一個有高機率獲勝的方法。</li>
<li>此處的策略是一個規則來告訴玩家要下哪一步，對遊戲所有可能狀態 — 每個可能的 O 或 X 的設定 (configuration)</li>
<li>對每一個策略，藉由大量的對戰來估計勝率。</li>
<li>藉由估計值來決定下一次的策略。</li>
<li>可能會用的經典演化式演算法：
<ul>
<li>爬坡演算法 (hill-climbing algorithm)，會連續的生成和估計策略</li>
<li>基因演算法類</li>
</ul>
</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h3 id=使用-rl-並配合價值函數>
使用 RL 並配合價值函數
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/Reinforcement-Learning-An-Introduction/introduction/#使用-rl-並配合價值函數 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor 使用 RL 並配合價值函數" href=#%e4%bd%bf%e7%94%a8-rl-%e4%b8%a6%e9%85%8d%e5%90%88%e5%83%b9%e5%80%bc%e5%87%bd%e6%95%b8><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p>步驟如下：</p>
<ol>
<li>設定數值表 (table of numbers)，每一個代表這場遊戲中可能的狀態，每一個數值是當前狀態下勝率的最新估計值。</li>
<li>把這個估計值當作這個狀態的價值，整個表就是學習到的價值函數。</li>
<li>對於所有三個連一直線的狀態的勝率是 1 (或是 0，被對手成功連線時) 。</li>
<li>所有其他狀態的勝率值都初始化為 0.5。</li>
<li>接下來開始對戰很多次。</li>
<li>選擇要下的點，大部分情況下可以使用貪心法挑選最大的價值，也就是最高的勝率。</li>
<li>少部份的情況下選擇其他沒有下過的落點，這稱之為 <strong>探索</strong>，讓我們可以看過沒見過的狀態。</li>
<li>在遊戲過程中會更新價值函數，使估計的更準確。方式是在每一步過後回補 (back up) 前一步的狀態的價值。</li>
<li>假設當前狀態是 $S_{t}$，經過一個貪心選擇過後下一步是 $S_{t+1}$，這時會更新 $S_{t}$ 的價值函數，標記為 $V(S_{t})$。可以被列式如下：
<ul>
<li>$V(S_{t}) \leftarrow V(S_{t}) + \alpha [ V(S_{t+1}) - V(S_{t})]$
<ul>
<li>$\alpha$: 步長參數 (<em>step-size parameter</em>)</li>
<li>這是一個 <em>temporal-difference</em> 學習法的例子</li>
</ul>
</li>
</ul>
</li>
</ol>
<div class="flex justify-center">
<figure class=gdoc-markdown__figure>
<a class=gdoc-markdown__link--raw href=/RL-notes/Reinforcement-Learning-An-Introduction/introduction/1.1.png>
<picture>
<source srcset=/RL-notes/Reinforcement-Learning-An-Introduction/introduction/1.1_hudf7d0fe0e09c47f203ab9db59b3473b7_67732_1200x0_resize_box_3.png>
<img loading=lazy src=/RL-notes/Reinforcement-Learning-An-Introduction/introduction/1.1_hudf7d0fe0e09c47f203ab9db59b3473b7_67732_1800x0_resize_box_3.png alt="每個實心點代表狀態，實線是狀態轉移的路徑，虛線是可以轉移的路徑 (輪到自己時有多個動作可以選擇) ，* 代表最大價值的狀態。">
</picture>
</a>
<figcaption>每個實心點代表狀態，實線是狀態轉移的路徑，虛線是可以轉移的路徑 (輪到自己時有多個動作可以選擇) ，* 代表最大價值的狀態。 (<a class=gdoc-markdown__link href=http://incompleteideas.net/index.html>Richard S. Sutton</a> and <a class=gdoc-markdown__link href=https://people.cs.umass.edu/~barto/>Andrew G. Barto</a> on <a class=gdoc-markdown__link href=http://incompleteideas.net/book/RLbook2020.pdf>Reinforcement Learning: An Introduction</a>)</figcaption>
</figure>
</div>
<p>這個方法在這個任務上可以做得很好，因為：</p>
<ul>
<li>步長參數在經過一段時間後適當的減少，會收斂到在給定每個狀態下真實的勝率值。</li>
<li>每一步都是根據對手的落子下的最佳解。</li>
</ul>
<p>這個例子突顯了 RL 的關鍵特色：</p>
<ul>
<li>強調在與環境互動中學習</li>
<li>目標明確</li>
<li>不只根據當前的狀態，也會考慮後面的發展</li>
</ul>
</article>
<div class="gdoc-page__footer flex flex-wrap justify-between">
<span class=gdoc-page__nav>
</span>
<span class=gdoc-page__nav>
<a class="gdoc-page__nav--next flex align-center" href=/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/ title="Part I: Tabular Solution Methods">Part I: Tabular Solution Methods </a>
</span>
</div>
</div>
</main>
<footer class=gdoc-footer>
<div class="container flex">
<div class="flex flex-wrap">
<span class="gdoc-footer__item gdoc-footer__item--row">
Built with <a href=https://gohugo.io/ class=gdoc-footer__link>Hugo</a> and<svg class="icon gdoc_heart"><use xlink:href="#gdoc_heart"/></svg>
</span>
</div>
<div class="flex flex-25 justify-end">
<span class=gdoc-footer__item>
<a class="gdoc-footer__link fake-link" href=# aria-label="Back to top"><svg class="icon gdoc_keyborad_arrow_up"><use xlink:href="#gdoc_keyborad_arrow_up"/></svg> <span class=hidden-mobile>Back to top</span>
</a>
</span>
</div>
</div>
</footer>
</div>
<script defer src=/RL-notes/js/zh.search.min.62011e6f91033499983c850099ab3e1c2c53d1691525ece5783685e0ddd1c310.js></script>
<script defer src=/RL-notes/js/clipboard-27784b7376.min.js></script>
<script defer src=/RL-notes/js/clipboard-loader-f0b5fbd5f6.min.js></script>
</body>
</html>