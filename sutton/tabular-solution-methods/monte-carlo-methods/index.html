<!doctype html><html lang=zh class=color-toggle-hidden>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=color-scheme content="light dark">
<meta name=description content="Monte Carlo Prediction Example 5.1: Blackjack Monte Carlo Estimation of Action Values Exploring starts Monte Carlo Control Example 5.3: Solving Blackjack Monte Carlo Control without Exploring Starts On-policy control methods Off-policy Prediction via Importance Sampling Example 5.4: Off-policy Estimation of a Blackjack State Value Incremental Implementation Ordinary importance sampling Weighted importance sampling Off-policy Monte Carlo Control *Discounting-aware Importance Sampling *Per-decision Importance Sampling Summary 和前面的方法相">
<title>Ch 5. Monte Carlo Methods | Reinforcement Learning Notes</title>
<link rel=icon href=/RL-notes/favicon/favicon-32x32.png type=image/x-icon>
<script src=/RL-notes/js/darkmode-ce906ea916.min.js></script>
<link rel=preload as=font href=/RL-notes/fonts/Metropolis.woff2 type=font/woff2 crossorigin=anonymous>
<link rel=preload as=font href=/RL-notes/fonts/LiberationSans.woff2 type=font/woff2 crossorigin=anonymous>
<link rel=preload href=/RL-notes/main-1be624d457.min.css as=style>
<link rel=stylesheet href=/RL-notes/main-1be624d457.min.css media=all>
<link rel=preload href=/RL-notes/mobile-3fc330242c.min.css as=style>
<link rel=stylesheet href=/RL-notes/mobile-3fc330242c.min.css media="screen and (max-width: 45rem)">
<link rel=preload href=/RL-notes/print-f79fc3e5d7.min.css as=style>
<link rel=stylesheet href=/RL-notes/print-f79fc3e5d7.min.css media=print>
<link rel=preload href=/RL-notes/custom.css as=style>
<link rel=stylesheet href=/RL-notes/custom.css media=all>
</head>
<body itemscope itemtype=https://schema.org/WebPage><svg class="svg-sprite" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_back" xmlns="http://www.w3.org/2000/svg"><path d="M24 10.526v2.947H5.755l8.351 8.421-2.105 2.105-12-12 12-12 2.105 2.105-8.351 8.421H24z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_left_alt" xmlns="http://www.w3.org/2000/svg"><path d="M5.965 10.526V6.035L0 12l5.965 5.965v-4.491H24v-2.947H5.965z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_right_alt" xmlns="http://www.w3.org/2000/svg"><path d="M18.035 10.526V6.035L24 12l-5.965 5.965v-4.491H0v-2.947h18.035z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_bitbucket" xmlns="http://www.w3.org/2000/svg"><path d="M15.905 13.355c.189 1.444-1.564 2.578-2.784 1.839-1.375-.602-1.375-2.784-.034-3.403 1.151-.705 2.818.223 2.818 1.564zm1.907-.361c-.309-2.44-3.076-4.056-5.328-3.042-1.426.636-2.389 2.148-2.32 3.747.086 2.097 2.08 3.815 4.176 3.626s3.729-2.234 3.472-4.331zm4.108-9.315c-.756-.997-2.045-1.169-3.179-1.358-3.214-.516-6.513-.533-9.727.034-1.066.172-2.269.361-2.939 1.323 1.1 1.031 2.664 1.186 4.073 1.358 2.544.327 5.156.344 7.699.017 1.426-.172 3.008-.309 4.073-1.375zm.979 17.788c-.481 1.684-.206 3.953-1.994 4.932-3.076 1.701-6.806 1.89-10.191 1.289-1.787-.327-3.884-.894-4.864-2.578-.43-1.65-.705-3.334-.98-5.018l.103-.275.309-.155c5.121 3.386 12.288 3.386 17.427.0.808.241.206 1.22.189 1.805zM26.01 4.951c-.584 3.764-1.255 7.51-1.908 11.257-.189 1.1-1.255 1.719-2.148 2.183-3.214 1.615-6.96 1.89-10.483 1.512-2.389-.258-4.829-.894-6.771-2.389-.911-.705-.911-1.908-1.083-2.922-.602-3.523-1.289-7.046-1.719-10.604.206-1.547 1.942-2.217 3.231-2.698C6.848.654 8.686.362 10.508.19c3.884-.378 7.854-.241 11.618.859 1.341.395 2.784.945 3.695 2.097.412.533.275 1.203.189 1.805z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_bookmark" xmlns="http://www.w3.org/2000/svg"><path d="M15.268 4.392q.868.0 1.532.638t.664 1.506v17.463l-7.659-3.268-7.608 3.268V6.536q0-.868.664-1.506t1.532-.638h10.876zm4.34 14.144V4.392q0-.868-.638-1.532t-1.506-.664H6.537q0-.868.664-1.532T8.733.0h10.876q.868.0 1.532.664t.664 1.532v17.412z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_auto" xmlns="http://www.w3.org/2000/svg"><path d="M16.846 18.938h2.382L15.22 7.785h-2.44L8.772 18.938h2.382l.871-2.44h3.95zm7.087-9.062L27.999 14l-4.066 4.124v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809zm-11.385 4.937L14 10.282l1.452 4.531h-2.904z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_dark" xmlns="http://www.w3.org/2000/svg"><path d="M14 21.435q3.079.0 5.257-2.178T21.435 14t-2.178-5.257T14 6.565q-1.51.0-3.079.697 1.917.871 3.108 2.701T15.22 14t-1.191 4.037-3.108 2.701q1.568.697 3.079.697zm9.933-11.559L27.999 14l-4.066 4.124v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_light" xmlns="http://www.w3.org/2000/svg"><path d="M14 21.435q3.079.0 5.257-2.178T21.435 14t-2.178-5.257T14 6.565 8.743 8.743 6.565 14t2.178 5.257T14 21.435zm9.933-3.311v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809L27.999 14z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_cloud_off" xmlns="http://www.w3.org/2000/svg"><path d="M9.023 10.5H7q-1.914.0-3.281 1.395t-1.367 3.309 1.367 3.281T7 19.852h11.375zM3.5 4.976l1.477-1.477L24.5 23.022l-1.477 1.477-2.352-2.297H6.999q-2.898.0-4.949-2.051t-2.051-4.949q0-2.844 1.969-4.867t4.758-2.133zm19.086 5.578q2.242.164 3.828 1.832T28 16.351q0 3.008-2.461 4.758l-1.695-1.695q1.805-.984 1.805-3.063.0-1.422-1.039-2.461t-2.461-1.039h-1.75v-.602q0-2.68-1.859-4.539t-4.539-1.859q-1.531.0-2.953.711l-1.75-1.695Q11.431 3.5 14.001 3.5q2.953.0 5.496 2.078t3.09 4.977z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_code" xmlns="http://www.w3.org/2000/svg"><path d="M9.917 24.5a1.75 1.75.0 10-3.501.001 1.75 1.75.0 003.501-.001zm0-21a1.75 1.75.0 10-3.501.001A1.75 1.75.0 009.917 3.5zm11.666 2.333a1.75 1.75.0 10-3.501.001 1.75 1.75.0 003.501-.001zm1.75.0a3.502 3.502.0 01-1.75 3.026c-.055 6.581-4.721 8.039-7.82 9.023-2.898.911-3.846 1.349-3.846 3.117v.474a3.502 3.502.0 011.75 3.026c0 1.932-1.568 3.5-3.5 3.5s-3.5-1.568-3.5-3.5c0-1.294.711-2.424 1.75-3.026V6.526A3.502 3.502.0 014.667 3.5c0-1.932 1.568-3.5 3.5-3.5s3.5 1.568 3.5 3.5a3.502 3.502.0 01-1.75 3.026v9.06c.93-.456 1.914-.766 2.807-1.039 3.391-1.075 5.323-1.878 5.359-5.687a3.502 3.502.0 01-1.75-3.026c0-1.932 1.568-3.5 3.5-3.5s3.5 1.568 3.5 3.5z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_date" xmlns="http://www.w3.org/2000/svg"><path d="M27.192 28.844V11.192H4.808v17.652h22.384zm0-25.689q1.277.0 2.253.976t.976 2.253v22.459q0 1.277-.976 2.216t-2.253.939H4.808q-1.352.0-2.291-.901t-.939-2.253V6.385q0-1.277.939-2.253t2.291-.976h1.577V.001h3.23v3.155h12.769V.001h3.23v3.155h1.577zm-3.155 11.267v3.155h-3.23v-3.155h3.23zm-6.46.0v3.155h-3.155v-3.155h3.155zm-6.384.0v3.155h-3.23v-3.155h3.23z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_download" xmlns="http://www.w3.org/2000/svg"><path d="M2.866 28.209h26.269v3.79H2.866v-3.79zm26.268-16.925L16 24.418 2.866 11.284h7.493V.001h11.283v11.283h7.493z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_email" xmlns="http://www.w3.org/2000/svg"><path d="M28.845 9.615v-3.23L16 14.422 3.155 6.385v3.23L16 17.577zm0-6.46q1.277.0 2.216.977T32 6.385v19.23q0 1.277-.939 2.253t-2.216.977H3.155q-1.277.0-2.216-.977T0 25.615V6.385q0-1.277.939-2.253t2.216-.977h25.69z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_git" xmlns="http://www.w3.org/2000/svg"><path d="M27.472 12.753 15.247.529a1.803 1.803.0 00-2.55.0l-2.84 2.84 2.137 2.137a2.625 2.625.0 013.501 3.501l3.499 3.499a2.625 2.625.0 11-1.237 1.237l-3.499-3.499c-.083.04-.169.075-.257.106v7.3a2.626 2.626.0 11-1.75.0v-7.3a2.626 2.626.0 01-1.494-3.607L8.62 4.606l-8.09 8.09a1.805 1.805.0 000 2.551l12.225 12.224a1.803 1.803.0 002.55.0l12.168-12.168a1.805 1.805.0 000-2.551z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_github" xmlns="http://www.w3.org/2000/svg"><path d="M16 .394c8.833.0 15.999 7.166 15.999 15.999.0 7.062-4.583 13.062-10.937 15.187-.813.146-1.104-.354-1.104-.771.0-.521.021-2.25.021-4.396.0-1.5-.5-2.458-1.083-2.958 3.562-.396 7.312-1.75 7.312-7.896.0-1.75-.625-3.167-1.646-4.291.167-.417.708-2.042-.167-4.25-1.333-.417-4.396 1.646-4.396 1.646a15.032 15.032.0 00-8 0S8.937 6.602 7.603 7.018c-.875 2.208-.333 3.833-.167 4.25-1.021 1.125-1.646 2.542-1.646 4.291.0 6.125 3.729 7.5 7.291 7.896-.458.417-.875 1.125-1.021 2.146-.917.417-3.25 1.125-4.646-1.333-.875-1.521-2.458-1.646-2.458-1.646-1.562-.021-.104.979-.104.979 1.042.479 1.771 2.333 1.771 2.333.938 2.854 5.396 1.896 5.396 1.896.0 1.333.021 2.583.021 2.979.0.417-.292.917-1.104.771C4.582 29.455-.001 23.455-.001 16.393-.001 7.56 7.165.394 15.998.394zM6.063 23.372c.042-.083-.021-.187-.146-.25-.125-.042-.229-.021-.271.042-.042.083.021.187.146.25.104.062.229.042.271-.042zm.646.709c.083-.062.062-.208-.042-.333-.104-.104-.25-.146-.333-.062-.083.062-.062.208.042.333.104.104.25.146.333.062zm.625.937c.104-.083.104-.25.0-.396-.083-.146-.25-.208-.354-.125-.104.062-.104.229.0.375s.271.208.354.146zm.875.875c.083-.083.042-.271-.083-.396-.146-.146-.333-.167-.417-.062-.104.083-.062.271.083.396.146.146.333.167.417.062zm1.187.521c.042-.125-.083-.271-.271-.333-.167-.042-.354.021-.396.146s.083.271.271.312c.167.062.354.0.396-.125zm1.313.104c0-.146-.167-.25-.354-.229-.187.0-.333.104-.333.229.0.146.146.25.354.229.187.0.333-.104.333-.229zm1.208-.208c-.021-.125-.187-.208-.375-.187-.187.042-.312.167-.292.312.021.125.187.208.375.167s.312-.167.292-.292z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_gitlab" xmlns="http://www.w3.org/2000/svg"><path d="M1.629 11.034 14 26.888.442 17.048a1.09 1.09.0 01-.39-1.203l1.578-4.811zm7.217.0h10.309l-5.154 15.854zM5.753 1.475l3.093 9.559H1.63l3.093-9.559a.548.548.0 011.031.0zm20.618 9.559 1.578 4.811c.141.437-.016.922-.39 1.203l-13.558 9.84 12.371-15.854zm0 0h-7.216l3.093-9.559a.548.548.0 011.031.0z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_heart" xmlns="http://www.w3.org/2000/svg"><path d="M16 29.714a1.11 1.11.0 01-.786-.321L4.072 18.643c-.143-.125-4.071-3.714-4.071-8 0-5.232 3.196-8.357 8.535-8.357 3.125.0 6.053 2.464 7.464 3.857 1.411-1.393 4.339-3.857 7.464-3.857 5.339.0 8.535 3.125 8.535 8.357.0 4.286-3.928 7.875-4.089 8.035L16.785 29.392c-.214.214-.5.321-.786.321z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_home" xmlns="http://www.w3.org/2000/svg"><path d="M24.003 15.695v8.336c0 .608-.504 1.111-1.111 1.111h-6.669v-6.669h-4.446v6.669H5.108a1.119 1.119.0 01-1.111-1.111v-8.336c0-.035.017-.069.017-.104L14 7.359l9.986 8.232a.224.224.0 01.017.104zm3.873-1.198-1.077 1.285a.578.578.0 01-.365.191h-.052a.547.547.0 01-.365-.122L14 5.831 1.983 15.851a.594.594.0 01-.417.122.578.578.0 01-.365-.191L.124 14.497a.57.57.0 01.069-.781L12.679 3.314c.729-.608 1.91-.608 2.64.0l4.237 3.543V3.471c0-.313.243-.556.556-.556h3.334c.313.0.556.243.556.556v7.085l3.803 3.161c.226.191.26.556.069.781z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_keyborad_arrow_down" xmlns="http://www.w3.org/2000/svg"><path d="M3.281 5.36 14 16.079 24.719 5.36 28 8.641l-14 14-14-14z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_keyborad_arrow_left" xmlns="http://www.w3.org/2000/svg"><path d="M25.875 28.25 22.125 32 6.126 16.001 22.125.002l3.75 3.75-12.25 12.25z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_keyborad_arrow_right" xmlns="http://www.w3.org/2000/svg"><path d="M6.125 28.25 18.375 16 6.125 3.75 9.875.0l15.999 15.999L9.875 31.998z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_keyborad_arrow_up" xmlns="http://www.w3.org/2000/svg"><path d="M24.719 22.64 14 11.921 3.281 22.64.0 19.359l14-14 14 14z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_link" xmlns="http://www.w3.org/2000/svg"><path d="M24.037 7.963q3.305.0 5.634 2.366T32 16t-2.329 5.671-5.634 2.366h-6.46v-3.08h6.46q2.028.0 3.493-1.465t1.465-3.493-1.465-3.493-3.493-1.465h-6.46v-3.08h6.46zM9.615 17.578v-3.155h12.77v3.155H9.615zM3.005 16q0 2.028 1.465 3.493t3.493 1.465h6.46v3.08h-6.46q-3.305.0-5.634-2.366T0 16.001t2.329-5.671 5.634-2.366h6.46v3.08h-6.46q-2.028.0-3.493 1.465t-1.465 3.493z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_menu" xmlns="http://www.w3.org/2000/svg"><path d="M.001 5.334h31.998v3.583H.001V5.334zm0 12.416v-3.5h31.998v3.5H.001zm0 8.916v-3.583h31.998v3.583H.001z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_notification" xmlns="http://www.w3.org/2000/svg"><path d="M22.615 19.384l2.894 2.894v1.413H2.49v-1.413l2.894-2.894V12.25q0-3.365 1.716-5.856t4.745-3.231v-1.01q0-.875.606-1.514T13.999.0t1.548.639.606 1.514v1.01q3.029.74 4.745 3.231t1.716 5.856v7.134zM14 27.999q-1.211.0-2.053-.808t-.841-2.019h5.788q0 1.144-.875 1.986T14 27.999z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_path" xmlns="http://www.w3.org/2000/svg"><path d="M28 12.62h-9.793V8.414h-2.826v11.173h2.826v-4.206H28V26.62h-9.793v-4.206H12.62v-14H9.794v4.206H.001V1.381h9.793v4.206h8.413V1.381H28V12.62z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_person" xmlns="http://www.w3.org/2000/svg"><path d="M16 20.023q5.052.0 10.526 2.199t5.473 5.754v4.023H0v-4.023q0-3.555 5.473-5.754t10.526-2.199zM16 16q-3.275.0-5.614-2.339T8.047 8.047t2.339-5.661T16 0t5.614 2.386 2.339 5.661-2.339 5.614T16 16z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_search" xmlns="http://www.w3.org/2000/svg"><path d="M11.925 20.161q3.432.0 5.834-2.402t2.402-5.834-2.402-5.834-5.834-2.402-5.834 2.402-2.402 5.834 2.402 5.834 5.834 2.402zm10.981.0L32 29.255 29.255 32l-9.094-9.094v-1.458l-.515-.515q-3.26 2.831-7.721 2.831-4.976.0-8.45-3.432T.001 11.925t3.474-8.45 8.45-3.474 8.407 3.474 3.432 8.45q0 1.802-.858 4.075t-1.973 3.646l.515.515h1.458z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_shield" xmlns="http://www.w3.org/2000/svg"><path d="M22.167 15.166V3.5h-8.166v20.726c.93-.492 2.424-1.349 3.883-2.497 1.95-1.531 4.284-3.919 4.284-6.562zm3.499-13.999v14c0 7.674-10.737 12.523-11.192 12.724-.146.073-.31.109-.474.109s-.328-.036-.474-.109c-.456-.201-11.192-5.049-11.192-12.724v-14C2.334.529 2.863.0 3.501.0H24.5c.638.0 1.167.529 1.167 1.167z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_tags" xmlns="http://www.w3.org/2000/svg"><path d="M6.606 7.549c0-1.047-.84-1.887-1.887-1.887s-1.887.84-1.887 1.887.84 1.887 1.887 1.887 1.887-.84 1.887-1.887zm15.732 8.493c0 .501-.206.988-.546 1.327l-7.239 7.254c-.354.339-.84.546-1.342.546s-.988-.206-1.327-.546L1.342 14.066C.59 13.329.0 11.899.0 10.852V4.718a1.9 1.9.0 011.887-1.887h6.134c1.047.0 2.477.59 3.229 1.342L21.792 14.7c.339.354.546.84.546 1.342zm5.661.0c0 .501-.206.988-.546 1.327l-7.239 7.254c-.354.339-.84.546-1.342.546-.767.0-1.15-.354-1.651-.87l6.93-6.93c.339-.339.546-.826.546-1.327s-.206-.988-.546-1.342L13.609 4.173c-.752-.752-2.182-1.342-3.229-1.342h3.303c1.047.0 2.477.59 3.229 1.342L27.454 14.7c.339.354.546.84.546 1.342z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_timer" xmlns="http://www.w3.org/2000/svg"><path d="M16 29q4.428.0 7.536-3.143t3.107-7.571-3.107-7.536T16 7.643 8.464 10.75t-3.107 7.536 3.107 7.571T16 29zM26.714 9.786q1.214 1.571 2.107 4.036t.893 4.464q0 5.643-4 9.678T16 32t-9.714-4.036-4-9.678 4-9.678T16 4.572q1.929.0 4.464.929t4.107 2.143l2.143-2.214q1.143.929 2.143 2.143zM14.5 19.857v-9.143h3v9.143h-3zM20.571.001v3.071h-9.143V.001h9.143z"/></symbol></svg>
<div class=wrapper>
<input type=checkbox class=hidden id=menu-control>
<input type=checkbox class=hidden id=menu-header-control>
<header class=gdoc-header>
<div class="container flex align-center justify-between">
<label for=menu-control class=gdoc-nav__control><svg class="icon gdoc_menu"><title>Open Nav Menu</title><use xlink:href="#gdoc_menu"/></svg><svg class="icon gdoc_arrow_back"><title>Close Nav Menu</title><use xlink:href="#gdoc_arrow_back"/></svg>
</label>
<a class=gdoc-header__link href=https://aquastripe.github.io/RL-notes/>
<span class="gdoc-brand flex align-center">
<img class=gdoc-brand__img src=/RL-notes/brand.svg alt>
<span class=gdoc-brand__title>Reinforcement Learning Notes</span>
</span>
</a>
<div class=gdoc-menu-header>
<span id=gdoc-dark-mode><svg class="icon gdoc_brightness_dark"><title>Toggle Dark/Light/Auto mode</title><use xlink:href="#gdoc_brightness_dark"/></svg><svg class="icon gdoc_brightness_light"><title>Toggle Dark/Light/Auto mode</title><use xlink:href="#gdoc_brightness_light"/></svg><svg class="icon gdoc_brightness_auto"><title>Toggle Dark/Light/Auto mode</title><use xlink:href="#gdoc_brightness_auto"/></svg>
</span>
</div>
</div>
</header>
<main class="container flex flex-even">
<aside class=gdoc-nav>
<nav>
<div class=gdoc-search><svg class="icon gdoc_search"><use xlink:href="#gdoc_search"/></svg>
<input type=text id=gdoc-search-input class=gdoc-search__input placeholder=Search... aria-label=Search maxlength=64>
<div class="gdoc-search__spinner spinner hidden"></div>
<ul id=gdoc-search-results class=gdoc-search__list></ul>
</div>
<section class=gdoc-nav--main>
<h2>Navigation</h2>
<ul class=gdoc-nav__list>
<li>
<span class=flex>Reinforcement Learning: An Introduction</span>
<ul class=gdoc-nav__list>
<li>
<span class=flex>
<a href=/RL-notes/sutton/introduction/ class=gdoc-nav__entry>
Ch 1. Introduction
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/sutton/tabular-solution-methods/ class=gdoc-nav__entry>
Part I: Tabular Solution Methods
</a>
</span>
<ul class=gdoc-nav__list>
<li>
<span class=flex>
<a href=/RL-notes/sutton/tabular-solution-methods/multi-armed-bandits/ class=gdoc-nav__entry>
Ch 2. Multi-Armed Bandits
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/sutton/tabular-solution-methods/finite-markov-decision-processes/ class=gdoc-nav__entry>
Ch 3. Finite Markov Decision Processes
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/sutton/tabular-solution-methods/dynamic-programming/ class=gdoc-nav__entry>
Ch 4. Dynamic Programming
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/ class="gdoc-nav__entry is-active">
Ch 5. Monte Carlo Methods
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/sutton/tabular-solution-methods/temporal-difference-learning/ class=gdoc-nav__entry>
Ch 6. Temporal-Difference Learning
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/sutton/tabular-solution-methods/n-step-bootstrapping/ class=gdoc-nav__entry>
Ch 7. n-step Bootstrapping
</a>
</span>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section class=gdoc-nav--more>
<h2>More</h2>
<ul class=gdoc-nav__list>
<li>
<span class=flex><svg class="icon gdoc_github"><use xlink:href="#gdoc_github"/></svg>
<a href=https://github.com/aquastripe/RL-notes class=gdoc-nav__entry>
View Source
</a>
</span>
</li>
</ul>
</section>
</nav>
</aside>
<div class=gdoc-page>
<div class="gdoc-page__header flex flex-wrap
justify-between
hidden-mobile" itemprop=breadcrumb>
<div><svg class="icon gdoc_path hidden-mobile"><use xlink:href="#gdoc_path"/></svg>
<ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList>
<li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemscope itemtype=https://schema.org/WebPage itemprop=item itemid=/RL-notes/ href=/RL-notes/><span itemprop=name>Reinforcement Learning Notes</span></a><meta itemprop=position content="2"></li><li> / </li><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemscope itemtype=https://schema.org/WebPage itemprop=item itemid=/RL-notes/sutton/ href=/RL-notes/sutton/><span itemprop=name>Sutton</span></a><meta itemprop=position content="3"></li><li> / </li><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemscope itemtype=https://schema.org/WebPage itemprop=item itemid=/RL-notes/sutton/tabular-solution-methods/ href=/RL-notes/sutton/tabular-solution-methods/><span itemprop=name>Part I: Tabular Solution Methods</span></a><meta itemprop=position content="4"></li><li> / </li><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Ch 5. Monte Carlo Methods</span><meta itemprop=position content="5"></li>
</ol>
</div>
</div>
<script>MathJax={tex:{inlineMath:[["$","$"]]},displayMath:[["$$","$$"],["[[","]]"]],svg:{fontCache:"global"}}</script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<article class="gdoc-markdown gdoc-markdown__align--left">
<h1>Ch 5. Monte Carlo Methods</h1>
<div class="gdoc-toc gdoc-toc__level--6"><nav id=TableOfContents><ul>
<li><a href=#monte-carlo-prediction>Monte Carlo Prediction</a>
<ul>
<li><a href=#example-51-blackjack>Example 5.1: Blackjack</a></li>
</ul>
</li>
<li><a href=#monte-carlo-estimation-of-action-values>Monte Carlo Estimation of Action Values</a>
<ul>
<li><a href=#exploring-starts>Exploring starts</a></li>
</ul>
</li>
<li><a href=#monte-carlo-control>Monte Carlo Control</a>
<ul>
<li><a href=#example-53-solving-blackjack>Example 5.3: Solving Blackjack</a></li>
</ul>
</li>
<li><a href=#monte-carlo-control-without-exploring-starts>Monte Carlo Control without Exploring Starts</a>
<ul>
<li><a href=#on-policy-control-methods>On-policy control methods</a></li>
</ul>
</li>
<li><a href=#off-policy-prediction-via-importance-sampling>Off-policy Prediction via Importance Sampling</a>
<ul>
<li><a href=#example-54-off-policy-estimation-of-a-blackjack-state-value>Example 5.4: Off-policy Estimation of a Blackjack State Value</a></li>
</ul>
</li>
<li><a href=#incremental-implementation>Incremental Implementation</a>
<ul>
<li><a href=#ordinary-importance-sampling>Ordinary importance sampling</a></li>
<li><a href=#weighted-importance-sampling>Weighted importance sampling</a></li>
</ul>
</li>
<li><a href=#off-policy-monte-carlo-control>Off-policy Monte Carlo Control</a></li>
<li><a href=#discounting-aware-importance-sampling>*Discounting-aware Importance Sampling</a></li>
<li><a href=#per-decision-importance-sampling>*Per-decision Importance Sampling</a></li>
<li><a href=#summary>Summary</a></li>
</ul></nav><hr></div>
<p>和前面的方法相比之下，Monte Carlo 不假設擁有環境的完整知識。<br>
Monte Carlo 只憑借「經驗」：從環境互動或是透過模擬來採樣 states, actions, and rewards.<br>
雖然還是需要環境模型 (model)，但只需要採樣 transitions，而不像 DP 需要完整、所有可能的機率分佈。</p>
<p>Monte Carlo 基於 <em>averaging sample returns</em>.<br>
為了確保 well-defined returns 存在，在此限定問題在 episodic tasks: 可以分成 episodes 且每個 episode 最後會停止。</p>
<p>Monte Carlo 採樣並平均 (sample and average) 每個 state-action pair 的 <strong>returns</strong>，類似於 Ch 2. bandit methods 採樣並平均 <strong>rewards</strong>。<br>
差異：現在的問題會有多個 states，問題變成 non-stationary。</p>
<p>為了解決 non-stationary 問題，採用 DP 法的 GPI。<br>
差異：</p>
<ul>
<li>DP: 計算 value functions</li>
<li>MC: 學習 value functions</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h2 id=monte-carlo-prediction>
Monte Carlo Prediction
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#monte-carlo-prediction class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Monte Carlo Prediction" href=#monte-carlo-prediction><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>問題：給定一個 policy，如何學習 state-value function $v_{\pi}(s)$？<br>
一個顯而易見的解法：直接採樣 returns 並計算平均值。</p>
<ul>
<li>First-visit MC method: the average of the returns following first visits to $s$
<ul>
<li>詳見下面的演算法</li>
<li>從 1940s 開始已經被大量研究</li>
</ul>
</li>
<li>Every-visit MC method: averages the returns following all visits to $s$
<ul>
<li>Ch 9 & Ch 12 再討論</li>
<li>不檢查是否第一次探訪 $S_t$</li>
</ul>
</li>
</ul>
<p>性質：</p>
<ul>
<li>探訪狀態 $s$ 次數趨近無限時，兩種方法都會收斂。</li>
<li>First-visit MC method
<ul>
<li>根據大數法則收斂到它的期望值</li>
<li>每個平均值都是 <strong>unbiased estimate</strong>，標準差為 $\frac{1}{\sqrt{n}}$，$n$ 為 returns 的數量</li>
</ul>
</li>
<li>Every-visit MC method 以 quadratically (二次函數的) 速率收斂</li>
</ul>
<p><img src=first-visit-mc-prediction.png alt></p>
<div class=gdoc-page__anchorwrap>
<h3 id=example-51-blackjack>
Example 5.1: Blackjack
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#example-51-blackjack class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Example 5.1: Blackjack" href=#example-51-blackjack><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p><img src=5.1.png alt></p>
<p>規則：</p>
<ul>
<li>無限牌組（抽完放回）</li>
<li>莊家加牌，直到 17 點</li>
<li>no discount</li>
<li>usable: 玩家拿到 ace 並且可以計數為 11 點，此時應該跟牌 (hit)</li>
<li>假設玩家的 policy 是：拿到 20 或 21 點停止，其餘加注</li>
<li>結果以下列數值表示：
<ul>
<li>+1: 贏</li>
<li>-1: 輸</li>
<li>0: 平手</li>
</ul>
</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h2 id=monte-carlo-estimation-of-action-values>
Monte Carlo Estimation of Action Values
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#monte-carlo-estimation-of-action-values class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Monte Carlo Estimation of Action Values" href=#monte-carlo-estimation-of-action-values><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>如果環境模型不存在，那麼估計 action-values $q_{\pi}(s, a)$ 會比 state-values $v_{\pi}(s)$ 更有效。</p>
<ul>
<li>First-visit MC method: 平均每個 episode 第一次遇到的 state-action 的 returns</li>
<li>Every-visit MC method: 平均所有遇到的 state-action 的 returns</li>
</ul>
<p>有些 state-action pairs 可能不會遇到。如果 $\pi$ 是確定性的：從每個 state 只會觀察到一個 action，對於沒有 returns 可以平均的 actions， MC 無法從經驗獲得改善。<br>
這個問題稱為 <strong>maintaining exploration</strong>。</p>
<div class=gdoc-page__anchorwrap>
<h3 id=exploring-starts>
Exploring starts
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#exploring-starts class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Exploring starts" href=#exploring-starts><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p>為了可以比較，必須估計所有 action 的 value。<strong>Exploring starts</strong> 假設：在每個 episode 開始時會指定一個 state-action pair，並強迫所有 pair 都有機會被選擇到。</p>
<p>這個假設有時候有用，除了「和環境模型直接互動」這種情形。這種情況下，最常見的替代方案是只考慮「在每個 state 以非 0 的機率 <em>隨機性的</em> 來選擇所有動作」的 policies，來確保所有 state-action pairs 都會遇到。</p>
<p>以下先討論 exploring starts 的假設情況。</p>
<div class=gdoc-page__anchorwrap>
<h2 id=monte-carlo-control>
Monte Carlo Control
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#monte-carlo-control class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Monte Carlo Control" href=#monte-carlo-control><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>這一章討論如何把 Monte Carlo estimation 用來近似最佳策略 (optimal policies)。<br>
整個想法是基於 GPI：以一個迭代的過程，固定 policy 近似 value function，固定 value function 近似 policy。</p>
<p>假設：</p>
<ul>
<li>我們觀察了無限次數 episodes</li>
<li>這些 episodes 使用 exploring starts 產生</li>
</ul>
<p>Policy improvement 會以 policy greedy 並根據目前的 value function 來完成。這種情況下，我們有一個 action-value function，因此不需要 model 來建構 policy greedy。Policy greedy 以決定性的方式選擇最大的 action-value：</p>
<p>$$
\pi(s) \doteq \arg \max _{a} q(s, a)
$$</p>
<p>根據 policy improvement 定理，對所有 $s \in \mathcal{S}$:</p>
<p>$$
\begin{aligned}
q_{\pi_{k}}\left(s, \pi_{k+1}(s)\right) &=q_{\pi_{k}}\left(s, \underset{a}{\arg \max } q_{\pi_{k}}(s, a)\right) \newline
&=\max <em>{a} q</em>{\pi_{k}}(s, a) \newline
& \geq q_{\pi_{k}}\left(s, \pi_{k}(s)\right) \newline
& \geq v_{\pi_{k}}(s)
\end{aligned}
$$</p>
<p>每一輪的 policy 都會比上一輪的更好。</p>
<p>接下來討論如何消除第一個假設：觀察 <em>無限次數</em> episodes。對 DP 和 MC 兩者來說，都有兩種方式來消除。<br>
一種方式是給定誤差範圍的大小來計算估計誤差的機率，可以確保在一定數量的步數內會收斂在這個誤差範圍內。不過，有可能需要太多的步數，導致只能套用在極少數的實務應用上。<br>
另一種方式，是放棄完整的 policy evaluation。一種極端的情形是 value iteration，在每兩個 policy improvement 之間它只會進行一次迭代 iterative policy evaluation。而 in-place 版本的 value iteration 更為極端，它會在 improvement 和 evaluation 迭代一個狀態。</p>
<p>Monte Carlo ES: 對於每個 episode，觀察到的 returns 會拿去算 policy evaluation，然後以該 episode 觀察到所有的 states 進行 policy improvement。完整演算法如下：</p>
<p><img src=monte-carlo-es.png alt></p>
<p>（可以用 2.4 的方式進行增量計算）</p>
<ul>
<li>Monte Carlo ES 不會收斂到任何 suboptimal policy。
<ul>
<li>如果發生了，value function 就會到該 policy，導致 policy 改變。</li>
</ul>
</li>
<li>只會在 policy 和 value function 都是 optimal 的時候進入穩定。</li>
<li>收歛性：雖然看起來無可避免的，隨著時間過去 action-value function 改變量會越來越少，不過還沒有形式化的證明。
<ul>
<li>以作者的觀點認為這是一個開放的 RL 基礎理論問題。</li>
</ul>
</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h3 id=example-53-solving-blackjack>
Example 5.3: Solving Blackjack
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#example-53-solving-blackjack class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Example 5.3: Solving Blackjack" href=#example-53-solving-blackjack><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p>接續 Example 5.1，使用 &ldquo;20, 21 就停止，其他則跟注&rdquo; 作為初始的 policy，再以 Monte Carlo ES 找到 optimal policy，結果如下圖：</p>
<p><img src=5.2.png alt></p>
<p>結果跟 Thorp&rsquo;s strategy 幾乎一致，除了 usable ace 左邊的缺口有所不同。</p>
<div class=gdoc-page__anchorwrap>
<h2 id=monte-carlo-control-without-exploring-starts>
Monte Carlo Control without Exploring Starts
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#monte-carlo-control-without-exploring-starts class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Monte Carlo Control without Exploring Starts" href=#monte-carlo-control-without-exploring-starts><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>Exploring starts 是一種不太可能發生的情形，因此以下討論如何避免這項假設。<br>
避免這項假設，需要確保所有動作都會無限次的被選擇
有兩種方式達到：</p>
<ul>
<li>on-policy: 嘗試評估 (evaluate) 或改善 (improve) 被選擇的用來決策的 policy
<ul>
<li>Monte Carlo ES method</li>
</ul>
</li>
<li>off-policy: 嘗試評估 (evaluate) 或改善 (improve) 被選擇的用來產生資料的 policy</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h3 id=on-policy-control-methods>
On-policy control methods
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#on-policy-control-methods class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor On-policy control methods" href=#on-policy-control-methods><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p>這類的方法，policy 一般來說都是 <em>soft</em>: $\pi(a | s) > 0$ 對所有 $s \in \mathcal{S}$ 和 $a \in \mathcal{A}(s)$，但是越來越接近成一個確定性的 optimal policy。大部分 Ch 2. 的方法都是這樣的機制，例如 $\varepsilon$&ndash;greedy: 大部分選擇的動作是根據最大的 action-value，而少部份根據 $\varepsilon$ 機率來隨機選擇，也就是 $\frac{\varepsilon}{| \mathcal{A}(s) |}$，而 greedy action 是 $1 - \varepsilon + \frac{\varepsilon}{| \mathcal{A}(s) |}$。</p>
<p>$\varepsilon$&ndash;soft: $\pi(a | s) \ge \frac{\varepsilon}{| \mathcal{A}(s) |}$ 對所有 states, action, 還有某個 $\varepsilon > 0$。<br>
$\varepsilon$&ndash;greedy 是一種 $\varepsilon$&ndash;soft。</p>
<p>On-policy Monte Carlo control 整體的想法還是基於 GPI。少了 exploring starts 的假設，不能直接根據目前的 value function 並使用 greedy 取樣來改善 policy，因為這樣會阻止探索 non-greedy actions。</p>
<p>對任意 $\varepsilon$&ndash;soft policy $\pi$，任何根據 $q_{\pi}$ 的 $\varepsilon$&ndash;greedy policy 都保證比 $\pi$ 更好，或是一樣好。演算法如下：</p>
<p><img src=on-policy-first-visit-mc-control.png alt></p>
<p>$$
\begin{aligned}
q_{\pi}\left(s, \pi^{\prime}(s)\right) &=\sum_{a} \pi^{\prime}(a \mid s) q_{\pi}(s, a) \newline
&=\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+(1-\varepsilon) \max <em>{a} q</em>{\pi}(s, a) \newline
& \geq \frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+(1-\varepsilon) \sum_{a} \frac{\pi(a \mid s)-\frac{\varepsilon}{|\mathcal{A}(s)|}}{1-\varepsilon} q_{\pi}(s, a) \newline
&=\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)-\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+\sum_{a} \pi(a \mid s) q_{\pi}(s, a) \newline
&=v_{\pi}(s)
\end{aligned}
$$</p>
<p>因此，根據 policy improvement 定理，$v_{\pi^\prime}(s) \ge v_{\pi}(s), \forall s \in \mathcal{S}$。</p>
<div class=gdoc-page__anchorwrap>
<h2 id=off-policy-prediction-via-importance-sampling>
Off-policy Prediction via Importance Sampling
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#off-policy-prediction-via-importance-sampling class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Off-policy Prediction via Importance Sampling" href=#off-policy-prediction-via-importance-sampling><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>所有的學習控制 (learning control) 方法都面臨兩難：</p>
<ul>
<li>學習最佳的動作</li>
<li>探索所有的動作</li>
</ul>
<p>如何做到這件事？</p>
<ul>
<li>On-policy: 不為最佳動作學習，而是學習靠近最佳動作</li>
<li>Off-policy: learning from data off the target policy<br>
採用兩個 policies:
<ul>
<li>目標策略 (target policy): 用來學習最佳策略</li>
<li>行為策略 (behavior policy): 用來產生行為進行探索</li>
</ul>
</li>
</ul>
<p>兩者的差異：</p>
<ul>
<li>On-policy:
<ul>
<li>較簡單</li>
</ul>
</li>
<li>Off-policy:
<ul>
<li>較複雜</li>
<li>較大的變異性，收斂更慢</li>
<li>更強大且更通用</li>
<li>更多額外的應用方式
<ul>
<li>可以學習由 專家或是傳統非學習的控制器 (controller) 所產生的資料</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>從兩種 policy 來估計 $v_\pi$ 或 $q_\pi$：</p>
<ul>
<li>目標策略: $\pi$</li>
<li>行為策略: $b$</li>
</ul>
<p>為了使用 $b$ 的資料來估計 $\pi$ 的價值，需要覆蓋性假設 (the assumption of coverage): 每個以 $\pi$ 策略所採取的動作也同時要考慮以 $b$ 策略來採取。亦即：$\pi (a|s) \gt \text{ implies } b(a|s) \gt 0$。</p>
<ul>
<li>因此，$b$ 必須是隨機性的，並且與 $\pi$ 不同。</li>
<li>$\pi$ 可能是確定性的</li>
</ul>
<p>幾乎所有 off-policy 方法都會利用 importance sampling: 一種通用的技術，在某個分佈下透過給定另一個分佈來估計期望值。<a class=gdoc-markdown__link href=https://en.wikipedia.org/wiki/Importance_sampling>(Importance sampling Wiki)</a></p>
<blockquote class="gdoc-hint info">
<p><strong>Importance sampling</strong></p>
<p>有時我們可能想從一個隨機分佈上面進行採樣，其中想要採樣的範圍發生的機率很低。使用 MC 採樣時，因為發生的機率很低，會導致產生的樣本數極少而失效。在這個「重要的區域」給予更多的權重，稱為「重要性採樣」。一個基本的重要性採樣方法是：想要計算某個分佈 $p$ 時，透過採用另一個分佈 $q$ 來完成。例如：</p>
<p>$$
\mu=\int_{\mathcal{D}} f(\boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}=\int_{\mathcal{D}} \frac{f(\boldsymbol{x}) p(\boldsymbol{x})}{q(\boldsymbol{x})} q(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}=\mathbb{E}_{q}\left(\frac{f(\boldsymbol{X}) p(\boldsymbol{X})}{q(\boldsymbol{X})}\right)
$$</p>
<p>Ref: <a class=gdoc-markdown__link href=https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf>https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf</a></p>
</blockquote>
<p>給定一個起始狀態 $S_t$，state-action trajectory $A_t, S_{t+1}, A_{t+1}, \ldots, S_T$ 在任意策略 $\pi$ 之下發生的機率為:</p>
<p>$$
\begin{array}{l}
\operatorname{Pr} \lbrace A_{t}, S_{t+1}, A_{t+1}, \ldots, S_{T} \mid S_{t}, A_{t: T-1} \sim \pi \rbrace \newline
\quad=\pi\left(A_{t} \mid S_{t}\right) p\left(S_{t+1} \mid S_{t}, A_{t}\right) \pi\left(A_{t+1} \mid S_{t+1}\right) \cdots p\left(S_{T} \mid S_{T-1}, A_{T-1}\right) \newline
\quad=\prod_{k=t}^{T-1} \pi\left(A_{k} \mid S_{k}\right) p\left(S_{k+1} \mid S_{k}, A_{k}\right)
\end{array}
$$</p>
<p>其中，$p$ 是狀態轉移機率函數，在 3.4 中定義。因此，在目標策略和行為策略之下的 trajectory 的相對機率 (importance sampling ratio) 為:</p>
<p>$$
\begin{aligned}
\rho_{t: T-1} &\doteq \frac{\prod_{k=t}^{T-1} \pi\left(A_{k} \mid S_{k}\right) p\left(S_{k+1} \mid S_{k}, A_{k}\right)}{\prod_{k=t}^{T-1} b\left(A_{k} \mid S_{k}\right) p\left(S_{k+1} \mid S_{k}, A_{k}\right)}\newline
&=\prod_{k=t}^{T-1} \frac{\pi\left(A_{k} \mid S_{k}\right)}{b\left(A_{k} \mid S_{k}\right)}
\end{aligned}
$$</p>
<p>雖然兩個 trajectory probabilities 依賴於 MDP 的狀態轉移機率，但它們在分子和分母都有出現且值一樣，所以兩個會消掉。最後，importance sampling ratio 只和兩個 policies 和序列有關，和 MDP 無關。</p>
<p>我們使用行為策略採樣到的 returns 會估計出 <strong>行為策略</strong> 的 value function:</p>
<p>$$
\mathbb{E}\left[ G_{t} \mid S_{t}=s \right]=v_{b}(s)
$$</p>
<p>但是，透過上面的 importance sampling ratio 可以得到 <strong>目標策略</strong> 的 value function:</p>
<p>$$
\mathbb{E}\left[\rho_{t: T-1} G_{t} \mid S_{t}=s\right]=v_{\pi}(s)
$$</p>
<p>Notation:</p>
<ul>
<li>$\mathcal{T}(s)$: 當 state $s$ 被探訪的情況下，所有 time steps 的集合</li>
<li>$T(t)$: 在時間點 $t$ 之後第一次停止的時間</li>
<li>$G_t$: 從 $t$ 到 $T(t)$ 之間的 returns</li>
<li>$\lbrace G_{t} \rbrace_{t \in \mathcal{T}(s)}$: 屬於 state $s$ 的 returns</li>
<li>$\lbrace \rho_{t: T(t)-1}\rbrace _{t \in \mathcal{T}(s)}$: 上面對應的重要性採樣比率 (importance-sampling ratios)</li>
</ul>
<p>為了估計 $v_\pi(s)$，我們簡單的藉由 ratios 並計算平均來放大 returns:</p>
<p>$$
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t: T(t)-1} G_{t}}{|\mathcal{T}(s)|}
$$</p>
<p>如果重要性採樣是透過單純的計算平均來完成，稱為 <strong>ordinary importance sampling</strong>。<br>
另一種方式，是計算加權平均，稱為 <strong>weighted importance sampling</strong>:</p>
<p>$$
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t: T(t)-1} G_{t}}{\sum_{t \in \mathcal{T}(s)} \rho_{t: T(t)-1}}
$$</p>
<div class=gdoc-page__anchorwrap>
<h3 id=example-54-off-policy-estimation-of-a-blackjack-state-value>
Example 5.4: Off-policy Estimation of a Blackjack State Value
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#example-54-off-policy-estimation-of-a-blackjack-state-value class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Example 5.4: Off-policy Estimation of a Blackjack State Value" href=#example-54-off-policy-estimation-of-a-blackjack-state-value><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p>以行為策略來估計目標策略的 value。</p>
<p>設定以下條件：</p>
<ul>
<li>莊家有 deuce</li>
<li>玩家牌總和為 13</li>
<li>玩家有 usable ace</li>
<li>行為策略: 以各半的機率決定要不要加牌</li>
<li>目標策略: 加牌加到總和為 20 或 21</li>
</ul>
<p>估計誤差如下：
<img src=5.3.png alt></p>
<div class=gdoc-page__anchorwrap>
<h2 id=incremental-implementation>
Incremental Implementation
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#incremental-implementation class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Incremental Implementation" href=#incremental-implementation><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>以下考慮如何以增量計算進行 MC prediction。</p>
<p>在 Ch 2. 中對 rewards 進行增量計算，在 on-policy MC prediction 只需要改成 returns 即可。
對於 off-policy，需要分開考慮 ordinary importance sampling 和 weighted importance sampling。</p>
<div class=gdoc-page__anchorwrap>
<h3 id=ordinary-importance-sampling>
Ordinary importance sampling
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#ordinary-importance-sampling class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Ordinary importance sampling" href=#ordinary-importance-sampling><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p>Ordinary importance sampling 中，returns 以 importance sampling ratio $\rho_{t:T(t)-1}$ 進行放大，如下：</p>
<p>$$
\begin{aligned}
\rho_{t: T-1} &\doteq \frac{\prod_{k=t}^{T-1} \pi\left(A_{k} \mid S_{k}\right) p\left(S_{k+1} \mid S_{k}, A_{k}\right)}{\prod_{k=t}^{T-1} b\left(A_{k} \mid S_{k}\right) p\left(S_{k+1} \mid S_{k}, A_{k}\right)}\newline
&=\prod_{k=t}^{T-1} \frac{\pi\left(A_{k} \mid S_{k}\right)}{b\left(A_{k} \mid S_{k}\right)}
\end{aligned}
$$</p>
<p>然後再平均：</p>
<p>$$
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t: T(t)-1} G_{t}}{|\mathcal{T}(s)|}
$$</p>
<p>可以直接套用 Ch 2. 的方法並且計算放大和平均來做增量計算。</p>
<div class=gdoc-page__anchorwrap>
<h3 id=weighted-importance-sampling>
Weighted importance sampling
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#weighted-importance-sampling class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Weighted importance sampling" href=#weighted-importance-sampling><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p>假設我們有 returns 序列 $G_1, G_2, \ldots, G_{n-1}$，從相同的狀態開始，每個狀態對應一個隨機的權重 $W_i$。我們要估計的如下式：</p>
<p>$$
V_{n} \doteq \frac{\sum_{k=1}^{n-1} W_{k} G_{k}}{\sum_{k=1}^{n-1} W_{k}}, \quad n \geq 2
$$</p>
<p>更新規則如下：</p>
<p>$$
V_{n+1} \doteq V_{n}+\frac{W_{n}}{C_{n}}\left[G_{n}-V_{n}\right], \quad n \geq 1
$$</p>
<p>$C_n$ 為累積和，計算如下：</p>
<p>$$
C_{n+1} \doteq C_{n}+W_{n+1}
$$</p>
<p>其中，$C_0 \doteq 0$。</p>
<p><img src=off-policy-mc-prediction.png alt></p>
<div class=gdoc-page__anchorwrap>
<h2 id=off-policy-monte-carlo-control>
Off-policy Monte Carlo Control
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#off-policy-monte-carlo-control class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Off-policy Monte Carlo Control" href=#off-policy-monte-carlo-control><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>以下是 off-policy MC control 基於 GPI 和 weighted importance sampling，用來估計 $\pi_<em>$ 和 $q_</em>$。</p>
<ul>
<li>目標策略：根據 $Q$ 的 greedy policy，是一個 $q_\pi$ 的估計值。</li>
<li>行為策略：可以是任意的，不過為了確保 $\pi$ 收斂到最佳策略，必須對每一組 state 和 action 取得無限多的 returns。
<ul>
<li>可以選擇將 $b$ 設定為 $\varepsilon$-soft 來達到這個條件。</li>
</ul>
</li>
</ul>
<p><img src=off-policy-mc-control.png alt></p>
<div class=gdoc-page__anchorwrap>
<h2 id=discounting-aware-importance-sampling>
*Discounting-aware Importance Sampling
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#discounting-aware-importance-sampling class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor *Discounting-aware Importance Sampling" href=#discounting-aware-importance-sampling><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>考慮一種情況：當 episodes 很長且 $\gamma \ll 1$，例如 100 steps 和 $\gamma = 0$。<br>
時間點 $t = 0$ 的時候，return 會是 $G_0 = R_1$，但 importance sampling ratio 會是 100 個乘積：</p>
<p>$$
\frac{\pi\left(A_{0} \mid S_{0}\right)}{b\left(A_{0} \mid S_{0}\right)} \frac{\pi\left(A_{1} \mid S_{1}\right)}{b\left(A_{1} \mid S_{1}\right)} \cdots \frac{\pi\left(A_{99} \mid S_{99}\right)}{b\left(A_{99} \mid S_{99}\right)}
$$</p>
<p>在 ordinary importance sampling 的情況下，return 會被這整個乘積放大，但實際需要的只有第一個，也就是 $\frac{\pi\left(A_{0} \mid S_{0}\right)}{b\left(A_{0} \mid S_{0}\right)}$，其他都是不相關的。期望值會是 1 不變，但變異數會不斷增加到無限大。</p>
<p><strong>Flat partial returns</strong>:</p>
<p>$$
\bar{G}_ {t: h} \doteq R_{t+1} + R_{t+2} + \cdots + R_{h}, \quad 0 \leq t&lt;h \leq T
$$</p>
<ul>
<li>$h$: horizon，return 所要考慮的時間長度。</li>
</ul>
<p>完整版的 return $G_t$ 可以視為 flat partial returns 的總和：</p>
<p>$$
\begin{aligned}
G_{t} \doteq & R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots+\gamma^{T-t-1} R_{T} \newline
=&(1-\gamma) R_{t+1} \newline
&+(1-\gamma) \gamma\left(R_{t+1}+R_{t+2}\right) \newline
&+(1-\gamma) \gamma^{2}\left(R_{t+1}+R_{t+2}+R_{t+3}\right) \newline
& \vdots \newline
&+(1-\gamma) \gamma^{T-t-2}\left(R_{t+1}+R_{t+2}+\cdots+R_{T-1}\right) \newline
&+\gamma^{T-t-1}\left(R_{t+1}+R_{t+2}+\cdots+R_{T}\right) \newline
=&(1-\gamma) \sum_{h=t+1}^{T-1} \gamma^{h-t-1} \bar{G}<em>{t: h}+\gamma^{T-t-1} \bar{G}</em>{t: T}
\end{aligned}
$$</p>
<p>對這個 flat partial returns 以 importance-sampling ratio 進行放大，oridinary importance sampling：</p>
<p>$$
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)}\left((1-\gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t: h-1} \bar{G}<em>{t: h}+\gamma^{T(t)-t-1} \rho</em>{t: T(t)-1} \bar{G}_{t: T(t)}\right)}{|\mathcal{T}(s)|}
$$</p>
<p>Weighted importance sampling:</p>
<p>$$
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)}\left((1-\gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t: h-1} \bar{G}<em>{t: h}+\gamma^{T(t)-t-1} \rho</em>{t: T(t)-1} \bar{G}<em>{t: T(t)}\right)}{\sum</em>{t \in \mathcal{T}(s)}\left((1-\gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t: h-1}+\gamma^{T(t)-t-1} \rho_{t: T(t)-1}\right)}
$$</p>
<div class=gdoc-page__anchorwrap>
<h2 id=per-decision-importance-sampling>
*Per-decision Importance Sampling
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#per-decision-importance-sampling class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor *Per-decision Importance Sampling" href=#per-decision-importance-sampling><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>另一種方式來減少變異數，可以用在即使有 discounting ($\gamma$) 的情況。</p>
<p>Off-policy estimators (value functions) 的分子，每一項的總和本身也是個總和，如下 (5.11)：</p>
<p>$$
\begin{aligned}
\rho_{t: T-1} G_{t} &=\rho_{t: T-1}\left(R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{T-t-1} R_{T}\right) \newline
&=\rho_{t: T-1} R_{t+1}+\gamma \rho_{t: T-1} R_{t+2}+\cdots+\gamma^{T-t-1} \rho_{t: T-1} R_{T}
\end{aligned}
$$</p>
<p>每一項都跟期望值有關，所以能用更簡單的方式描述。上式的每項都是一個隨機的 reward 和隨機的 importance-sampling ratio 的乘積，例如：</p>
<p>$$
\rho_{t: T-1} R_{t+1}=\frac{\pi\left(A_{t} \mid S_{t}\right)}{b\left(A_{t} \mid S_{t}\right)} \frac{\pi\left(A_{t+1} \mid S_{t+1}\right)}{b\left(A_{t+1} \mid S_{t+1}\right)} \frac{\pi\left(A_{t+2} \mid S_{t+2}\right)}{b\left(A_{t+2} \mid S_{t+2}\right)} \cdots \frac{\pi\left(A_{T-1} \mid S_{T-1}\right)}{b\left(A_{T-1} \mid S_{T-1}\right)} R_{t+1}
$$</p>
<ul>
<li>有人認為只有第一個和最後一個乘數 (reward) 有用，其他都是在這個 reward 之後發生的事件。</li>
<li>其他項的期望值是 $1$：</li>
</ul>
<p>$$
\mathbb{E}\left[\frac{\pi\left(A_{k} \mid S_{k}\right)}{b\left(A_{k} \mid S_{k}\right)}\right] \doteq \sum_{a} b\left(a \mid S_{k}\right) \frac{\pi\left(a \mid S_{k}\right)}{b\left(a \mid S_{k}\right)}=\sum_{a} \pi\left(a \mid S_{k}\right)=1
$$</p>
<p>每個乘數在期望值上都沒有作用，也就是：</p>
<p>$$
\mathbb{E}\left[\rho_{t: T-1} R_{t+1}\right]=\mathbb{E}\left[\rho_{t: t} R_{t+1}\right]
$$</p>
<p>如果重複這個流程，對於第 k 個 (5.11) 的子項，可以得到以下：</p>
<p>$$
\mathbb{E}\left[\rho_{t: T-1} R_{t+k}\right]=\mathbb{E}\left[\rho_{t: t+k-1} R_{t+k}\right]
$$</p>
<p>原本的期望值可以被寫成如下：</p>
<p>$$
\mathbb{E}\left[\rho_{t: T-1} G_{t}\right]=\mathbb{E}\left[\tilde{G}_{t}\right]
$$</p>
<p>其中，</p>
<p>$$
\tilde{G}_ {t}=\rho_{t: t} R_{t+1}+\gamma \rho_{t: t+1} R_{t+2}+\gamma^{2} \rho_{t: t+2} R_{t+3}+\cdots+\gamma^{T-t-1} \rho_{t: T-1} R_{T}
$$</p>
<p>這個稱為 <strong>per-decision</strong> importance sampling。</p>
<p>使用這個 importance sampling 套在 ordinary importance sampling 之後，可以得到以下：</p>
<p>$$
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \tilde{G}_{t}}{|\mathcal{T}(s)|}
$$</p>
<p>這個會產生較小的變異數。</p>
<div class=gdoc-page__anchorwrap>
<h2 id=summary>
Summary
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/#summary class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Summary" href=#summary><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>Monte Carlo 方法被提出來，以採樣的方式來學習 value functions 和 optimal policies。對比於 DP 法，有至少三個優勢：</p>
<ul>
<li>直接跟環境互動，不需要環境模型。</li>
<li>可以用模擬 (simulation) 或是採樣模型 (sample models)</li>
<li>有簡單有效的方式把 Monte Carlo 方法用來聚焦處理 所有狀態中的一個小子集 (it is easy and efficient to focus Monte Carlo methods on a small subset of the states)，在 Ch 8. 進行討論。</li>
<li>較少違反 Markov 性質，因為它不會基於連續狀態的 value 來更新 value。也就是沒有 bootstrap。</li>
</ul>
</article>
<div class="gdoc-page__footer flex flex-wrap justify-between">
<span class=gdoc-page__nav>
<a class="gdoc-page__nav--prev flex align-center" href=/RL-notes/sutton/tabular-solution-methods/dynamic-programming/ title="Ch 4. Dynamic Programming"> Ch 4. Dynamic Programming</a>
</span>
<span class=gdoc-page__nav>
<a class="gdoc-page__nav--next flex align-center" href=/RL-notes/sutton/tabular-solution-methods/temporal-difference-learning/ title="Ch 6. Temporal-Difference Learning">Ch 6. Temporal-Difference Learning </a>
</span>
</div>
</div>
</main>
<footer class=gdoc-footer>
<div class="container flex">
<div class="flex flex-wrap">
<span class="gdoc-footer__item gdoc-footer__item--row">
Built with <a href=https://gohugo.io/ class=gdoc-footer__link>Hugo</a> and<svg class="icon gdoc_heart"><use xlink:href="#gdoc_heart"/></svg>
</span>
</div>
<div class="flex flex-25 justify-end">
<span class=gdoc-footer__item>
<a class="gdoc-footer__link fake-link" href=# aria-label="Back to top"><svg class="icon gdoc_keyborad_arrow_up"><use xlink:href="#gdoc_keyborad_arrow_up"/></svg> <span class=hidden-mobile>Back to top</span>
</a>
</span>
</div>
</div>
</footer>
</div>
<script defer src=/RL-notes/js/zh.search.min.62011e6f91033499983c850099ab3e1c2c53d1691525ece5783685e0ddd1c310.js></script>
<script defer src=/RL-notes/js/clipboard-27784b7376.min.js></script>
<script defer src=/RL-notes/js/clipboard-loader-f0b5fbd5f6.min.js></script>
</body>
</html>