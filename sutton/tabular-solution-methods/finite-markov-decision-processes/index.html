<!doctype html><html lang=zh class=color-toggle-hidden>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=color-scheme content="light dark">
<meta name=description content="The Agent-Environment Interface Goals and Rewards Returns and Episodes Episodic tasks Continuing tasks Discounting Unified Notation for Episodic and Continuing Tasks Policies and Value Functions Optimal Policies and Optimal Value Functions Optimality and Approximation 這章談論的問題是 associative aspect: 在不同狀況選擇不同的動作。 MDPs 是一個經典的連續決策的形">
<title>Ch 3. Finite Markov Decision Processes | Reinforcement Learning Notes</title>
<link rel=icon href=/RL-notes/favicon/favicon-32x32.png type=image/x-icon>
<script src=/RL-notes/js/darkmode-ce906ea916.min.js></script>
<link rel=preload as=font href=/RL-notes/fonts/Metropolis.woff2 type=font/woff2 crossorigin=anonymous>
<link rel=preload as=font href=/RL-notes/fonts/LiberationSans.woff2 type=font/woff2 crossorigin=anonymous>
<link rel=preload href=/RL-notes/main-1be624d457.min.css as=style>
<link rel=stylesheet href=/RL-notes/main-1be624d457.min.css media=all>
<link rel=preload href=/RL-notes/mobile-3fc330242c.min.css as=style>
<link rel=stylesheet href=/RL-notes/mobile-3fc330242c.min.css media="screen and (max-width: 45rem)">
<link rel=preload href=/RL-notes/print-f79fc3e5d7.min.css as=style>
<link rel=stylesheet href=/RL-notes/print-f79fc3e5d7.min.css media=print>
<link rel=preload href=/RL-notes/custom.css as=style>
<link rel=stylesheet href=/RL-notes/custom.css media=all>
</head>
<body itemscope itemtype=https://schema.org/WebPage><svg class="svg-sprite" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_back" xmlns="http://www.w3.org/2000/svg"><path d="M24 10.526v2.947H5.755l8.351 8.421-2.105 2.105-12-12 12-12 2.105 2.105-8.351 8.421H24z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_left_alt" xmlns="http://www.w3.org/2000/svg"><path d="M5.965 10.526V6.035L0 12l5.965 5.965v-4.491H24v-2.947H5.965z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_arrow_right_alt" xmlns="http://www.w3.org/2000/svg"><path d="M18.035 10.526V6.035L24 12l-5.965 5.965v-4.491H0v-2.947h18.035z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_bitbucket" xmlns="http://www.w3.org/2000/svg"><path d="M15.905 13.355c.189 1.444-1.564 2.578-2.784 1.839-1.375-.602-1.375-2.784-.034-3.403 1.151-.705 2.818.223 2.818 1.564zm1.907-.361c-.309-2.44-3.076-4.056-5.328-3.042-1.426.636-2.389 2.148-2.32 3.747.086 2.097 2.08 3.815 4.176 3.626s3.729-2.234 3.472-4.331zm4.108-9.315c-.756-.997-2.045-1.169-3.179-1.358-3.214-.516-6.513-.533-9.727.034-1.066.172-2.269.361-2.939 1.323 1.1 1.031 2.664 1.186 4.073 1.358 2.544.327 5.156.344 7.699.017 1.426-.172 3.008-.309 4.073-1.375zm.979 17.788c-.481 1.684-.206 3.953-1.994 4.932-3.076 1.701-6.806 1.89-10.191 1.289-1.787-.327-3.884-.894-4.864-2.578-.43-1.65-.705-3.334-.98-5.018l.103-.275.309-.155c5.121 3.386 12.288 3.386 17.427.0.808.241.206 1.22.189 1.805zM26.01 4.951c-.584 3.764-1.255 7.51-1.908 11.257-.189 1.1-1.255 1.719-2.148 2.183-3.214 1.615-6.96 1.89-10.483 1.512-2.389-.258-4.829-.894-6.771-2.389-.911-.705-.911-1.908-1.083-2.922-.602-3.523-1.289-7.046-1.719-10.604.206-1.547 1.942-2.217 3.231-2.698C6.848.654 8.686.362 10.508.19c3.884-.378 7.854-.241 11.618.859 1.341.395 2.784.945 3.695 2.097.412.533.275 1.203.189 1.805z"/></symbol><symbol viewBox="-2.29 -2.29 28.57 28.57" id="gdoc_bookmark" xmlns="http://www.w3.org/2000/svg"><path d="M15.268 4.392q.868.0 1.532.638t.664 1.506v17.463l-7.659-3.268-7.608 3.268V6.536q0-.868.664-1.506t1.532-.638h10.876zm4.34 14.144V4.392q0-.868-.638-1.532t-1.506-.664H6.537q0-.868.664-1.532T8.733.0h10.876q.868.0 1.532.664t.664 1.532v17.412z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_auto" xmlns="http://www.w3.org/2000/svg"><path d="M16.846 18.938h2.382L15.22 7.785h-2.44L8.772 18.938h2.382l.871-2.44h3.95zm7.087-9.062L27.999 14l-4.066 4.124v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809zm-11.385 4.937L14 10.282l1.452 4.531h-2.904z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_dark" xmlns="http://www.w3.org/2000/svg"><path d="M14 21.435q3.079.0 5.257-2.178T21.435 14t-2.178-5.257T14 6.565q-1.51.0-3.079.697 1.917.871 3.108 2.701T15.22 14t-1.191 4.037-3.108 2.701q1.568.697 3.079.697zm9.933-11.559L27.999 14l-4.066 4.124v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_brightness_light" xmlns="http://www.w3.org/2000/svg"><path d="M14 21.435q3.079.0 5.257-2.178T21.435 14t-2.178-5.257T14 6.565 8.743 8.743 6.565 14t2.178 5.257T14 21.435zm9.933-3.311v5.809h-5.809L14 27.999l-4.124-4.066H4.067v-5.809L.001 14l4.066-4.124V4.067h5.809L14 .001l4.124 4.066h5.809v5.809L27.999 14z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_cloud_off" xmlns="http://www.w3.org/2000/svg"><path d="M9.023 10.5H7q-1.914.0-3.281 1.395t-1.367 3.309 1.367 3.281T7 19.852h11.375zM3.5 4.976l1.477-1.477L24.5 23.022l-1.477 1.477-2.352-2.297H6.999q-2.898.0-4.949-2.051t-2.051-4.949q0-2.844 1.969-4.867t4.758-2.133zm19.086 5.578q2.242.164 3.828 1.832T28 16.351q0 3.008-2.461 4.758l-1.695-1.695q1.805-.984 1.805-3.063.0-1.422-1.039-2.461t-2.461-1.039h-1.75v-.602q0-2.68-1.859-4.539t-4.539-1.859q-1.531.0-2.953.711l-1.75-1.695Q11.431 3.5 14.001 3.5q2.953.0 5.496 2.078t3.09 4.977z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_code" xmlns="http://www.w3.org/2000/svg"><path d="M9.917 24.5a1.75 1.75.0 10-3.501.001 1.75 1.75.0 003.501-.001zm0-21a1.75 1.75.0 10-3.501.001A1.75 1.75.0 009.917 3.5zm11.666 2.333a1.75 1.75.0 10-3.501.001 1.75 1.75.0 003.501-.001zm1.75.0a3.502 3.502.0 01-1.75 3.026c-.055 6.581-4.721 8.039-7.82 9.023-2.898.911-3.846 1.349-3.846 3.117v.474a3.502 3.502.0 011.75 3.026c0 1.932-1.568 3.5-3.5 3.5s-3.5-1.568-3.5-3.5c0-1.294.711-2.424 1.75-3.026V6.526A3.502 3.502.0 014.667 3.5c0-1.932 1.568-3.5 3.5-3.5s3.5 1.568 3.5 3.5a3.502 3.502.0 01-1.75 3.026v9.06c.93-.456 1.914-.766 2.807-1.039 3.391-1.075 5.323-1.878 5.359-5.687a3.502 3.502.0 01-1.75-3.026c0-1.932 1.568-3.5 3.5-3.5s3.5 1.568 3.5 3.5z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_date" xmlns="http://www.w3.org/2000/svg"><path d="M27.192 28.844V11.192H4.808v17.652h22.384zm0-25.689q1.277.0 2.253.976t.976 2.253v22.459q0 1.277-.976 2.216t-2.253.939H4.808q-1.352.0-2.291-.901t-.939-2.253V6.385q0-1.277.939-2.253t2.291-.976h1.577V.001h3.23v3.155h12.769V.001h3.23v3.155h1.577zm-3.155 11.267v3.155h-3.23v-3.155h3.23zm-6.46.0v3.155h-3.155v-3.155h3.155zm-6.384.0v3.155h-3.23v-3.155h3.23z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_download" xmlns="http://www.w3.org/2000/svg"><path d="M2.866 28.209h26.269v3.79H2.866v-3.79zm26.268-16.925L16 24.418 2.866 11.284h7.493V.001h11.283v11.283h7.493z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_email" xmlns="http://www.w3.org/2000/svg"><path d="M28.845 9.615v-3.23L16 14.422 3.155 6.385v3.23L16 17.577zm0-6.46q1.277.0 2.216.977T32 6.385v19.23q0 1.277-.939 2.253t-2.216.977H3.155q-1.277.0-2.216-.977T0 25.615V6.385q0-1.277.939-2.253t2.216-.977h25.69z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_git" xmlns="http://www.w3.org/2000/svg"><path d="M27.472 12.753 15.247.529a1.803 1.803.0 00-2.55.0l-2.84 2.84 2.137 2.137a2.625 2.625.0 013.501 3.501l3.499 3.499a2.625 2.625.0 11-1.237 1.237l-3.499-3.499c-.083.04-.169.075-.257.106v7.3a2.626 2.626.0 11-1.75.0v-7.3a2.626 2.626.0 01-1.494-3.607L8.62 4.606l-8.09 8.09a1.805 1.805.0 000 2.551l12.225 12.224a1.803 1.803.0 002.55.0l12.168-12.168a1.805 1.805.0 000-2.551z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_github" xmlns="http://www.w3.org/2000/svg"><path d="M16 .394c8.833.0 15.999 7.166 15.999 15.999.0 7.062-4.583 13.062-10.937 15.187-.813.146-1.104-.354-1.104-.771.0-.521.021-2.25.021-4.396.0-1.5-.5-2.458-1.083-2.958 3.562-.396 7.312-1.75 7.312-7.896.0-1.75-.625-3.167-1.646-4.291.167-.417.708-2.042-.167-4.25-1.333-.417-4.396 1.646-4.396 1.646a15.032 15.032.0 00-8 0S8.937 6.602 7.603 7.018c-.875 2.208-.333 3.833-.167 4.25-1.021 1.125-1.646 2.542-1.646 4.291.0 6.125 3.729 7.5 7.291 7.896-.458.417-.875 1.125-1.021 2.146-.917.417-3.25 1.125-4.646-1.333-.875-1.521-2.458-1.646-2.458-1.646-1.562-.021-.104.979-.104.979 1.042.479 1.771 2.333 1.771 2.333.938 2.854 5.396 1.896 5.396 1.896.0 1.333.021 2.583.021 2.979.0.417-.292.917-1.104.771C4.582 29.455-.001 23.455-.001 16.393-.001 7.56 7.165.394 15.998.394zM6.063 23.372c.042-.083-.021-.187-.146-.25-.125-.042-.229-.021-.271.042-.042.083.021.187.146.25.104.062.229.042.271-.042zm.646.709c.083-.062.062-.208-.042-.333-.104-.104-.25-.146-.333-.062-.083.062-.062.208.042.333.104.104.25.146.333.062zm.625.937c.104-.083.104-.25.0-.396-.083-.146-.25-.208-.354-.125-.104.062-.104.229.0.375s.271.208.354.146zm.875.875c.083-.083.042-.271-.083-.396-.146-.146-.333-.167-.417-.062-.104.083-.062.271.083.396.146.146.333.167.417.062zm1.187.521c.042-.125-.083-.271-.271-.333-.167-.042-.354.021-.396.146s.083.271.271.312c.167.062.354.0.396-.125zm1.313.104c0-.146-.167-.25-.354-.229-.187.0-.333.104-.333.229.0.146.146.25.354.229.187.0.333-.104.333-.229zm1.208-.208c-.021-.125-.187-.208-.375-.187-.187.042-.312.167-.292.312.021.125.187.208.375.167s.312-.167.292-.292z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_gitlab" xmlns="http://www.w3.org/2000/svg"><path d="M1.629 11.034 14 26.888.442 17.048a1.09 1.09.0 01-.39-1.203l1.578-4.811zm7.217.0h10.309l-5.154 15.854zM5.753 1.475l3.093 9.559H1.63l3.093-9.559a.548.548.0 011.031.0zm20.618 9.559 1.578 4.811c.141.437-.016.922-.39 1.203l-13.558 9.84 12.371-15.854zm0 0h-7.216l3.093-9.559a.548.548.0 011.031.0z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_heart" xmlns="http://www.w3.org/2000/svg"><path d="M16 29.714a1.11 1.11.0 01-.786-.321L4.072 18.643c-.143-.125-4.071-3.714-4.071-8 0-5.232 3.196-8.357 8.535-8.357 3.125.0 6.053 2.464 7.464 3.857 1.411-1.393 4.339-3.857 7.464-3.857 5.339.0 8.535 3.125 8.535 8.357.0 4.286-3.928 7.875-4.089 8.035L16.785 29.392c-.214.214-.5.321-.786.321z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_home" xmlns="http://www.w3.org/2000/svg"><path d="M24.003 15.695v8.336c0 .608-.504 1.111-1.111 1.111h-6.669v-6.669h-4.446v6.669H5.108a1.119 1.119.0 01-1.111-1.111v-8.336c0-.035.017-.069.017-.104L14 7.359l9.986 8.232a.224.224.0 01.017.104zm3.873-1.198-1.077 1.285a.578.578.0 01-.365.191h-.052a.547.547.0 01-.365-.122L14 5.831 1.983 15.851a.594.594.0 01-.417.122.578.578.0 01-.365-.191L.124 14.497a.57.57.0 01.069-.781L12.679 3.314c.729-.608 1.91-.608 2.64.0l4.237 3.543V3.471c0-.313.243-.556.556-.556h3.334c.313.0.556.243.556.556v7.085l3.803 3.161c.226.191.26.556.069.781z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_keyborad_arrow_down" xmlns="http://www.w3.org/2000/svg"><path d="M3.281 5.36 14 16.079 24.719 5.36 28 8.641l-14 14-14-14z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_keyborad_arrow_left" xmlns="http://www.w3.org/2000/svg"><path d="M25.875 28.25 22.125 32 6.126 16.001 22.125.002l3.75 3.75-12.25 12.25z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_keyborad_arrow_right" xmlns="http://www.w3.org/2000/svg"><path d="M6.125 28.25 18.375 16 6.125 3.75 9.875.0l15.999 15.999L9.875 31.998z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_keyborad_arrow_up" xmlns="http://www.w3.org/2000/svg"><path d="M24.719 22.64 14 11.921 3.281 22.64.0 19.359l14-14 14 14z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_link" xmlns="http://www.w3.org/2000/svg"><path d="M24.037 7.963q3.305.0 5.634 2.366T32 16t-2.329 5.671-5.634 2.366h-6.46v-3.08h6.46q2.028.0 3.493-1.465t1.465-3.493-1.465-3.493-3.493-1.465h-6.46v-3.08h6.46zM9.615 17.578v-3.155h12.77v3.155H9.615zM3.005 16q0 2.028 1.465 3.493t3.493 1.465h6.46v3.08h-6.46q-3.305.0-5.634-2.366T0 16.001t2.329-5.671 5.634-2.366h6.46v3.08h-6.46q-2.028.0-3.493 1.465t-1.465 3.493z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_menu" xmlns="http://www.w3.org/2000/svg"><path d="M.001 5.334h31.998v3.583H.001V5.334zm0 12.416v-3.5h31.998v3.5H.001zm0 8.916v-3.583h31.998v3.583H.001z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_notification" xmlns="http://www.w3.org/2000/svg"><path d="M22.615 19.384l2.894 2.894v1.413H2.49v-1.413l2.894-2.894V12.25q0-3.365 1.716-5.856t4.745-3.231v-1.01q0-.875.606-1.514T13.999.0t1.548.639.606 1.514v1.01q3.029.74 4.745 3.231t1.716 5.856v7.134zM14 27.999q-1.211.0-2.053-.808t-.841-2.019h5.788q0 1.144-.875 1.986T14 27.999z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_path" xmlns="http://www.w3.org/2000/svg"><path d="M28 12.62h-9.793V8.414h-2.826v11.173h2.826v-4.206H28V26.62h-9.793v-4.206H12.62v-14H9.794v4.206H.001V1.381h9.793v4.206h8.413V1.381H28V12.62z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_person" xmlns="http://www.w3.org/2000/svg"><path d="M16 20.023q5.052.0 10.526 2.199t5.473 5.754v4.023H0v-4.023q0-3.555 5.473-5.754t10.526-2.199zM16 16q-3.275.0-5.614-2.339T8.047 8.047t2.339-5.661T16 0t5.614 2.386 2.339 5.661-2.339 5.614T16 16z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_search" xmlns="http://www.w3.org/2000/svg"><path d="M11.925 20.161q3.432.0 5.834-2.402t2.402-5.834-2.402-5.834-5.834-2.402-5.834 2.402-2.402 5.834 2.402 5.834 5.834 2.402zm10.981.0L32 29.255 29.255 32l-9.094-9.094v-1.458l-.515-.515q-3.26 2.831-7.721 2.831-4.976.0-8.45-3.432T.001 11.925t3.474-8.45 8.45-3.474 8.407 3.474 3.432 8.45q0 1.802-.858 4.075t-1.973 3.646l.515.515h1.458z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_shield" xmlns="http://www.w3.org/2000/svg"><path d="M22.167 15.166V3.5h-8.166v20.726c.93-.492 2.424-1.349 3.883-2.497 1.95-1.531 4.284-3.919 4.284-6.562zm3.499-13.999v14c0 7.674-10.737 12.523-11.192 12.724-.146.073-.31.109-.474.109s-.328-.036-.474-.109c-.456-.201-11.192-5.049-11.192-12.724v-14C2.334.529 2.863.0 3.501.0H24.5c.638.0 1.167.529 1.167 1.167z"/></symbol><symbol viewBox="-2.29 -2.29 32.57 32.57" id="gdoc_tags" xmlns="http://www.w3.org/2000/svg"><path d="M6.606 7.549c0-1.047-.84-1.887-1.887-1.887s-1.887.84-1.887 1.887.84 1.887 1.887 1.887 1.887-.84 1.887-1.887zm15.732 8.493c0 .501-.206.988-.546 1.327l-7.239 7.254c-.354.339-.84.546-1.342.546s-.988-.206-1.327-.546L1.342 14.066C.59 13.329.0 11.899.0 10.852V4.718a1.9 1.9.0 011.887-1.887h6.134c1.047.0 2.477.59 3.229 1.342L21.792 14.7c.339.354.546.84.546 1.342zm5.661.0c0 .501-.206.988-.546 1.327l-7.239 7.254c-.354.339-.84.546-1.342.546-.767.0-1.15-.354-1.651-.87l6.93-6.93c.339-.339.546-.826.546-1.327s-.206-.988-.546-1.342L13.609 4.173c-.752-.752-2.182-1.342-3.229-1.342h3.303c1.047.0 2.477.59 3.229 1.342L27.454 14.7c.339.354.546.84.546 1.342z"/></symbol><symbol viewBox="-2.29 -2.29 36.57 36.57" id="gdoc_timer" xmlns="http://www.w3.org/2000/svg"><path d="M16 29q4.428.0 7.536-3.143t3.107-7.571-3.107-7.536T16 7.643 8.464 10.75t-3.107 7.536 3.107 7.571T16 29zM26.714 9.786q1.214 1.571 2.107 4.036t.893 4.464q0 5.643-4 9.678T16 32t-9.714-4.036-4-9.678 4-9.678T16 4.572q1.929.0 4.464.929t4.107 2.143l2.143-2.214q1.143.929 2.143 2.143zM14.5 19.857v-9.143h3v9.143h-3zM20.571.001v3.071h-9.143V.001h9.143z"/></symbol></svg>
<div class=wrapper>
<input type=checkbox class=hidden id=menu-control>
<input type=checkbox class=hidden id=menu-header-control>
<header class=gdoc-header>
<div class="container flex align-center justify-between">
<label for=menu-control class=gdoc-nav__control><svg class="icon gdoc_menu"><title>Open Nav Menu</title><use xlink:href="#gdoc_menu"/></svg><svg class="icon gdoc_arrow_back"><title>Close Nav Menu</title><use xlink:href="#gdoc_arrow_back"/></svg>
</label>
<a class=gdoc-header__link href=https://aquastripe.github.io/RL-notes/>
<span class="gdoc-brand flex align-center">
<img class=gdoc-brand__img src=/RL-notes/brand.svg alt>
<span class=gdoc-brand__title>Reinforcement Learning Notes</span>
</span>
</a>
<div class=gdoc-menu-header>
<span id=gdoc-dark-mode><svg class="icon gdoc_brightness_dark"><title>Toggle Dark/Light/Auto mode</title><use xlink:href="#gdoc_brightness_dark"/></svg><svg class="icon gdoc_brightness_light"><title>Toggle Dark/Light/Auto mode</title><use xlink:href="#gdoc_brightness_light"/></svg><svg class="icon gdoc_brightness_auto"><title>Toggle Dark/Light/Auto mode</title><use xlink:href="#gdoc_brightness_auto"/></svg>
</span>
</div>
</div>
</header>
<main class="container flex flex-even">
<aside class=gdoc-nav>
<nav>
<div class=gdoc-search><svg class="icon gdoc_search"><use xlink:href="#gdoc_search"/></svg>
<input type=text id=gdoc-search-input class=gdoc-search__input placeholder=Search... aria-label=Search maxlength=64>
<div class="gdoc-search__spinner spinner hidden"></div>
<ul id=gdoc-search-results class=gdoc-search__list></ul>
</div>
<section class=gdoc-nav--main>
<h2>Navigation</h2>
<ul class=gdoc-nav__list>
<li>
<span class=flex>Reinforcement Learning: An Introduction</span>
<ul class=gdoc-nav__list>
<li>
<span class=flex>
<a href=/RL-notes/sutton/introduction/ class=gdoc-nav__entry>
Ch 1. Introduction
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/sutton/tabular-solution-methods/ class=gdoc-nav__entry>
Part I: Tabular Solution Methods
</a>
</span>
<ul class=gdoc-nav__list>
<li>
<span class=flex>
<a href=/RL-notes/sutton/tabular-solution-methods/multi-armed-bandits/ class=gdoc-nav__entry>
Ch 2. Multi-Armed Bandits
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/sutton/tabular-solution-methods/finite-markov-decision-processes/ class="gdoc-nav__entry is-active">
Ch 3. Finite Markov Decision Processes
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/sutton/tabular-solution-methods/dynamic-programming/ class=gdoc-nav__entry>
Ch 4. Dynamic Programming
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/ class=gdoc-nav__entry>
Ch 5. Monte Carlo Methods
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/sutton/tabular-solution-methods/temporal-difference-learning/ class=gdoc-nav__entry>
Ch 6. Temporal-Difference Learning
</a>
</span>
</li>
<li>
<span class=flex>
<a href=/RL-notes/sutton/tabular-solution-methods/n-step-bootstrapping/ class=gdoc-nav__entry>
Ch 7. n-step Bootstrapping
</a>
</span>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section class=gdoc-nav--more>
<h2>More</h2>
<ul class=gdoc-nav__list>
<li>
<span class=flex><svg class="icon gdoc_github"><use xlink:href="#gdoc_github"/></svg>
<a href=https://github.com/aquastripe/RL-notes class=gdoc-nav__entry>
View Source
</a>
</span>
</li>
</ul>
</section>
</nav>
</aside>
<div class=gdoc-page>
<div class="gdoc-page__header flex flex-wrap
justify-between
hidden-mobile" itemprop=breadcrumb>
<div><svg class="icon gdoc_path hidden-mobile"><use xlink:href="#gdoc_path"/></svg>
<ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList>
<li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemscope itemtype=https://schema.org/WebPage itemprop=item itemid=/RL-notes/ href=/RL-notes/><span itemprop=name>Reinforcement Learning Notes</span></a><meta itemprop=position content="2"></li><li> / </li><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemscope itemtype=https://schema.org/WebPage itemprop=item itemid=/RL-notes/sutton/ href=/RL-notes/sutton/><span itemprop=name>Sutton</span></a><meta itemprop=position content="3"></li><li> / </li><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemscope itemtype=https://schema.org/WebPage itemprop=item itemid=/RL-notes/sutton/tabular-solution-methods/ href=/RL-notes/sutton/tabular-solution-methods/><span itemprop=name>Part I: Tabular Solution Methods</span></a><meta itemprop=position content="4"></li><li> / </li><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Ch 3. Finite Markov Decision Processes</span><meta itemprop=position content="5"></li>
</ol>
</div>
</div>
<script>MathJax={tex:{inlineMath:[["$","$"]]},displayMath:[["$$","$$"],["[[","]]"]],svg:{fontCache:"global"}}</script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<article class="gdoc-markdown gdoc-markdown__align--left">
<h1>Ch 3. Finite Markov Decision Processes</h1>
<div class="gdoc-toc gdoc-toc__level--6"><nav id=TableOfContents><ul>
<li><a href=#the-agent-environment-interface>The Agent-Environment Interface</a></li>
<li><a href=#goals-and-rewards>Goals and Rewards</a></li>
<li><a href=#returns-and-episodes>Returns and Episodes</a>
<ul>
<li><a href=#episodic-tasks>Episodic tasks</a></li>
<li><a href=#continuing-tasks>Continuing tasks</a></li>
<li><a href=#discounting>Discounting</a></li>
</ul>
</li>
<li><a href=#unified-notation-for-episodic-and-continuing-tasks>Unified Notation for Episodic and Continuing Tasks</a></li>
<li><a href=#policies-and-value-functions>Policies and Value Functions</a></li>
<li><a href=#optimal-policies-and-optimal-value-functions>Optimal Policies and Optimal Value Functions</a></li>
<li><a href=#optimality-and-approximation>Optimality and Approximation</a></li>
</ul></nav><hr></div>
<p>這章談論的問題是 associative aspect: 在不同狀況選擇不同的動作。</p>
<p>MDPs 是一個經典的連續決策的形式，動作會影響當下的獎勵、子序列的狀態，還有未來的獎勵。</p>
<p>權衡:</p>
<ul>
<li>immediate reward (當下的獎勵)</li>
<li>delayed reward (延遲的獎勵)</li>
</ul>
<p>差異:</p>
<ul>
<li>bandit problem: 估計價值 $q_ {* }(a)$</li>
<li>MDPs: 估計
<ul>
<li>狀態動作價值 $q_ {* }(s, a)$</li>
<li>狀態價值 $v_ {* }(s)$ (given optimal action selections)</li>
</ul>
</li>
</ul>
<p>關鍵元素:</p>
<ul>
<li>mathematical structure</li>
<li>returns</li>
<li>value functions</li>
<li>Bellman equations</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h2 id=the-agent-environment-interface>
The Agent-Environment Interface
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/finite-markov-decision-processes/#the-agent-environment-interface class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor The Agent-Environment Interface" href=#the-agent-environment-interface><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p><img src=3.1.png alt></p>
<p>$$S_ 0, A_ 0, R_ 1, S_ 1, A_ 1, R_ 2, \dots$$</p>
<p>在有限的 MDP, 隨機變數 $R_ t$ 和 $S_ t$ 有隨機機率分佈只依賴於先前的動作和狀態。</p>
<p>$$p( s^{\prime}, r \mid s, a) \doteq \operatorname{Pr} \lbrace S_ {t}=s^{\prime}, R_ {t}=r \mid S_ {t-1}=s, A_ {t-1}=a \rbrace$$</p>
<p>for all $s', s \in \mathcal{S}, r \in \mathcal{R}$, and $a \in \mathcal{A}(s)$.</p>
<p>函數 $p: \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ 定義了 MDP 的 * * dynamics* * .</p>
<p>條件機率的符號提醒我們：$p$ 對每個選擇 $s$ 和 $a$ 指定一個機率分佈，也就是:</p>
<p>$$\sum_ {s^{\prime} \in \mathcal{S}} \sum_ {r \in \mathcal{R}} p (s^{\prime}, r \mid s, a ) = 1, \text { for all } s \in \mathcal{S}, a \in \mathcal{A}(s)$$</p>
<p>$S_ t$ 和 $A_ t$ 的機率只依賴於先前的狀態和動作, $S_ {t-1}$ and $A_ {t-1}$.</p>
<ul>
<li>這是一個馬可夫性質 (Markov property)。</li>
</ul>
<p>對於環境，我們可以計算狀態轉移機率 $p: \mathcal{S} \times \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$</p>
<p>$$p\left(s^{\prime} \mid s, a\right) \doteq \operatorname{Pr}\lbrace S_ {t}=s^{\prime} \mid S_ {t-1}=s, A_ {t-1}=a\rbrace=\sum_ {r \in \mathcal{R}} p\left(s^{\prime}, r \mid s, a\right)$$</p>
<p>計算 狀態-動作 (state-action pairs) 的期望獎勵 (expected rewards) $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$</p>
<p>$$r(s, a) \doteq \mathbb{E}\left[R_ {t} \mid S_ {t-1}=s, A_ {t-1}=a\right]=\sum_ {r \in \mathcal{R}} r \sum_ {s^{\prime} \in \mathcal{S}} p\left(s^{\prime}, r \mid s, a\right)$$</p>
<p>計算 狀態-動作-下個狀態 (state-action-next-state pairs) 的期望獎勵 $r: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$</p>
<p>$$r\left(s, a, s^{\prime}\right) \doteq \mathbb{E}\left[R_ {t} \mid S_ {t-1}=s, A_ {t-1}=a, S_ {t}=s^{\prime}\right]=\sum_ {r \in \mathcal{R}} r \frac{p\left(s^{\prime}, r \mid s, a\right)}{p\left(s^{\prime} \mid s, a\right)}$$</p>
<p>通用的規則是：任何事情無法被智慧主體任意改變的，都被視為它的外部，也就成為環境的一部分。智慧主體和環境的邊界在於智慧主體的可以絕對控制的範圍。</p>
<div class=gdoc-page__anchorwrap>
<h2 id=goals-and-rewards>
Goals and Rewards
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/finite-markov-decision-processes/#goals-and-rewards class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Goals and Rewards" href=#goals-and-rewards><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>Reward hypothesis:</p>
<p>可以把目標視為最大化累積獎勵的期望值。</p>
<p>The reward signal is your way of communicating to the agent what you want to achieve, not how you want it to achieve.</p>
<div class=gdoc-page__anchorwrap>
<h2 id=returns-and-episodes>
Returns and Episodes
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/finite-markov-decision-processes/#returns-and-episodes class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Returns and Episodes" href=#returns-and-episodes><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>We have said that the agent’s goal is to maximize the cumulative reward it receives in the long run.</p>
<p>$$G_ {t} \doteq R_ {t+1}+R_ {t+2}+R_ {t+3}+\cdots+R_ {T}$$</p>
<ul>
<li>Return: 獎勵序列的某個特定函數</li>
<li>Episodes: 當智慧主體和環境的互動自然的分成子序列的時刻</li>
<li>Terminal state: 每個 episode 在特別的狀態下結束，之後會重整為起始狀態。</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h3 id=episodic-tasks>
Episodic tasks
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/finite-markov-decision-processes/#episodic-tasks class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Episodic tasks" href=#episodic-tasks><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p>有 episode 的任務稱作 episodic tasks。</p>
<ul>
<li>non-terminal states: $\mathcal{S}$</li>
<li>terminal state: $\mathcal{S}^+$</li>
<li>終止時間 $T$ 是一個隨機變數，會隨著 episode 變化。</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h3 id=continuing-tasks>
Continuing tasks
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/finite-markov-decision-processes/#continuing-tasks class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Continuing tasks" href=#continuing-tasks><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p>$T=\infty$</p>
<p>Thus, in this book we usually use a definition of return that is slightly more complex conceptually but much simpler mathematically</p>
<div class=gdoc-page__anchorwrap>
<h3 id=discounting>
Discounting
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/finite-markov-decision-processes/#discounting class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Discounting" href=#discounting><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h3>
</div><p>$$G_ {t} \doteq R_ {t+1}+\gamma R_ {t+2}+\gamma^{2} R_ {t+3}+\cdots=\sum_ {k=0}^{\infty} \gamma^{k} R_ {t+k+1}$$</p>
<ul>
<li>$\gamma$: a parameter $0 \le \gamma \le 1$</li>
<li>$\gamma \lt 1$: $G_ t$ is a finite value as long as the reward sequence ${R_ k}$ is bounded.
<ul>
<li>$R_ k$ 數值有上限</li>
</ul>
</li>
<li>$\gamma = 0$: the agent is myopic</li>
<li>$\gamma \approx 1$: the agent is farsighted</li>
</ul>
<p>$$\begin{aligned}
G_ {t} & \doteq R_ {t+1}+\gamma R_ {t+2}+\gamma^{2} R_ {t+3}+\gamma^{3} R_ {t+4}+\cdots \newline
&=R_ {t+1}+\gamma\left(R_ {t+2}+\gamma R_ {t+3}+\gamma^{2} R_ {t+4}+\cdots\right) \newline
&=R_ {t+1}+\gamma G_ {t+1}
\end{aligned}$$</p>
<p>即使是無限項，reward 還是有限的</p>
<p>e.g. if the reward is constant $+1$</p>
<p>$$G_ {t}=\sum_ {k=0}^{\infty} \gamma^{k}=\frac{1}{1-\gamma}$$</p>
<div class=gdoc-page__anchorwrap>
<h2 id=unified-notation-for-episodic-and-continuing-tasks>
Unified Notation for Episodic and Continuing Tasks
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/finite-markov-decision-processes/#unified-notation-for-episodic-and-continuing-tasks class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Unified Notation for Episodic and Continuing Tasks" href=#unified-notation-for-episodic-and-continuing-tasks><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>可以藉由把 episode termination 是否進入 absorbing state 來統一兩種 tasks</p>
<p>吸收狀態 (absorbing state): 狀態只會轉移到自己，並且產生 0 reward</p>
<p><img src=3.4.png alt></p>
<p>重新定義 return 的計算，省略了 episode 次數：</p>
<p>$$G_ {t} \doteq \sum_ {k=t+1}^{T} \gamma^{k-t-1} R_ {k}$$</p>
<div class=gdoc-page__anchorwrap>
<h2 id=policies-and-value-functions>
Policies and Value Functions
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/finite-markov-decision-processes/#policies-and-value-functions class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Policies and Value Functions" href=#policies-and-value-functions><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>Policy: 映射 states 到選擇某個動作的機率</p>
<ul>
<li>$\pi (a | s)$</li>
</ul>
<p>Value functions: 給定 states 或 state-action pairs，估計 agent 的 states 有多好。</p>
<ul>
<li>$v_ {\pi} (s)$</li>
</ul>
<p>$$v_ {\pi}(s) \doteq \mathbb{E}_ {\pi}\left[G_ {t} \mid S_ {t}=s\right]=\mathbb{E}_ {\pi}\left[\sum_ {k=0}^{\infty} \gamma^{k} R_ {t+k+1} \mid S_ {t}=s\right], \text { for all } s \in \mathcal{S}$$</p>
<ul>
<li>the state-action-value function for policy $\pi$</li>
</ul>
<p>$$q_ {\pi}(s, a) \doteq \mathbb{E}_ {\pi}\left[G_ {t} \mid S_ {t}=s, A_ {t}=a\right]=\mathbb{E}_ {\pi}\left[\sum_ {k=0}^{\infty} \gamma^{k} R_ {t+k+1} \mid S_ {t}=s, A_ {t}=a\right]$$</p>
<ul>
<li>the state-value function for policy $\pi$</li>
</ul>
<p>$$\begin{aligned}
v_ {\pi}(s) & \doteq \mathbb{E}_ {\pi}\left[G_ {t} \mid S_ {t}=s\right] \newline
&=\mathbb{E}_ {\pi}\left[R_ {t+1}+\gamma G_ {t+1} \mid S_ {t}=s\right] \newline
&=\sum_ {a} \pi(a \mid s) \sum_ {s^{\prime}} \sum_ {r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma \mathbb{E}_ {\pi}\left[G_ {t+1} \mid S_ {t+1}=s^{\prime}\right]\right] \newline
&=\sum_ {a} \pi(a \mid s) \sum_ {s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma v_ {\pi}\left(s^{\prime}\right)\right], \quad \text { for all } s \in \mathcal{S},
\end{aligned}$$</p>
<p>Monte Carlo method: 採樣的次數越多 (趨近於無限)，價值估計就會越準 (收斂)。</p>
<p>一個使用 RL 和 動態規劃的價值函數性質：符合遞迴關係式。</p>
<p>Bellman equation:</p>
<p>$$\begin{aligned}
v_ {\pi}(s) & \doteq \mathbb{E}_ {\pi}\left[G_ {t} \mid S_ {t}=s\right] \newline
&=\mathbb{E}_ {\pi}\left[R_ {t+1}+\gamma G_ {t+1} \mid S_ {t}=s\right] \newline
&=\sum_ {a} \pi(a \mid s) \sum_ {s^{\prime}} \sum_ {r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma \mathbb{E}_ {\pi}\left[G_ {t+1} \mid S_ {t+1}=s^{\prime}\right]\right] \newline
&=\sum_ {a} \pi(a \mid s) \sum_ {s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma v_ {\pi}\left(s^{\prime}\right)\right], \quad \text { for all } s \in \mathcal{S},
\end{aligned}$$</p>
<p>$s^\prime$: 下個狀態</p>
<p>![](backup_ diagram.png)</p>
<ul>
<li>空心圓代表一個 state</li>
<li>實心圓代表一個 state-action pair</li>
</ul>
<div class=gdoc-page__anchorwrap>
<h2 id=optimal-policies-and-optimal-value-functions>
Optimal Policies and Optimal Value Functions
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/finite-markov-decision-processes/#optimal-policies-and-optimal-value-functions class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Optimal Policies and Optimal Value Functions" href=#optimal-policies-and-optimal-value-functions><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>Optimal policy: $\pi \ge \pi^\prime$ if and only if $v_ \pi (s) \ge v_ {\pi^\prime} (s)$ for all $s \in \mathcal{S}$</p>
<p>如果超過一個：就把所有的 optimal policies 定義為 $\pi_ {* }$</p>
<p>Optimal state-value function:</p>
<p>$$\begin{array}{l}
v_ {* }(s) \doteq \max_ {\pi}{v_ {\pi}(s)} \newline
\text { for all } s \in \mathcal{S} .
\end{array}$$</p>
<p>Optimal action-value function</p>
<p>$$q_ {* }(s, a) \doteq \max_ {\pi} {q_ {\pi}(s, a)},$$</p>
<p>可以用 $v_ {* }$ 重寫如下：</p>
<p>$$q_ {* }(s, a)=\mathbb{E}\left[R_ {t+1}+\gamma v_ {* }\left(S_ {t+1}\right) \mid S_ {t}=s, A_ {t}=a\right]$$</p>
<div class=gdoc-page__anchorwrap>
<h2 id=optimality-and-approximation>
Optimality and Approximation
<a data-clipboard-text=https://aquastripe.github.io/RL-notes/sutton/tabular-solution-methods/finite-markov-decision-processes/#optimality-and-approximation class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Optimality and Approximation" href=#optimality-and-approximation><svg class="icon gdoc_link"><use xlink:href="#gdoc_link"/></svg>
</a>
</h2>
</div><p>我們定義了 optimal value functions 和 optimal policies。不過實務上難以使用，因為計算量太大了。即使我們有個精準估計環境的模型，通常也不可能透過解出 Bellman optimality equation 計算一個 optimal policy。</p>
<p>記憶體大小是重要的條件。對於狀態空間小的，可以求出近似解，這種案例稱為 tabular case。實務上，多數的問題涉及更大的狀態，這類的必須要用某種參數化的函數 (parameterized function representation)。</p>
</article>
<div class="gdoc-page__footer flex flex-wrap justify-between">
<span class=gdoc-page__nav>
<a class="gdoc-page__nav--prev flex align-center" href=/RL-notes/sutton/tabular-solution-methods/multi-armed-bandits/ title="Ch 2. Multi-Armed Bandits"> Ch 2. Multi-Armed Bandits</a>
</span>
<span class=gdoc-page__nav>
<a class="gdoc-page__nav--next flex align-center" href=/RL-notes/sutton/tabular-solution-methods/dynamic-programming/ title="Ch 4. Dynamic Programming">Ch 4. Dynamic Programming </a>
</span>
</div>
</div>
</main>
<footer class=gdoc-footer>
<div class="container flex">
<div class="flex flex-wrap">
<span class="gdoc-footer__item gdoc-footer__item--row">
Built with <a href=https://gohugo.io/ class=gdoc-footer__link>Hugo</a> and<svg class="icon gdoc_heart"><use xlink:href="#gdoc_heart"/></svg>
</span>
</div>
<div class="flex flex-25 justify-end">
<span class=gdoc-footer__item>
<a class="gdoc-footer__link fake-link" href=# aria-label="Back to top"><svg class="icon gdoc_keyborad_arrow_up"><use xlink:href="#gdoc_keyborad_arrow_up"/></svg> <span class=hidden-mobile>Back to top</span>
</a>
</span>
</div>
</div>
</footer>
</div>
<script defer src=/RL-notes/js/zh.search.min.62011e6f91033499983c850099ab3e1c2c53d1691525ece5783685e0ddd1c310.js></script>
<script defer src=/RL-notes/js/clipboard-27784b7376.min.js></script>
<script defer src=/RL-notes/js/clipboard-loader-f0b5fbd5f6.min.js></script>
</body>
</html>