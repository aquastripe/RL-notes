[{"id":0,"href":"/RL-notes/reinforcement-learning-an-introduction/","title":"Reinforcement Learning: An Introduction","parent":"Reinforcement Learning Notes","content":""},{"id":1,"href":"/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/","title":"Tabular Solution Methods","parent":"Reinforcement Learning: An Introduction","content":"  表格式解法 (Tabular Solution Method) 為 RL 最簡單的形式：當所有的狀態和動作數量少到可以用多維陣列來表示價值函數。這些方法通常可以找到精確解 (exact solution)，也就是精確的 價值函數 和 策略 的極值。\n第一章是 RL 的特殊形式：只有一個狀態，稱為 吃角子老虎機問題 (bandit problems)。\n第二章是更通用的問題形式：有限馬可夫決策過程，主要概念包含 貝爾曼方程 (Bellman equations；又稱為 動態規劃方程) 和 價值函數。\n第三、四、五章描述三個基本類型的方法來解決 有限馬可夫決策 問題：動態規劃 (dynamic programming)、蒙特卡羅法 (Monte Carlo methods)、時序差分學習法 (temporal difference learning)。每個方法各有優劣：\n 動態規劃：需要完整且精確的環境模型 蒙特卡羅法：不需要完整且精確的環境模型，但是不適合逐步的增量計算。  增量計算 (incremental computation): 是一個軟體功能，當只有一小塊資料改變的時候，只會對產生變化的部分進行計算和更新，以節省計算時間。(is a software feature which, whenever a piece of data changes, attempts to save time by only recomputing those outputs which depend on the changed data.)   時序差分學習法：不需要完整的環境模型，也完全支援增量計算，但是更複雜而難以分析。  第六、七章描述這三種方法如何結合各自的優點。第六章描述如何用 多步拔靴法 (自助法；自助抽樣法；multi-step bootstrapping methods) 結合 蒙特卡羅法 和 時間差分學習法。第七章描述如何以 時序差分學習法 結合 模型學習 (model learning) 和 規劃法 (例如動態規劃) 來解決通用的表格式 RL 問題。\n"},{"id":2,"href":"/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/","title":"Multi-Armed Bandits","parent":"Tabular Solution Methods","content":"    A k-armed Bandit Problem Action-value Methods  Sample-average method Greedy $\\varepsilon$-greedy   The 10-armed Testbed     A k-armed Bandit Problem    k 臂吃角子老虎機問題：你需要從 k 個不同 選項 或是 動作 中做出選擇，每個選擇都會帶來獎勵，獎勵多寡來自固定的機率分佈。你的目標是要在經過一段時間後得到最大的累積獎勵值。\n在這個問題中，每個動作會有期望或平均的獎勵值，讓我們稱之為該動作的 價值。我們把在時間點 $t$ 選擇的動作表示為 $A_t$，對應的獎勵表示為 $R_t$。那麼：任意動作 $a$ 的價值 $q_{*}(a)$ 是選擇該動作的期望值：\n$$q_{*}(a) \\doteq \\mathbb{E}[R_t | A_t = a].$$\n我們把 在時間點 $t$ 選擇動作 $a$ 的估計價值表示為 $Q_t(a)$，我們想要此估計值要接近 $q_{*}(a)$。\nAction-value Methods    估計動作的價值，作為根據來做決策，這類的方法通稱為 action-value methods。\nSample-average method    一個自然的方式是計算平均得到多少的獎勵來當作估計值：\n$$Q_t(a) \\doteq \\dfrac{\\text{sum of rewards when } a \\text{ taken prior to }t}{\\text{number of times } a \\text{ taken prior to }t} = \\dfrac{\\sum_{i=1}^{t-1}{R_i \\cdot \\mathbb{1}_{A_i=a}}}{\\sum_{i=1}^{t-1}{\\mathbb{1}_{A_i=a}}}$$\n其中 $\\mathbb{1}_{predicate}$ 表示 $predicate$ 為真時設定為 $1$，否則為 $0$。\n 當分母趨近於 $0$ 的時候，把 $Q_{t}(a)$ 設定為預設值 (例如 $0$)。 當分母趨近於無限大時，根據大數法則，$Q_t(a)$ 會收斂於 $q_*(a)$。  Greedy    最簡單的方法是選擇 最大的估計價值，如下：\n$$A_t \\doteq \\arg \\max_{a} Q_t (a)$$\n這個方法完全的利用已知的資訊，不採樣任何較差的動作來進行任何探索。\n$\\varepsilon$-greedy    一種替代方案是大部分時間都用貪心法，以機率為 $\\varepsilon$ 採樣其他的動作來進行少量的探索。\nThe 10-armed Testbed    Figure 2.1 為 $k=10$ 的實例，每個都是 $\\mathcal{N}(0,1)$ 的常態分佈。\n$\\varepsilon$-greedy 比 greedy 還要看情況決定\n 如果獎勵的變異數是 10 而非 1，那麼要花費更多的探索才會找到最佳解。 如果獎勵的變異數是 0，那麼 greedy 可能會找到最佳解而不需要再探索。 如果吃角子老虎機是確定性 (deterministic) 但不穩定的 (nonstationarity)，例如：會隨著時間改變，那麼 $\\varepsilon$-greedy 還是比較好。  "},{"id":3,"href":"/RL-notes/Reinforcement-Learning-An-Introduction/introduction/","title":"Introduction","parent":"Reinforcement Learning: An Introduction","content":"    Reinforcement Learning (增強式學習) Elements of Reinforcement Learning  Policy (策略) Reward signal (獎勵訊號) Value function (價值函數) Model (模型)   Limitations and Scope An Extended Example: Tic-Tac-Toe (井字遊戲)  Minimax 動態規劃 演化式方法 使用 RL 並配合價值函數       Reinforcement Learning (增強式學習)    RL 的目標：學習怎麼在給定狀態下，輸出可以得到最大的獎勵 (reward) 的動作 (action)。\nRL 最特別的性質：\n trial-and-error search delayed reward  Reinforcement learning 和其他 -ing 結尾的主題類似 (e.g. machine learning)，同時是個問題也是解決問題的方法。區分問題和解法在 RL 非常重要，搞不清楚時常會造成困惑。\nRL 借用動態系統理論 (dynamical systems theroy) 的馬可夫決策過程 (Markov decision processes) 的觀念來形式化。基本觀念：一個 學習代理 (learning agent) 會隨著 時間 與 環境 (environment) 互動來達成一個 目標 (goal)。學習代理必須可以：\n 感測環境的 狀態 (state) 採取 動作 (action) 來影響環境的狀態  馬可夫決策過程以三個觀點提供最簡化的形式涵蓋這個問題：\n sensation action goal  機器學習三大類：\n supervised learning (監督式學習) unsupervised learning (非監督式學習) 增強式學習  不同之處：\n RL 與 supervised learning 的不同：（略） RL 與 unsupervised learning 的不同：（略）  RL 必須在 exploration (探索) 和 exploitation (利用) 之間取捨：\n 利用：為了要獲得大量的獎勵，RL 代理必須偏好選擇「過去嘗試過最好的」動作 探索：為了做到這件事情，必須探索沒有嘗試過的動作  Elements of Reinforcement Learning    RL 的基本組成：\n the agent (施事，主動進行動作並影響環境狀態的主體) the environment (環境) RL 系統的子元素:  a policy (策略) a reward signal (獎勵訊號) a value function (價值函數) (optional) a model of the environment (環境的模型)    Policy (策略)    策略定義了施事的行為。在給定一個時間點，施事從環境接收狀態，並選擇動作來改變環境。而策略是狀態到動作的映射函數。\n 可能是簡單的函數或是查表，也可能是會涉及複雜計算的演算法 可能是隨機性的，提供採取每個動作的機率  Reward signal (獎勵訊號)    獎勵訊號定義了 RL 問題的目標。在每個時間點，環境會傳送單一數值的訊號給 RL 施事，稱為 獎勵。獎勵給學習代理定義事件的好壞。\n 可能是一個隨機性的函數，根據環境的狀態和採取的動作  Value function (價值函數)    一個狀態的 價值 是施事預期未來會在這個狀態下取得多少獎勵的總和。\n 以最大的價值而非最大的獎勵來採取動作。 獎勵是立即的回饋，價值函數是長期的 獎勵是主要的，價值函數是次要的。  沒有獎勵就沒有價值，估計價值的唯一目的是獲得更多的獎勵。   決定價值比獎勵更難  獎勵通常可以直接由環境取得 價值必須估計、以及來自一個生命週期觀測的結果來重新估計    Model (模型)    模型 用來模擬環境的行為。例如：給定狀態和動作，模型要預測下個狀態和下個獎勵\n模型是用來 planning (規劃)\n model-based model-free  trial-and-error    Limitations and Scope     極度依賴狀態：作為輸入給策略、價值函數和模型，以及來自模型的輸出。  本書探討的問題是假設狀態已經被良好的處理過，可以直接使用。 為了專注在討論決策問題，不考慮如何設計狀態的訊號的問題。   大部分 RL 的方法都圍繞在如何估計價值函數，但這非 RL 的必要條件。不涉及估計價值函數的方法的例子：  基因演算法 (genetic algorithms) 基因規劃 (genetic programming) 模擬退火法 (simulated annealing) 以上是演化式 (evolutionary) 方法，在生命週期中不學習，而是在下個世代產生具備能力的個體。 什麼情況下演化式方法會有優勢：  如果策略空間很小，或是容易被找到、有足夠多的時間搜尋 當無法從環境中感測出完整的狀態     本書專注在環境互動中學習的方法，不包含演化式方法。  An Extended Example: Tic-Tac-Toe (井字遊戲)    井字遊戲是個簡單的問題，但沒有辦法由經典的演算法來適當的解決。以下舉幾個例子來說明：\n 使用 minimax 使用動態規劃 使用演化式方法 使用 RL 並配合價值函數  Minimax     這個方法假設了對手的遊戲策略 讓玩家無法到達一個「可能會輸掉、但實際上對手可能會失誤而因此勝利」的狀態。  動態規劃     需要關於對手的完整規格，包含在每個狀態下會以何種機率採取行動。通常這種資訊是不會先驗的 (a prior) 獲得，大部分實務也不會有。 有一種方式是學習模型來模擬對手的行為，再根據模型來計算動態規劃求出最佳解。最後，這個方法和某些 RL 的方法並無不同。  演化式方法     直接搜尋所有可能的策略，找出一個有高機率獲勝的方法。 此處的策略是一個規則來告訴玩家要下哪一步，對遊戲所有可能狀態 — 每個可能的 O 或 X 的設定 (configuration) 對每一個策略，藉由大量的對戰來估計勝率。 藉由估計值來決定下一次的策略。 可能會用的經典演化式演算法：  爬坡演算法 (hill-climbing algorithm)，會連續的生成和估計策略 基因演算法類    使用 RL 並配合價值函數    步驟如下：\n 設定數值表 (table of numbers)，每一個代表這場遊戲中可能的狀態，每一個數值是當前狀態下勝率的最新估計值。 把這個估計值當作這個狀態的價值，整個表就是學習到的價值函數。 對於所有三個連一直線的狀態的勝率是 1 (或是 0，被對手成功連線時) 。 所有其他狀態的勝率值都初始化為 0.5。 接下來開始對戰很多次。 選擇要下的點，大部分情況下可以使用貪心法挑選最大的價值，也就是最高的勝率。 少部份的情況下選擇其他沒有下過的落點，這稱之為 探索，讓我們可以看過沒見過的狀態。 在遊戲過程中會更新價值函數，使估計的更準確。方式是在每一步過後回補 (back up) 前一步的狀態的價值。 假設當前狀態是 $S_{t}$，經過一個貪心選擇過後下一步是 $S_{t+1}$，這時會更新 $S_{t}$ 的價值函數，標記為 $V(S_{t})$。可以被列式如下：  $V(S_{t}) \\leftarrow V(S_{t}) + \\alpha [ V(S_{t+1}) - V(S_{t})]$  $\\alpha$: 步長參數 (step-size parameter) 這是一個 temporal-difference 學習法的例子         每個實心點代表狀態，實線是狀態轉移的路徑，虛線是可以轉移的路徑 (輪到自己時有多個動作可以選擇) ，* 代表最大價值的狀態。 (Richard S. Sutton and Andrew G. Barto on Reinforcement Learning: An Introduction)   這個方法在這個任務上可以做得很好，因為：\n 步長參數在經過一段時間後適當的減少，會收斂到在給定每個狀態下真實的勝率值。 每一步都是根據對手的落子下的最佳解。  這個例子突顯了 RL 的關鍵特色：\n 強調在與環境互動中學習 目標明確 不只根據當前的狀態，也會考慮後面的發展  "},{"id":4,"href":"/RL-notes/","title":"Reinforcement Learning Notes","parent":"","content":"學習資源     Reinforcement Learning: An Introduction (Richard S. Sutton and Andrew G. Barto)  http://incompleteideas.net/book/the-book.html   OpenAI  https://openai.com/    "},{"id":5,"href":"/RL-notes/categories/","title":"Categories","parent":"Reinforcement Learning Notes","content":""},{"id":6,"href":"/RL-notes/tags/","title":"Tags","parent":"Reinforcement Learning Notes","content":""}]