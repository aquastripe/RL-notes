[{"id":0,"href":"/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/finite-markov-decision-processes/","title":"Ch 3. Finite Markov Decision Processes","parent":"Part I: Tabular Solution Methods","content":"    The Agent-Environment Interface Goals and Rewards Returns and Episodes  Episodic tasks Continuing tasks Discounting   Unified Notation for Episodic and Continuing Tasks Policies and Value Functions Optimal Policies and Optimal Value Functions Optimality and Approximation     這章談論的問題是 associative aspect: 在不同狀況選擇不同的動作。\nMDPs 是一個經典的連續決策的形式，動作會影響當下的獎勵、子序列的狀態，還有未來的獎勵。\n權衡:\n immediate reward (當下的獎勵) delayed reward (延遲的獎勵)  差異:\n bandit problem: 估計價值 $q_{*}(a)$ MDPs: 估計  狀態動作價值 $q_{*}(s, a)$ 狀態價值 $v_{*}(s)$ (given optimal action selections)    關鍵元素:\n mathematical structure returns value functions Bellman equations  The Agent-Environment Interface    $$S_0, A_0, R_1, S_1, A_1, R_2, \\dots$$\n在有限的 MDP, 隨機變數 $R_t$ 和 $S_t$ 有隨機機率分佈只依賴於先前的動作和狀態。\n$$p( s^{\\prime}, r \\mid s, a) \\doteq \\operatorname{Pr} \\lbrace S_{t}=s^{\\prime}, R_{t}=r \\mid S_{t-1}=s, A_{t-1}=a \\rbrace$$\nfor all $s', s \\in \\mathcal{S}, r \\in \\mathcal{R}$, and $a \\in \\mathcal{A}(s)$.\n函數 $p: \\mathcal{S} \\times \\mathcal{R} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$ 定義了 MDP 的 dynamics.\n條件機率的符號提醒我們：$p$ 對每個選擇 $s$ 和 $a$ 指定一個機率分佈，也就是:\n$$\\sum_{s^{\\prime} \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p (s^{\\prime}, r \\mid s, a ) = 1, \\text { for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)$$\n$S_t$ 和 $A_t$ 的機率只依賴於先前的狀態和動作, $S_{t-1}$ and $A_{t-1}$.\n 這是一個馬可夫性質 (Markov property)。  對於環境，我們可以計算狀態轉移機率 $p: \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$\n$$p\\left(s^{\\prime} \\mid s, a\\right) \\doteq \\operatorname{Pr}\\lbrace S_{t}=s^{\\prime} \\mid S_{t-1}=s, A_{t-1}=a\\rbrace=\\sum_{r \\in \\mathcal{R}} p\\left(s^{\\prime}, r \\mid s, a\\right)$$\n計算 狀態-動作 (state-action pairs) 的期望獎勵 (expected rewards) $r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$\n$$r(s, a) \\doteq \\mathbb{E}\\left[R_{t} \\mid S_{t-1}=s, A_{t-1}=a\\right]=\\sum_{r \\in \\mathcal{R}} r \\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime}, r \\mid s, a\\right)$$\n計算 狀態-動作-下個狀態 (state-action-next-state pairs) 的期望獎勵 $r: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}$\n$$r\\left(s, a, s^{\\prime}\\right) \\doteq \\mathbb{E}\\left[R_{t} \\mid S_{t-1}=s, A_{t-1}=a, S_{t}=s^{\\prime}\\right]=\\sum_{r \\in \\mathcal{R}} r \\frac{p\\left(s^{\\prime}, r \\mid s, a\\right)}{p\\left(s^{\\prime} \\mid s, a\\right)}$$\n通用的規則是：任何事情無法被智慧主體任意改變的，都被視為它的外部，也就成為環境的一部分。智慧主體和環境的邊界在於智慧主體的可以絕對控制的範圍。\nGoals and Rewards    Reward hypothesis:\n可以把目標視為最大化累積獎勵的期望值。\nThe reward signal is your way of communicating to the agent what you want to achieve, not how you want it to achieve.\nReturns and Episodes    We have said that the agent’s goal is to maximize the cumulative reward it receives in the long run.\n$$G_{t} \\doteq R_{t+1}+R_{t+2}+R_{t+3}+\\cdots+R_{T}$$\n Return: 獎勵序列的某個特定函數 Episodes: 當智慧主體和環境的互動自然的分成子序列的時刻 Terminal state: 每個 episode 在特別的狀態下結束，之後會重整為起始狀態。  Episodic tasks    有 episode 的任務稱作 episodic tasks。\n non-terminal states: $\\mathcal{S}$ terminal state: $\\mathcal{S}^+$ 終止時間 $T$ 是一個隨機變數，會隨著 episode 變化。  Continuing tasks    $T=\\infty$\nThus, in this book we usually use a definition of return that is slightly more complex conceptually but much simpler mathematically\nDiscounting    $$G_{t} \\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\cdots=\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}$$\n $\\gamma$: a parameter $0 \\le \\gamma \\le 1$ $\\gamma \\lt 1$: $G_t$ is a finite value as long as the reward sequence ${R_k}$ is bounded.  $R_k$ 數值有上限   $\\gamma = 0$: the agent is myopic $\\gamma \\approx 1$: the agent is farsighted  $$\\begin{aligned} G_{t} \u0026amp; \\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\gamma^{3} R_{t+4}+\\cdots \\newline \u0026amp;=R_{t+1}+\\gamma\\left(R_{t+2}+\\gamma R_{t+3}+\\gamma^{2} R_{t+4}+\\cdots\\right) \\newline \u0026amp;=R_{t+1}+\\gamma G_{t+1} \\end{aligned}$$\n即使是無限項，reward 還是有限的\ne.g. if the reward is constant $+1$\n$$G_{t}=\\sum_{k=0}^{\\infty} \\gamma^{k}=\\frac{1}{1-\\gamma}$$\nUnified Notation for Episodic and Continuing Tasks    可以藉由把 episode termination 是否進入 absorbing state 來統一兩種 tasks\n吸收狀態 (absorbing state): 狀態只會轉移到自己，並且產生 0 reward\n重新定義 return 的計算，省略了 episode 次數：\n$$G_{t} \\doteq \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_{k}$$\nPolicies and Value Functions    Policy: 映射 states 到選擇某個動作的機率\n $\\pi (a | s)$  Value functions: 給定 states 或 state-action pairs，估計 agent 的 states 有多好。\n $v_{\\pi} (s)$  $$v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s\\right]=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\mid S_{t}=s\\right], \\text { for all } s \\in \\mathcal{S}$$\n the state-action-value function for policy $\\pi$  $$q_{\\pi}(s, a) \\doteq \\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s, A_{t}=a\\right]=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\mid S_{t}=s, A_{t}=a\\right]$$\n the state-value function for policy $\\pi$  $$\\begin{aligned} v_{\\pi}(s) \u0026amp; \\doteq \\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s\\right] \\newline \u0026amp;=\\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s\\right] \\newline \u0026amp;=\\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}} \\sum_{r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma \\mathbb{E}_{\\pi}\\left[G_{t+1} \\mid S_{t+1}=s^{\\prime}\\right]\\right] \\newline \u0026amp;=\\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right], \\quad \\text { for all } s \\in \\mathcal{S}, \\end{aligned}$$\nMonte Carlo method: 採樣的次數越多 (趨近於無限)，價值估計就會越準 (收斂)。\n一個使用 RL 和 動態規劃的價值函數性質：符合遞迴關係式。\nBellman equation:\n$$\\begin{aligned} v_{\\pi}(s) \u0026amp; \\doteq \\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s\\right] \\newline \u0026amp;=\\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s\\right] \\newline \u0026amp;=\\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}} \\sum_{r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma \\mathbb{E}_{\\pi}\\left[G_{t+1} \\mid S_{t+1}=s^{\\prime}\\right]\\right] \\newline \u0026amp;=\\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right], \\quad \\text { for all } s \\in \\mathcal{S}, \\end{aligned}$$\n$s^\\prime$: 下個狀態\n 空心圓代表一個 state 實心圓代表一個 state-action pair  Optimal Policies and Optimal Value Functions    Optimal policy: $\\pi \\ge \\pi^\\prime$ if and only if $v_\\pi (s) \\ge v_{\\pi^\\prime} (s)$ for all $s \\in \\mathcal{S}$\n如果超過一個：就把所有的 optimal policies 定義為 $\\pi_{*}$\nOptimal state-value function:\n$$\\begin{array}{l} v_{*}(s) \\doteq \\max_{\\pi}{v_{\\pi}(s)} \\newline \\text { for all } s \\in \\mathcal{S} . \\end{array}$$\nOptimal action-value function\n$$q_{*}(s, a) \\doteq \\max_{\\pi} {q_{\\pi}(s, a)},$$\n可以用 $v_{*}$ 重寫如下：\n$$q_{*}(s, a)=\\mathbb{E}\\left[R_{t+1}+\\gamma v_{*}\\left(S_{t+1}\\right) \\mid S_{t}=s, A_{t}=a\\right]$$\nOptimality and Approximation    我們定義了 optimal value functions 和 optimal policies。不過實務上難以使用，因為計算量太大了。即使我們有個精準估計環境的模型，通常也不可能透過解出 Bellman optimality equation 計算一個 optimal policy。\n記憶體大小是重要的條件。對於狀態空間小的，可以求出近似解，這種案例稱為 tabular case。實務上，多數的問題涉及更大的狀態，這類的必須要用某種參數化的函數 (parameterized function representation)。\n"},{"id":1,"href":"/RL-notes/reinforcement-learning-an-introduction/","title":"Reinforcement Learning: An Introduction","parent":"Reinforcement Learning Notes","content":""},{"id":2,"href":"/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/","title":"Part I: Tabular Solution Methods","parent":"Reinforcement Learning: An Introduction","content":"  表格式解法 (Tabular Solution Method) 為 RL 最簡單的形式：當所有的狀態和動作數量少到可以用多維陣列來表示價值函數。這些方法通常可以找到精確解 (exact solution)，也就是精確的 價值函數 和 策略 的極值。\n第一章是 RL 的特殊形式：只有一個狀態，稱為 吃角子老虎機問題 (bandit problems)。\n第二章是更通用的問題形式：有限馬可夫決策過程，主要概念包含 貝爾曼方程 (Bellman equations；又稱為 動態規劃方程) 和 價值函數。\n第三、四、五章描述三個基本類型的方法來解決 有限馬可夫決策 問題：動態規劃 (dynamic programming)、蒙地卡羅法 (Monte Carlo methods)、時序差分學習法 (temporal difference learning)。每個方法各有優劣：\n 動態規劃：需要完整且精確的環境模型 蒙地卡羅法：不需要完整且精確的環境模型，但是不適合逐步的增量計算。  增量計算 (incremental computation): 是一個軟體功能，當只有一小塊資料改變的時候，只會對產生變化的部分進行計算和更新，以節省計算時間。(is a software feature which, whenever a piece of data changes, attempts to save time by only recomputing those outputs which depend on the changed data.)   時序差分學習法：不需要完整的環境模型，也完全支援增量計算，但是更複雜而難以分析。  第六、七章描述這三種方法如何結合各自的優點。第六章描述如何用 多步拔靴法 (自助法；自助抽樣法；multi-step bootstrapping methods) 結合 蒙地卡羅法 和 時間差分學習法。第七章描述如何以 時序差分學習法 結合 模型學習 (model learning) 和 規劃法 (例如動態規劃) 來解決通用的表格式 RL 問題。\n Multi-Armed Bandits  "},{"id":3,"href":"/RL-notes/Reinforcement-Learning-An-Introduction/tabular-solution-methods/multi-armed-bandits/","title":"Ch 2. Multi-Armed Bandits","parent":"Part I: Tabular Solution Methods","content":"    A k-armed Bandit Problem Action-value Methods  Sample-average method Greedy $\\varepsilon$-greedy   The 10-armed Testbed Incremental Implementation Tracking a Nonstationary Problem  可變長度的步長 (step-size)   Optimistic Initial Value Upper-Confidence-Bound Action Selection Gradient Bandit Algorithms Associative Search (Contextual Bandits) Summary     A k-armed Bandit Problem    k 臂吃角子老虎機問題：你需要從 k 個不同 選項 或是 動作 中做出選擇，每個選擇都會帶來獎勵，獎勵多寡來自固定的機率分佈。你的目標是要在經過一段時間後得到最大的累積獎勵值。\n在這個問題中，每個動作會有期望或平均的獎勵值，讓我們稱之為該動作的 價值。我們把在時間點 $t$ 選擇的動作表示為 $A_t$，對應的獎勵表示為 $R_t$。那麼：任意動作 $a$ 的價值 $q_{*}(a)$ 是選擇該動作的期望值：\n$$q_{*}(a) \\doteq \\mathbb{E}[R_t | A_t = a].$$\n我們把 在時間點 $t$ 選擇動作 $a$ 的估計價值表示為 $Q_t(a)$，我們想要此估計值要接近 $q_{*}(a)$。\nAction-value Methods    估計動作的價值，作為根據來做決策，這類的方法通稱為 action-value methods。\nSample-average method    一個自然的方式是計算平均得到多少的獎勵來當作估計值：\n$$Q_t(a) \\doteq \\dfrac{\\text{sum of rewards when } a \\text{ taken prior to }t}{\\text{number of times } a \\text{ taken prior to }t} = \\dfrac{\\sum_{i=1}^{t-1}{R_i \\cdot \\mathbb{1}_{A_i=a}}}{\\sum_{i=1}^{t-1}{\\mathbb{1}_{A_i=a}}}$$\n其中 $\\mathbb{1}_{predicate}$ 表示 $predicate$ 為真時設定為 $1$，否則為 $0$。\n 當分母趨近於 $0$ 的時候，把 $Q_{t}(a)$ 設定為預設值 (例如 $0$)。 當分母趨近於無限大時，根據大數法則，$Q_t(a)$ 會收斂於 $q_*(a)$。  Greedy    最簡單的方法是選擇 最大的估計價值，如下：\n$$A_t \\doteq \\arg \\max_{a} Q_t (a)$$\n這個方法完全的利用已知的資訊，不採樣任何較差的動作來進行任何探索。\n$\\varepsilon$-greedy    一種替代方案是大部分時間都用貪心法，以機率為 $\\varepsilon$ 採樣其他的動作來進行少量的探索。\nThe 10-armed Testbed    Figure 2.1 為 $k=10$ 的實例，每個都是 $\\mathcal{N}(0,1)$ 的常態分佈。\n$\\varepsilon$-greedy 比 greedy 還要看情況決定\n 如果獎勵的變異數是 10 而非 1，那麼要花費更多的探索才會找到最佳解。 如果獎勵的變異數是 0，那麼 greedy 可能會找到最佳解而不需要再探索。 如果吃角子老虎機是確定性 (deterministic) 但不固定的 (nonstationarity)，例如：會隨著時間改變，那麼 $\\varepsilon$-greedy 還是比較好。  Incremental Implementation    以上都是以 平均獎勵值 來估計 價值。以下討論如何以更有效率的方式計算。\n設 $R_{i}$ 為當前動作中第 $i$ 個選擇接收的獎勵值，$Q_n$ 為這個動作第 $n-1$ 次選擇後的估計價值，可以簡寫如下：\n$$Q_n \\doteq \\dfrac{R_1 + R_2 + \\ldots + R_{n-1}}{n-1}$$\n計算複雜度會隨著次數越來越多次以後變得更多。可以改寫為遞迴式：\n$$ \\begin{aligned} Q_{n+1} \u0026amp;= \\frac{1}{n} \\sum_{i=1}^{n} R_{i}\\newline \u0026amp;= \\frac{1}{n}\\left(R_{n}+\\sum_{i=1}^{n-1} R_{i}\\right)\\newline \u0026amp;=\\frac{1}{n}\\left(R_{n}+(n-1) \\frac{1}{n-1} \\sum_{i=1}^{n-1} R_{i}\\right)\\newline \u0026amp;=\\frac{1}{n}\\left(R_{n}+(n-1) Q_{n}\\right)\\newline \u0026amp;=\\frac{1}{n}\\left(R_{n}+n Q_{n}-Q_{n}\\right)\\newline \u0026amp;=Q_{n}+\\frac{1}{n}\\left[R_{n}-Q_{n}\\right],\\newline \\end{aligned} $$\n通用形式如下：\n$$\\text{NewEstimate} \\leftarrow \\text{OldEstimate} + \\text{StepSize}[\\text{Target} - \\text{OldEstimate}]$$\n其中 $\\text{Target} - \\text{OldEstimate}$ 為 誤差，$\\text{StepSize}$ 為 步長，通常以符號 $\\alpha$ 或 $\\alpha_{t}(a)$ 表示。\n增量法的完整演算法如下：\nTracking a Nonstationary Problem    以上方法適用於固定的 (stationary) 吃角子老虎機問題，也就是獲得的 獎勵值 不會隨著時間改變。為了計算不固定 (nonstationary) 的問題，通常會為每一步給個 權重值，可以經由 $\\text{StepSize}$ $\\alpha$ 給定。\n例如：原增量法的計算式如下：\n$$Q_{n+1} \\doteq Q_{n}+\\alpha\\left[R_{n}-Q_{n}\\right]$$\n其中 $\\alpha \\in (0, 1]$ 為常數。那麼展開後的 價值 為過去 獎勵值 的 加權平均 (weighted average)，結果計算如下 (2.6)：\n$$\\begin{aligned} Q_{n+1} \u0026amp;=Q_{n}+\\alpha\\left[R_{n}-Q_{n}\\right] \\newline \u0026amp;=\\alpha R_{n}+(1-\\alpha) Q_{n} \\newline \u0026amp;=\\alpha R_{n}+(1-\\alpha)\\left[\\alpha R_{n-1}+(1-\\alpha) Q_{n-1}\\right] \\newline \u0026amp;=\\alpha R_{n}+(1-\\alpha) \\alpha R_{n-1}+(1-\\alpha)^{2} Q_{n-1} \\newline \u0026amp;=\\alpha R_{n}+(1-\\alpha) \\alpha R_{n-1}+(1-\\alpha)^{2} \\alpha R_{n-2}+\\newline \u0026amp; \\quad \\cdots+(1-\\alpha)^{n-1} \\alpha R_{1}+(1-\\alpha)^{n} Q_{1} \\newline \u0026amp;=(1-\\alpha)^{n} Q_{1}+\\sum_{i=1}^{n} \\alpha(1-\\alpha)^{n-i} R_{i} \\end{aligned}$$\n稱為加權平均是因為 $(1-\\alpha)^{n}+\\sum_{i=1}^{n} \\alpha(1-\\alpha)^{n-i}=1$。\n $(1 - \\alpha) \u0026lt; 1$: 所以給予過去探索的獎勵值 $R_i$ 權重會越來越小 當 $(1 - \\alpha) \u0026lt; 0$: 稱為 exponential recency-weighted average 當 $(1 - \\alpha) = 0$: 所有權重都會給最近的獎勵值。  可變長度的步長 (step-size)    設 在第 $n$ 個選擇了動作 $a$ 之後的步長 (step-size) 為 $\\alpha_n (a)$\n $\\alpha_n (a) = \\dfrac{1}{n}$: 採樣平均法 (sample-average method)，根據大數法則 (the law of large numbers) 保證收斂到真實的動作價值。 $\\sum_{n=1}^\\infty \\alpha_n (a) = \\infty \\text{ and } \\sum_{n=1}^\\infty \\alpha_n^2 (a) \u0026lt; \\infty$: 根據隨機近似理論 (stochastic approximation theory) 符合條件保證收斂到機率為 $1$。  第一個條件保證步長總和大到可以克服任意初始條件和隨機波動。 第二個條件保證最後步長足夠小到可以收斂。    問題：\n 雖然保證收斂，但是收斂速度緩慢，或是要經過調整才能得到良好的收斂率。 理論上可行，實務上很少使用。  Optimistic Initial Value    以上的方法都依賴於初始的動作價值 (initial action-value) $Q_1(a)$。在統計學裡，這稱為 偏差 (biased by their initial estimates)。\n 在採樣平均法中，偏差會在所有動作至少採樣過一次以後消失。 在所有 $\\alpha$ 為常數的方法，根據 2.6 式，偏差會隨著時間削減但永久存在。  壞處：初始值成為一種使用者設定的參數，除非把它們設定為 $0$。 好處：以一種簡單的方式提供某種先驗的知識，告訴我們預期可以得到什麼程度的獎勵值。    初始動作價值也可以用來 鼓勵探索。假設不把初始值設定為 $0$，在 10-armed testbed 中把所有都設定成 $+5$，而 $q_*(a)$ 在先前設定為平均為 $0$ 變異數為 $1$ 的常態分佈。\n 這種情況下，初始的估計值都是 $+5$ 是樂觀的 (optimistic)，這種樂觀會鼓勵 動作價值法 去探索。 不論選擇哪個動作，獎勵值都會少於估計值，學習器 (learner) 會對獎勵值失望 (disappointed)。 所有動作在嘗試幾次以後收斂。  下圖為 10-armed testbed 比較兩種設定 ($\\alpha = 0.1$)：\n greedy: $Q_1(a) = +5$ $\\varepsilon$-greedy: $Q_1(a) = 0$  它是一個簡單的技巧，但是不適用於不固定問題 (nonstationary problems)。\n 當任務改變時，需要開啟新的探索，這個方法就會無效。  Upper-Confidence-Bound Action Selection    因為不確定 動作價值 (action-value) 估計值的準確度，所以需要 探索。問題是如何選擇可能最好的動作？以下提供一個有效的方式：\n$$A_{t} \\doteq \\underset{a}{\\arg \\max }\\left[Q_{t}(a)+c \\sqrt{\\frac{\\ln t}{N_{t}(a)}}\\right],$$\n $N_t(a)$ 表示動作 $a$ 隨著時間 $t$ 經過後採樣的次數。 $c \u0026gt; 0$ 用來控制探索的程度。 平方根內的值用來測量動作 $a$ 的不確定性 (uncertainty) 或 變異數 (variance)。  下圖為 10-armed testbed 比較兩種設定：\n UCB: $c = 2$ $\\varepsilon$-greedy: $\\varepsilon = 0.1$  解決更通用的 RL 問題的困難：\n 處理不固定問題 (nonstationary problems) 方法更複雜。 處理更大的狀態空間 (state spaces)。  Gradient Bandit Algorithms    先前的作法是估計動作價值之後根據這些估計值來選擇動作。\n這節的作法是學習一個動作的 偏好 (preference)，表示為 $H_t (a) \\in \\mathbb{R}$。\n 偏好跟獎勵值無關。 偏好是相對的。如果對所有偏好都增加 $1000$，根據 soft-max 將不影響計算結果：  $$ \\operatorname{Pr}\\{A_{t}=a \\} \\doteq \\frac{e^{H_{t}(a)}}{\\sum_{b=1}^{k} e^{H_{t}(b)}} \\doteq \\pi_{t}(a) $$\n使用 stochastic gradient ascent 來更新偏好值：\n$$\\begin{aligned} H_{t+1}\\left(A_{t}\\right) \u0026amp; \\doteq H_{t}\\left(A_{t}\\right)+\\alpha\\left(R_{t}-\\bar{R}_{t}\\right)\\left(1-\\pi_{t}\\left(A_{t}\\right)\\right), \u0026amp; \u0026amp; \\text { and } \\newline H_{t+1}(a) \u0026amp; \\doteq H_{t}(a)-\\alpha\\left(R_{t}-\\bar{R}_{t}\\right) \\pi_{t}(a), \u0026amp; \u0026amp; \\text { for all } a \\neq A_{t}, \\end{aligned}$$\n $\\alpha \u0026gt; 0$: 步長 (step-size) 參數。 $\\bar{R}_t \\in \\mathbb{R}$: 平均但不包含時間點 $t$ 的獎勵值，通常當作比較基準 (baseline)。  如果獎勵比基準大，未來選擇動作 $A_t$ 的機率會增加；否則就降低。   原本應該要計算的梯度: $\\dfrac{\\partial \\pi_t(x)}{\\partial H_t(a)} = \\pi_t(x)(\\mathbb{1}_{a=x} - \\pi_t(a))$ (證明略)  下圖為 10-armed testbed $\\mathcal{N}(4,1)$ 比較兩種設定：\n 包含 baseline 不含 baseline ($\\bar{R}_t = 0$)  Associative Search (Contextual Bandits)    目前為止考慮的都是非結合式的任務 (nonassociative tasks)，不需要在不同情況下結合不同的動作。更通用的 RL 任務是學習一個策略 (policy)：從一個狀態映射到一個最好的動作選項 (a mapping from situations to the actions that are best in those situations)。\n舉例：\n 假設有幾個不同的 k-armed 吃角子老虎機任務。 每一步都是隨機的。 假設機器是可以辨識的，例如當他改變動作價值 (action values) 時你會知道顏色。 目標是要學習策略來結合每個任務，根據你看到的顏色，選擇當前任務最好的動作。  在文獻中也稱為 contextual bandits。\nSummary     k-armed Bandit Problem  $\\varepsilon$-greedy 隨機性的探索不同動作 greedy with optimistic initialization 利用初始值來當作探索策略 UCB 確定性的選擇，並且計算採樣過的動作的不確定性來取得平衡 Gradient bandit algorithm 估計動作偏好 (action preference) 而非動作價值    下圖為各個方法在控制不同參數過後平均取得的獎勵值：\n"},{"id":4,"href":"/RL-notes/Reinforcement-Learning-An-Introduction/introduction/","title":"Ch 1. Introduction","parent":"Reinforcement Learning: An Introduction","content":"    Reinforcement Learning (增強式學習) Elements of Reinforcement Learning  Policy (策略) Reward signal (獎勵訊號) Value function (價值函數) Model (模型)   Limitations and Scope An Extended Example: Tic-Tac-Toe (井字遊戲)  Minimax 動態規劃 演化式方法 使用 RL 並配合價值函數       Reinforcement Learning (增強式學習)    RL 的目標：學習怎麼在給定狀態下，輸出可以得到最大的獎勵 (reward) 的動作 (action)。\nRL 最特別的性質：\n trial-and-error search delayed reward  Reinforcement learning 和其他 -ing 結尾的主題類似 (e.g. machine learning)，同時是個問題也是解決問題的方法。區分問題和解法在 RL 非常重要，搞不清楚時常會造成困惑。\nRL 借用動態系統理論 (dynamical systems theroy) 的馬可夫決策過程 (Markov decision processes) 的觀念來形式化。基本觀念：一個 學習代理 (learning agent) 會隨著 時間 與 環境 (environment) 互動來達成一個 目標 (goal)。學習代理必須可以：\n 感測環境的 狀態 (state) 採取 動作 (action) 來影響環境的狀態  馬可夫決策過程以三個觀點提供最簡化的形式涵蓋這個問題：\n sensation action goal  機器學習三大類：\n supervised learning (監督式學習) unsupervised learning (非監督式學習) 增強式學習  不同之處：\n RL 與 supervised learning 的不同：（略） RL 與 unsupervised learning 的不同：（略）  RL 必須在 exploration (探索) 和 exploitation (利用) 之間取捨：\n 利用：為了要獲得大量的獎勵，RL 代理必須偏好選擇「過去嘗試過最好的」動作 探索：為了做到這件事情，必須探索沒有嘗試過的動作  Elements of Reinforcement Learning    RL 的基本組成：\n the agent (智慧主體，主動進行動作並影響環境狀態的主體) the environment (環境) RL 系統的子元素:  a policy (策略) a reward signal (獎勵訊號) a value function (價值函數) (optional) a model of the environment (環境的模型)    Policy (策略)    策略定義了智慧主體的行為。在給定一個時間點，智慧主體從環境接收狀態，並選擇動作來改變環境。而策略是狀態到動作的映射函數。\n 可能是簡單的函數或是查表，也可能是會涉及複雜計算的演算法 可能是隨機性的，提供採取每個動作的機率  Reward signal (獎勵訊號)    獎勵訊號定義了 RL 問題的目標。在每個時間點，環境會傳送單一數值的訊號給 RL 智慧主體，稱為 獎勵。獎勵給學習代理定義事件的好壞。\n 可能是一個隨機性的函數，根據環境的狀態和採取的動作  Value function (價值函數)    一個狀態的 價值 是智慧主體預期未來會在這個狀態下取得多少獎勵的總和。\n 以最大的價值而非最大的獎勵來採取動作。 獎勵是立即的回饋，價值函數是長期的 獎勵是主要的，價值函數是次要的。  沒有獎勵就沒有價值，估計價值的唯一目的是獲得更多的獎勵。   決定價值比獎勵更難  獎勵通常可以直接由環境取得 價值必須估計、以及來自一個生命週期觀測的結果來重新估計    Model (模型)    模型 用來模擬環境的行為。例如：給定狀態和動作，模型要預測下個狀態和下個獎勵\n模型是用來 planning (規劃)\n model-based model-free  trial-and-error    Limitations and Scope     極度依賴狀態：作為輸入給策略、價值函數和模型，以及來自模型的輸出。  本書探討的問題是假設狀態已經被良好的處理過，可以直接使用。 為了專注在討論決策問題，不考慮如何設計狀態的訊號的問題。   大部分 RL 的方法都圍繞在如何估計價值函數，但這非 RL 的必要條件。不涉及估計價值函數的方法的例子：  基因演算法 (genetic algorithms) 基因規劃 (genetic programming) 模擬退火法 (simulated annealing) 以上是演化式 (evolutionary) 方法，在生命週期中不學習，而是在下個世代產生具備能力的個體。 什麼情況下演化式方法會有優勢：  如果策略空間很小，或是容易被找到、有足夠多的時間搜尋 當無法從環境中感測出完整的狀態     本書專注在環境互動中學習的方法，不包含演化式方法。  An Extended Example: Tic-Tac-Toe (井字遊戲)    井字遊戲是個簡單的問題，但沒有辦法由經典的演算法來適當的解決。以下舉幾個例子來說明：\n 使用 minimax 使用動態規劃 使用演化式方法 使用 RL 並配合價值函數  Minimax     這個方法假設了對手的遊戲策略 讓玩家無法到達一個「可能會輸掉、但實際上對手可能會失誤而因此勝利」的狀態。  動態規劃     需要關於對手的完整規格，包含在每個狀態下會以何種機率採取行動。通常這種資訊是不會先驗的 (a prior) 獲得，大部分實務也不會有。 有一種方式是學習模型來模擬對手的行為，再根據模型來計算動態規劃求出最佳解。最後，這個方法和某些 RL 的方法並無不同。  演化式方法     直接搜尋所有可能的策略，找出一個有高機率獲勝的方法。 此處的策略是一個規則來告訴玩家要下哪一步，對遊戲所有可能狀態 — 每個可能的 O 或 X 的設定 (configuration) 對每一個策略，藉由大量的對戰來估計勝率。 藉由估計值來決定下一次的策略。 可能會用的經典演化式演算法：  爬坡演算法 (hill-climbing algorithm)，會連續的生成和估計策略 基因演算法類    使用 RL 並配合價值函數    步驟如下：\n 設定數值表 (table of numbers)，每一個代表這場遊戲中可能的狀態，每一個數值是當前狀態下勝率的最新估計值。 把這個估計值當作這個狀態的價值，整個表就是學習到的價值函數。 對於所有三個連一直線的狀態的勝率是 1 (或是 0，被對手成功連線時) 。 所有其他狀態的勝率值都初始化為 0.5。 接下來開始對戰很多次。 選擇要下的點，大部分情況下可以使用貪心法挑選最大的價值，也就是最高的勝率。 少部份的情況下選擇其他沒有下過的落點，這稱之為 探索，讓我們可以看過沒見過的狀態。 在遊戲過程中會更新價值函數，使估計的更準確。方式是在每一步過後回補 (back up) 前一步的狀態的價值。 假設當前狀態是 $S_{t}$，經過一個貪心選擇過後下一步是 $S_{t+1}$，這時會更新 $S_{t}$ 的價值函數，標記為 $V(S_{t})$。可以被列式如下：  $V(S_{t}) \\leftarrow V(S_{t}) + \\alpha [ V(S_{t+1}) - V(S_{t})]$  $\\alpha$: 步長參數 (step-size parameter) 這是一個 temporal-difference 學習法的例子         每個實心點代表狀態，實線是狀態轉移的路徑，虛線是可以轉移的路徑 (輪到自己時有多個動作可以選擇) ，* 代表最大價值的狀態。 (Richard S. Sutton and Andrew G. Barto on Reinforcement Learning: An Introduction)   這個方法在這個任務上可以做得很好，因為：\n 步長參數在經過一段時間後適當的減少，會收斂到在給定每個狀態下真實的勝率值。 每一步都是根據對手的落子下的最佳解。  這個例子突顯了 RL 的關鍵特色：\n 強調在與環境互動中學習 目標明確 不只根據當前的狀態，也會考慮後面的發展  "},{"id":5,"href":"/RL-notes/","title":"Reinforcement Learning Notes","parent":"","content":"學習資源     Reinforcement Learning: An Introduction (Richard S. Sutton and Andrew G. Barto)  http://incompleteideas.net/book/the-book.html   OpenAI  https://openai.com/    "},{"id":6,"href":"/RL-notes/categories/","title":"Categories","parent":"Reinforcement Learning Notes","content":""},{"id":7,"href":"/RL-notes/tags/","title":"Tags","parent":"Reinforcement Learning Notes","content":""}]