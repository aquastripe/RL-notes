[{"id":0,"href":"/RL-notes/sutton/tabular-solution-methods/n-step-bootstrapping/","title":"n-step Bootstrapping","parent":"Part I: Tabular Solution Methods","content":"本章探討如何結合 Monte Carlo (MC) 與 temporal difference (TD)。\n   n-step TD Prediction  Error reduction property Example 7.1: n-step TD Methods on the Random Walk       n-step TD Prediction    比較 MC 與 TC 之間的差異。\n考慮如何從以 $\\pi$ 產生的 episodes 預估 $v_{\\pi}$：\n MC: 根據整個 episode 的所有 states 進行更新 1-step TD: 只根據下一個 state 進行更新 n-step TD: 介於兩者的中間，根據下 n 個 state 進行更新 $\\infty$-step TD: 等同於 MC  n-step 的 backup diagram 如下:\n 空心: state 實心: action 方形: 中止狀態  考慮到如何估計 $S_{t}, R_{t+1}, S_{t+1}, R_{t+2}, \\ldots, R_{T}, S_{T}$ 的 value:\n已知 MC 的更新式如下： $$ G_{t} \\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\cdots+\\gamma^{T-t-1} R_{T} $$ 其中 $T$ 是最後一個 time step。在此，稱這個量為更新的 目標 (the target of the update)。\n1-step TD 的更新目標如下： $$ G_{t: t+1} \\doteq R_{t+1}+\\gamma V_{t}\\left(S_{t+1}\\right) $$ 其中 $V_{t}: \\mathcal{S} \\rightarrow \\mathbb{R}$ 代表 $v_{\\pi}$ 在時間 $t$ 時的估計值。\n2-step TD 的更新目標可以推廣如下： $$ G_{t: t+2} \\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} V_{t+1}\\left(S_{t+2}\\right) $$\n以此類推，n-step TD 更新目標如下： $$ G_{t: t+n} \\doteq R_{t+1}+\\gamma R_{t+2}+\\cdots+\\gamma^{n-1} R_{t+n}+\\gamma^{n} V_{t+n-1}\\left(S_{t+n}\\right) $$ 對所有 $n, t$ 使得 $n \\ge 1$ 且 $0 \\le t \\le T-n$。\n所有的 n-step 的 returns 都可以被視為用來近似於 \u0026ldquo;所有的 returns\u0026rdquo;，近似的部份為 $V_{t+n-1}(S_{t+n})$。如果 $t+n \\ge T$ (亦即：n-step 會考慮到超出中止狀態之後的狀態) 則少掉的項會設定為 0。\n注意：當 $n \\gt 1$ 時，n-step returns 會涉及未來的 rewards，所以要到 $t+n$ 後採樣到才能夠計算 $R_{t+n}$ 和 $V_{t+n-1}$。使用 n-step returns 的 state-value 學習演算法如下： $$ V_{t+n}\\left(S_{t}\\right) \\doteq V_{t+n-1}\\left(S_{t}\\right)+\\alpha\\left[G_{t: t+n}-V_{t+n-1}\\left(S_{t}\\right)\\right], \\quad 0 \\leq t\u0026lt;T $$ 同時，對於所有 $s \\ne S_{t}$ 的 state-value 都保持不變。\nError reduction property    n-step returns 使用 $V_{t+n-1}$ 來近似在 $R_{t+n}$ 之後的未知的 rewards。一個重要的性質是：在最糟的狀態下，n-step returns 的期望值保證會比 $V_{t+n-1}$ 更好： $$ \\max {s}\\left|\\mathbb{E}{\\pi}\\left[G_{t: t+n} \\mid S_{t}=s\\right]-v_{\\pi}(s)\\right| \\leq \\gamma^{n} \\max {s}\\left|V{t+n-1}(s)-v_{\\pi}(s)\\right| $$ 對所有 $n \\ge 1$。\n這個性質稱為 error reduction property。這個性質可以說明所有的 n-step 方法都收斂到正確的預測值 (predictions)。\nExample 7.1: n-step TD Methods on the Random Walk    參考 Example 6.2，以下探討設定多少的 n 結果最好。實驗設定：\n 參數 $n$ 與 $\\alpha$ 計算前 10 個 episodes 的結果 實驗重複執行 100 次取平均  結果如下圖：\n從這個實驗可以知道，n-step 有機會比兩個極端 (1-step TD 與 MC) 結果更好。\n"},{"id":1,"href":"/RL-notes/sutton/","title":"Sutton","parent":"Reinforcement Learning Notes","content":""},{"id":2,"href":"/RL-notes/sutton/tabular-solution-methods/temporal-difference-learning/","title":"Temporal Difference Learning","parent":"Part I: Tabular Solution Methods","content":"Temporal-difference (TD) learning 是一個 RL 新穎且重要的觀念，它結合 Monte Carlo 和 dynamic programming (DP) 的想法。\n Monte Carlo: 類似於 Monte Carlo，TD 法可以直接從原始經驗學習，而不需要環境模型。 DP: 類似於 DP，TD 法可以從另一個已學習的估計來更新估計值，而不用等最後結果。     TD Prediction Advantages of TD Prediction Methods  Example 6.2   Optimality of TD(0)  Batch updating   Sarsa: On-policy TD Control Q-learning: Off-policy TD Control  Example 6.6: Cliff Walking   Expected Sarsa Games, Afterstates, and Other Special Cases Summary     TD Prediction    一個 every-visit Monte Carlo 適用於 nonstationary 的環境，value function 更新如下：\n$$ V\\left(S_{t}\\right) \\leftarrow V\\left(S_{t}\\right)+\\alpha\\left[G_{t}-V\\left(S_{t}\\right)\\right] $$\n $G_t$: return following $t$ $\\alpha$: step-size  這個稱為 constant-$\\alpha$ MC。Monte Carlo 法必須等到 episode 結束以後才能更新 $V(S_t)$。因此，TD 法概念是希望可以在每個 time step 都能進行更新。最簡單的 TD 形式如下：\n$$ V\\left(S_{t}\\right) \\leftarrow V\\left(S_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma V\\left(S_{t+1}\\right)-V\\left(S_{t}\\right)\\right] $$\n稱為 TD(0) 或 one-step TD。因為 TD(0) 基於現有的估計值進行更新，所以它是一種 bootstrapping 法，像是 DP。\n比較：\n Monte Carlo: 更新的目標是 $G_t$ TD: 更新的目標是 $R_{t+1} + \\gamma V(S_{t+1})$  $$ \\begin{aligned} v_{\\pi}(s) \u0026amp; \\doteq \\mathbb{E}{\\pi}\\left[G{t} \\mid S_{t}=s\\right] \\newline \u0026amp;=\\mathbb{E}{\\pi}\\left[R{t+1}+\\gamma G_{t+1} \\mid S_{t}=s\\right] \\newline \u0026amp;=\\mathbb{E}{\\pi}\\left[R{t+1}+\\gamma v_{\\pi}\\left(S_{t+1}\\right) \\mid S_{t}=s\\right] \\end{aligned} $$\n Monte Carlo 使用第一式的估計值當作目標。因為 DP 使用第三式的估計值當作目標  \u0026hellip; (?)\nTD(0) 更新值是一種誤差，這個值稱為 TD error:\n$$ \\delta_{t} \\doteq R_{t+1}+\\gamma V\\left(S_{t+1}\\right)-V\\left(S_{t}\\right) $$\n TD error 每個時刻的當下產生。但 TD error 相依於下次的 state 和 reward，它要到下一個 time step 才會存在。 如果 $V$ 在 episode 期間不會改變，那麼 Monte Carlo error 可以寫成 TD error 的和：  $$ \\begin{aligned} G_{t}-V\\left(S_{t}\\right) \u0026amp;=R_{t+1}+\\gamma G_{t+1}-V\\left(S_{t}\\right)+\\gamma V\\left(S_{t+1}\\right)-\\gamma V\\left(S_{t+1}\\right) \\quad(\\text { from }(3.9)) \\newline \u0026amp;=\\delta_{t}+\\gamma\\left(G_{t+1}-V\\left(S_{t+1}\\right)\\right) \\newline \u0026amp;=\\delta_{t}+\\gamma \\delta_{t+1}+\\gamma^{2}\\left(G_{t+2}-V\\left(S_{t+2}\\right)\\right) \\newline \u0026amp;=\\delta_{t}+\\gamma \\delta_{t+1}+\\gamma^{2} \\delta_{t+2}+\\cdots+\\gamma^{T-t-1} \\delta_{T-1}+\\gamma^{T-t}\\left(G_{T}-V\\left(S_{T}\\right)\\right) \\newline \u0026amp;=\\delta_{t}+\\gamma \\delta_{t+1}+\\gamma^{2} \\delta_{t+2}+\\cdots+\\gamma^{T-t-1} \\delta_{T-1}+\\gamma^{T-t}(0-0) \\newline \u0026amp;=\\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_{k} \\end{aligned} $$\n 如果 $V$ 在 episode 期間會改變，這個等式就不精確，不過 step size 夠小的話還是很接近。  Advantages of TD Prediction Methods    TD 與 DP 相比的優勢：\n 不需要環境模型 (model of environment)  TD 與 Monte Carlo 相比的優勢：\n Monte Carlo 必須等待整個 episode 結束以後才能進行計算  收歛性：對任意固定的 policy $\\pi$，如果某個常數的 step-size 參數足夠的小，TD(0) 平均會收斂到 $v_{\\pi}$。如果 step-size 參數根據以下 隨機近似條件 (stochastic approximation conditions) 進行減少，則收斂的機率是 $1$：\n$$ \\sum_{n=1}^{\\infty} \\alpha_{n}(a)=\\infty \\quad \\text { and } \\quad \\sum_{n=1}^{\\infty} \\alpha_{n}^{2}(a)\u0026lt;\\infty $$\nTD 和 Monte Carlo 都能保證收斂的情況下，哪個收斂得更快？\n 這還是一個開放問題，目前沒有數學方法證明出來。 實務上，在隨機的任務上 TD 法通常比 constant-$\\alpha$ MC 收斂得更快。  Example 6.2    Optimality of TD(0)    Batch updating    假設在有限個 episodes。這種情況下，有一種常見的訓練策略：\n利用 (6.1) or (6.2) 的 value function 更新式先對每個 time step $t$ 進行增量計算，最後把整個結果加總後更新 value function 一次。更新完畢以後，再重複以上的過程直到收斂。這個方法稱為批次更新 (batch updating)。\n 只要 step-size 足夠小，TD(0) 就會收斂到一個確定的解。 然而，constant-$\\alpha$ MC 也會確定性的收斂，但有不同的解。  Figure 6.2 展示了利用 batch updating 進行 Example 6.2 的結果。TD 的結果總是比 MC 更好，因為 TD 更相關於預測 returns。\n給定以上的 state-rewards，那麼 $V(A)$ 和 $V(B)$ 為何？\n 有些人可能會認同 $V(B)=\\frac{3}{4}$，因為 $\\frac{6}{8}$ 個 returns 為 $1$，剩下的為 $0$。 $V(A)$ 則有兩種可能：  根據觀察，$A$ 總是轉移到 $B$，而 $V(B)=\\frac{3}{4}$，因此 $V(A)=V(B)=\\frac{3}{4}$。可以把它視為一種 Markov process，結果如下圖。這也是 batch TD(0) 計算的結果。 另一種方式，則是簡單的認為 $A$ 只看過一次且產生出 reward 為 $0$ 的結果，因此 $V(A)=0$。這是 batch MC 的結果。注意到這也會產生最小的訓練誤差，但我們會期望認為第一個答案會更好。如果這個是 Markov process，將會預期在未來的資料產生更小的誤差，而 MC 則會在已知的訓練資料產生較小的誤差。    更進一步說明兩種方法的差異：\n batch MC: 總是找到最小的訓練誤差。 batch TD(0): 總是找到正確的 maximum-likelihood。  Maximum-likelihood estimate 的參數，是會使得產生這些資料的機率為最大值的。 Maximum-likelihood estimate 是 Markov process 的模型：  預測 從 $i$ 到 $j$ 的轉移機率是觀測到所有從 $i$ 到 $j$ 的轉移的分數 (fraction) 對應的 reward 期望值是這些轉移所產生的 rewards 的平均值。   在這個模型下，如果模型絕對正確，我們就可以計算絕對正確的 value function。 這稱為 certainty-equivalence estimate (確定等值估計)。 通常 batch TD(0) 會收練到這個 certainty-equivalence estimate。    複雜度：如果有 $n$ 個狀態，\n 形成 maximum-likelihood estimate 需要 $O(n^2)$ 記憶體 計算 value function 需要 $O(n^3)$ 的複雜度。  Sarsa: On-policy TD Control    現在來探討如何使用 TD prediction 來解決 control problem.\n確保收歛性的定理也適用於 action values，如下:\n$$ Q\\left(S_{t}, A_{t}\\right) \\leftarrow Q\\left(S_{t}, A_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma Q\\left(S_{t+1}, A_{t+1}\\right)-Q\\left(S_{t}, A_{t}\\right)\\right] $$\n收歛性：\n 只要所有 state-action pairs 都以無限次數經歷過，sarsa 會以 $1$ 的機率收斂到 optimal policy 和 action-value function，並且 policy 收斂到 greedy policy 的極限 (converges in the limit to the greedy policy)。  Q-learning: Off-policy TD Control    Q-learning 的更新式定義如下：\n$$ Q\\left(S_{t}, A_{t}\\right) \\leftarrow Q\\left(S_{t}, A_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma \\max {a} Q\\left(S{t+1}, a\\right)-Q\\left(S_{t}, A_{t}\\right)\\right] $$\n要被學習的 action-value function $Q$ 直接近似 optimal action-value function $q_*$，更新過程與 policy 無關。\n Policy 決定哪個 state-action pairs 要被經歷。 為了要正確的收斂，所有的 pairs 都要連續的被更新。  Example 6.6: Cliff Walking    Reward:\n 在 The Cliff 時: $-100$ 其他: $-1$  比較:\n Sarsa Q-learning + $\\varepsilon$-greedy with $\\varepsilon = 0.1$  結果：\n Sarsa: 走了較安全的路，儘管走了較長的路。 Q-learning: 學習到 optimal policy，但是 online performance 較差。  Expected Sarsa    把尋找 最大 的 value 改為尋找其 期望值，如下：\n$$ \\begin{aligned} Q\\left(S_{t}, A_{t}\\right) \u0026amp; \\leftarrow Q\\left(S_{t}, A_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma \\mathbb{E}{\\pi}\\left[Q\\left(S{t+1}, A_{t+1}\\right) \\mid S_{t+1}\\right]-Q\\left(S_{t}, A_{t}\\right)\\right] \\newline \u0026amp; \\leftarrow Q\\left(S_{t}, A_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma \\sum_{a} \\pi\\left(a \\mid S_{t+1}\\right) Q\\left(S_{t+1}, a\\right)-Q\\left(S_{t}, A_{t}\\right)\\right] \\end{aligned} $$\n這稱為 Expected Sarsa。\n 計算量更大 消除選擇動作的變異性   Asymptotic: $100000$ episodes 並實驗 10 次的平均 Interim: $100$ episodes 並實驗 50000 次的平均  討論：\n 在走懸崖的例子，狀態轉移都是確定性的，所有的隨機性來自選擇動作。 因此，Expected Sarsa 可以安全的設定為 $\\alpha = 1$ 而不會減少任何效能。 相對來說，Sarsa 只能用在長期且很小的 $\\alpha$，效果也比較差。  Games, Afterstates, and Other Special Cases    這本書嘗試提出一個通用的方法來處理廣大的應用情境，不過總是會有必須特別對待的例子。\n 通用的目標是尋找 action-value，不過第一章使用 state-value 來進行 tic-tac-toe 遊戲 更仔細來看，一般學習到的 function 不是 action-value 也不是 state-value  傳統的 state-value function 評估一個 state 在 agent 有選擇動作的選項時 tic-tac-toe 的 state-value function 評估其狀態在 agent 移動之後。稱為 afterstates。    Afterstates 好用在當我們得知 一部分 環境的動態 (dynamics) 而不必是 所有的。\n 例如：下棋的時候我們會知道每一個落子之後的位置，但不知道對手如何應對。 Afterstates value function 更有效的處理這類問題。   這個例子中，下面的結果都來自上面兩種落子順序。 傳統的方法會個別計算上面兩種落子的 action-value 但是 afterstates 可以馬上發現這兩種情況是相同的  Afterstates 其他應用\n queuing tasks:  指派 customers 給 servers 拒絕 customers 丟棄資訊    Summary    Temporal-difference (TD)\n prediction problem control problem  GPI    TD control\n on-policy  Sarsa   off-policy  Q-learning expected Sarsa   actor-critic (Ch. 13)  "},{"id":3,"href":"/RL-notes/sutton/tabular-solution-methods/monte-carlo-methods/","title":"Ch 5. Monte Carlo Methods","parent":"Part I: Tabular Solution Methods","content":"    Monte Carlo Prediction  Example 5.1: Blackjack   Monte Carlo Estimation of Action Values  Exploring starts   Monte Carlo Control  Example 5.3: Solving Blackjack   Monte Carlo Control without Exploring Starts  On-policy control methods   Off-policy Prediction via Importance Sampling  Example 5.4: Off-policy Estimation of a Blackjack State Value   Incremental Implementation  Ordinary importance sampling Weighted importance sampling   Off-policy Monte Carlo Control *Discounting-aware Importance Sampling *Per-decision Importance Sampling Summary     和前面的方法相比之下，Monte Carlo 不假設擁有環境的完整知識。\nMonte Carlo 只憑借「經驗」：從環境互動或是透過模擬來採樣 states, actions, and rewards.\n雖然還是需要環境模型 (model)，但只需要採樣 transitions，而不像 DP 需要完整、所有可能的機率分佈。\nMonte Carlo 基於 averaging sample returns.\n為了確保 well-defined returns 存在，在此限定問題在 episodic tasks: 可以分成 episodes 且每個 episode 最後會停止。\nMonte Carlo 採樣並平均 (sample and average) 每個 state-action pair 的 returns，類似於 Ch 2. bandit methods 採樣並平均 rewards。\n差異：現在的問題會有多個 states，問題變成 non-stationary。\n為了解決 non-stationary 問題，採用 DP 法的 GPI。\n差異：\n DP: 計算 value functions MC: 學習 value functions  Monte Carlo Prediction    問題：給定一個 policy，如何學習 state-value function $v_{\\pi}(s)$？\n一個顯而易見的解法：直接採樣 returns 並計算平均值。\n First-visit MC method: the average of the returns following first visits to $s$  詳見下面的演算法 從 1940s 開始已經被大量研究   Every-visit MC method: averages the returns following all visits to $s$  Ch 9 \u0026amp; Ch 12 再討論 不檢查是否第一次探訪 $S_t$    性質：\n 探訪狀態 $s$ 次數趨近無限時，兩種方法都會收斂。 First-visit MC method  根據大數法則收斂到它的期望值 每個平均值都是 unbiased estimate，標準差為 $\\frac{1}{\\sqrt{n}}$，$n$ 為 returns 的數量   Every-visit MC method 以 quadratically (二次函數的) 速率收斂  Example 5.1: Blackjack    規則：\n 無限牌組（抽完放回） 莊家加牌，直到 17 點 no discount usable: 玩家拿到 ace 並且可以計數為 11 點，此時應該跟牌 (hit) 假設玩家的 policy 是：拿到 20 或 21 點停止，其餘加注 結果以下列數值表示：  +1: 贏 -1: 輸 0: 平手    Monte Carlo Estimation of Action Values    如果環境模型不存在，那麼估計 action-values $q_{\\pi}(s, a)$ 會比 state-values $v_{\\pi}(s)$ 更有效。\n First-visit MC method: 平均每個 episode 第一次遇到的 state-action 的 returns Every-visit MC method: 平均所有遇到的 state-action 的 returns  有些 state-action pairs 可能不會遇到。如果 $\\pi$ 是確定性的：從每個 state 只會觀察到一個 action，對於沒有 returns 可以平均的 actions， MC 無法從經驗獲得改善。\n這個問題稱為 maintaining exploration。\nExploring starts    為了可以比較，必須估計所有 action 的 value。Exploring starts 假設：在每個 episode 開始時會指定一個 state-action pair，並強迫所有 pair 都有機會被選擇到。\n這個假設有時候有用，除了「和環境模型直接互動」這種情形。這種情況下，最常見的替代方案是只考慮「在每個 state 以非 0 的機率 隨機性的 來選擇所有動作」的 policies，來確保所有 state-action pairs 都會遇到。\n以下先討論 exploring starts 的假設情況。\nMonte Carlo Control    這一章討論如何把 Monte Carlo estimation 用來近似最佳策略 (optimal policies)。\n整個想法是基於 GPI：以一個迭代的過程，固定 policy 近似 value function，固定 value function 近似 policy。\n假設：\n 我們觀察了無限次數 episodes 這些 episodes 使用 exploring starts 產生  Policy improvement 會以 policy greedy 並根據目前的 value function 來完成。這種情況下，我們有一個 action-value function，因此不需要 model 來建構 policy greedy。Policy greedy 以決定性的方式選擇最大的 action-value：\n$$ \\pi(s) \\doteq \\arg \\max _{a} q(s, a) $$\n根據 policy improvement 定理，對所有 $s \\in \\mathcal{S}$:\n$$ \\begin{aligned} q_{\\pi_{k}}\\left(s, \\pi_{k+1}(s)\\right) \u0026amp;=q_{\\pi_{k}}\\left(s, \\underset{a}{\\arg \\max } q_{\\pi_{k}}(s, a)\\right) \\newline \u0026amp;=\\max {a} q{\\pi_{k}}(s, a) \\newline \u0026amp; \\geq q_{\\pi_{k}}\\left(s, \\pi_{k}(s)\\right) \\newline \u0026amp; \\geq v_{\\pi_{k}}(s) \\end{aligned} $$\n每一輪的 policy 都會比上一輪的更好。\n接下來討論如何消除第一個假設：觀察 無限次數 episodes。對 DP 和 MC 兩者來說，都有兩種方式來消除。\n一種方式是給定誤差範圍的大小來計算估計誤差的機率，可以確保在一定數量的步數內會收斂在這個誤差範圍內。不過，有可能需要太多的步數，導致只能套用在極少數的實務應用上。\n另一種方式，是放棄完整的 policy evaluation。一種極端的情形是 value iteration，在每兩個 policy improvement 之間它只會進行一次迭代 iterative policy evaluation。而 in-place 版本的 value iteration 更為極端，它會在 improvement 和 evaluation 迭代一個狀態。\nMonte Carlo ES: 對於每個 episode，觀察到的 returns 會拿去算 policy evaluation，然後以該 episode 觀察到所有的 states 進行 policy improvement。完整演算法如下：\n（可以用 2.4 的方式進行增量計算）\n Monte Carlo ES 不會收斂到任何 suboptimal policy。  如果發生了，value function 就會到該 policy，導致 policy 改變。   只會在 policy 和 value function 都是 optimal 的時候進入穩定。 收歛性：雖然看起來無可避免的，隨著時間過去 action-value function 改變量會越來越少，不過還沒有形式化的證明。  以作者的觀點認為這是一個開放的 RL 基礎理論問題。    Example 5.3: Solving Blackjack    接續 Example 5.1，使用 \u0026ldquo;20, 21 就停止，其他則跟注\u0026rdquo; 作為初始的 policy，再以 Monte Carlo ES 找到 optimal policy，結果如下圖：\n結果跟 Thorp\u0026rsquo;s strategy 幾乎一致，除了 usable ace 左邊的缺口有所不同。\nMonte Carlo Control without Exploring Starts    Exploring starts 是一種不太可能發生的情形，因此以下討論如何避免這項假設。\n避免這項假設，需要確保所有動作都會無限次的被選擇 有兩種方式達到：\n on-policy: 嘗試評估 (evaluate) 或改善 (improve) 被選擇的用來決策的 policy  Monte Carlo ES method   off-policy: 嘗試評估 (evaluate) 或改善 (improve) 被選擇的用來產生資料的 policy  On-policy control methods    這類的方法，policy 一般來說都是 soft: $\\pi(a | s) \u0026gt; 0$ 對所有 $s \\in \\mathcal{S}$ 和 $a \\in \\mathcal{A}(s)$，但是越來越接近成一個確定性的 optimal policy。大部分 Ch 2. 的方法都是這樣的機制，例如 $\\varepsilon$\u0026ndash;greedy: 大部分選擇的動作是根據最大的 action-value，而少部份根據 $\\varepsilon$ 機率來隨機選擇，也就是 $\\frac{\\varepsilon}{| \\mathcal{A}(s) |}$，而 greedy action 是 $1 - \\varepsilon + \\frac{\\varepsilon}{| \\mathcal{A}(s) |}$。\n$\\varepsilon$\u0026ndash;soft: $\\pi(a | s) \\ge \\frac{\\varepsilon}{| \\mathcal{A}(s) |}$ 對所有 states, action, 還有某個 $\\varepsilon \u0026gt; 0$。\n$\\varepsilon$\u0026ndash;greedy 是一種 $\\varepsilon$\u0026ndash;soft。\nOn-policy Monte Carlo control 整體的想法還是基於 GPI。少了 exploring starts 的假設，不能直接根據目前的 value function 並使用 greedy 取樣來改善 policy，因為這樣會阻止探索 non-greedy actions。\n對任意 $\\varepsilon$\u0026ndash;soft policy $\\pi$，任何根據 $q_{\\pi}$ 的 $\\varepsilon$\u0026ndash;greedy policy 都保證比 $\\pi$ 更好，或是一樣好。演算法如下：\n$$ \\begin{aligned} q_{\\pi}\\left(s, \\pi^{\\prime}(s)\\right) \u0026amp;=\\sum_{a} \\pi^{\\prime}(a \\mid s) q_{\\pi}(s, a) \\newline \u0026amp;=\\frac{\\varepsilon}{|\\mathcal{A}(s)|} \\sum_{a} q_{\\pi}(s, a)+(1-\\varepsilon) \\max {a} q{\\pi}(s, a) \\newline \u0026amp; \\geq \\frac{\\varepsilon}{|\\mathcal{A}(s)|} \\sum_{a} q_{\\pi}(s, a)+(1-\\varepsilon) \\sum_{a} \\frac{\\pi(a \\mid s)-\\frac{\\varepsilon}{|\\mathcal{A}(s)|}}{1-\\varepsilon} q_{\\pi}(s, a) \\newline \u0026amp;=\\frac{\\varepsilon}{|\\mathcal{A}(s)|} \\sum_{a} q_{\\pi}(s, a)-\\frac{\\varepsilon}{|\\mathcal{A}(s)|} \\sum_{a} q_{\\pi}(s, a)+\\sum_{a} \\pi(a \\mid s) q_{\\pi}(s, a) \\newline \u0026amp;=v_{\\pi}(s) \\end{aligned} $$\n因此，根據 policy improvement 定理，$v_{\\pi^\\prime}(s) \\ge v_{\\pi}(s), \\forall s \\in \\mathcal{S}$。\nOff-policy Prediction via Importance Sampling    所有的學習控制 (learning control) 方法都面臨兩難：\n 學習最佳的動作 探索所有的動作  如何做到這件事？\n On-policy: 不為最佳動作學習，而是學習靠近最佳動作 Off-policy: learning from data off the target policy\n採用兩個 policies:  目標策略 (target policy): 用來學習最佳策略 行為策略 (behavior policy): 用來產生行為進行探索    兩者的差異：\n On-policy:  較簡單   Off-policy:  較複雜 較大的變異性，收斂更慢 更強大且更通用 更多額外的應用方式  可以學習由 專家或是傳統非學習的控制器 (controller) 所產生的資料      從兩種 policy 來估計 $v_\\pi$ 或 $q_\\pi$：\n 目標策略: $\\pi$ 行為策略: $b$  為了使用 $b$ 的資料來估計 $\\pi$ 的價值，需要覆蓋性假設 (the assumption of coverage): 每個以 $\\pi$ 策略所採取的動作也同時要考慮以 $b$ 策略來採取。亦即：$\\pi (a|s) \\gt \\text{ implies } b(a|s) \\gt 0$。\n 因此，$b$ 必須是隨機性的，並且與 $\\pi$ 不同。 $\\pi$ 可能是確定性的  幾乎所有 off-policy 方法都會利用 importance sampling: 一種通用的技術，在某個分佈下透過給定另一個分佈來估計期望值。(Importance sampling Wiki)\nImportance sampling\n有時我們可能想從一個隨機分佈上面進行採樣，其中想要採樣的範圍發生的機率很低。使用 MC 採樣時，因為發生的機率很低，會導致產生的樣本數極少而失效。在這個「重要的區域」給予更多的權重，稱為「重要性採樣」。一個基本的重要性採樣方法是：想要計算某個分佈 $p$ 時，透過採用另一個分佈 $q$ 來完成。例如：\n$$ \\mu=\\int_{\\mathcal{D}} f(\\boldsymbol{x}) p(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}=\\int_{\\mathcal{D}} \\frac{f(\\boldsymbol{x}) p(\\boldsymbol{x})}{q(\\boldsymbol{x})} q(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}=\\mathbb{E}_{q}\\left(\\frac{f(\\boldsymbol{X}) p(\\boldsymbol{X})}{q(\\boldsymbol{X})}\\right) $$\nRef: https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf\n 給定一個起始狀態 $S_t$，state-action trajectory $A_t, S_{t+1}, A_{t+1}, \\ldots, S_T$ 在任意策略 $\\pi$ 之下發生的機率為:\n$$ \\begin{array}{l} \\operatorname{Pr} \\lbrace A_{t}, S_{t+1}, A_{t+1}, \\ldots, S_{T} \\mid S_{t}, A_{t: T-1} \\sim \\pi \\rbrace \\newline \\quad=\\pi\\left(A_{t} \\mid S_{t}\\right) p\\left(S_{t+1} \\mid S_{t}, A_{t}\\right) \\pi\\left(A_{t+1} \\mid S_{t+1}\\right) \\cdots p\\left(S_{T} \\mid S_{T-1}, A_{T-1}\\right) \\newline \\quad=\\prod_{k=t}^{T-1} \\pi\\left(A_{k} \\mid S_{k}\\right) p\\left(S_{k+1} \\mid S_{k}, A_{k}\\right) \\end{array} $$\n其中，$p$ 是狀態轉移機率函數，在 3.4 中定義。因此，在目標策略和行為策略之下的 trajectory 的相對機率 (importance sampling ratio) 為:\n$$ \\begin{aligned} \\rho_{t: T-1} \u0026amp;\\doteq \\frac{\\prod_{k=t}^{T-1} \\pi\\left(A_{k} \\mid S_{k}\\right) p\\left(S_{k+1} \\mid S_{k}, A_{k}\\right)}{\\prod_{k=t}^{T-1} b\\left(A_{k} \\mid S_{k}\\right) p\\left(S_{k+1} \\mid S_{k}, A_{k}\\right)}\\newline \u0026amp;=\\prod_{k=t}^{T-1} \\frac{\\pi\\left(A_{k} \\mid S_{k}\\right)}{b\\left(A_{k} \\mid S_{k}\\right)} \\end{aligned} $$\n雖然兩個 trajectory probabilities 依賴於 MDP 的狀態轉移機率，但它們在分子和分母都有出現且值一樣，所以兩個會消掉。最後，importance sampling ratio 只和兩個 policies 和序列有關，和 MDP 無關。\n我們使用行為策略採樣到的 returns 會估計出 行為策略 的 value function:\n$$ \\mathbb{E}\\left[ G_{t} \\mid S_{t}=s \\right]=v_{b}(s) $$\n但是，透過上面的 importance sampling ratio 可以得到 目標策略 的 value function:\n$$ \\mathbb{E}\\left[\\rho_{t: T-1} G_{t} \\mid S_{t}=s\\right]=v_{\\pi}(s) $$\nNotation:\n $\\mathcal{T}(s)$: 當 state $s$ 被探訪的情況下，所有 time steps 的集合 $T(t)$: 在時間點 $t$ 之後第一次停止的時間 $G_t$: 從 $t$ 到 $T(t)$ 之間的 returns $\\lbrace G_{t} \\rbrace_{t \\in \\mathcal{T}(s)}$: 屬於 state $s$ 的 returns $\\lbrace \\rho_{t: T(t)-1}\\rbrace _{t \\in \\mathcal{T}(s)}$: 上面對應的重要性採樣比率 (importance-sampling ratios)  為了估計 $v_\\pi(s)$，我們簡單的藉由 ratios 並計算平均來放大 returns:\n$$ V(s) \\doteq \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t: T(t)-1} G_{t}}{|\\mathcal{T}(s)|} $$\n如果重要性採樣是透過單純的計算平均來完成，稱為 ordinary importance sampling。\n另一種方式，是計算加權平均，稱為 weighted importance sampling:\n$$ V(s) \\doteq \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t: T(t)-1} G_{t}}{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t: T(t)-1}} $$\nExample 5.4: Off-policy Estimation of a Blackjack State Value    以行為策略來估計目標策略的 value。\n設定以下條件：\n 莊家有 deuce 玩家牌總和為 13 玩家有 usable ace 行為策略: 以各半的機率決定要不要加牌 目標策略: 加牌加到總和為 20 或 21  估計誤差如下： Incremental Implementation    以下考慮如何以增量計算進行 MC prediction。\n在 Ch 2. 中對 rewards 進行增量計算，在 on-policy MC prediction 只需要改成 returns 即可。 對於 off-policy，需要分開考慮 ordinary importance sampling 和 weighted importance sampling。\nOrdinary importance sampling    Ordinary importance sampling 中，returns 以 importance sampling ratio $\\rho_{t:T(t)-1}$ 進行放大，如下：\n$$ \\begin{aligned} \\rho_{t: T-1} \u0026amp;\\doteq \\frac{\\prod_{k=t}^{T-1} \\pi\\left(A_{k} \\mid S_{k}\\right) p\\left(S_{k+1} \\mid S_{k}, A_{k}\\right)}{\\prod_{k=t}^{T-1} b\\left(A_{k} \\mid S_{k}\\right) p\\left(S_{k+1} \\mid S_{k}, A_{k}\\right)}\\newline \u0026amp;=\\prod_{k=t}^{T-1} \\frac{\\pi\\left(A_{k} \\mid S_{k}\\right)}{b\\left(A_{k} \\mid S_{k}\\right)} \\end{aligned} $$\n然後再平均：\n$$ V(s) \\doteq \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t: T(t)-1} G_{t}}{|\\mathcal{T}(s)|} $$\n可以直接套用 Ch 2. 的方法並且計算放大和平均來做增量計算。\nWeighted importance sampling    假設我們有 returns 序列 $G_1, G_2, \\ldots, G_{n-1}$，從相同的狀態開始，每個狀態對應一個隨機的權重 $W_i$。我們要估計的如下式：\n$$ V_{n} \\doteq \\frac{\\sum_{k=1}^{n-1} W_{k} G_{k}}{\\sum_{k=1}^{n-1} W_{k}}, \\quad n \\geq 2 $$\n更新規則如下：\n$$ V_{n+1} \\doteq V_{n}+\\frac{W_{n}}{C_{n}}\\left[G_{n}-V_{n}\\right], \\quad n \\geq 1 $$\n$C_n$ 為累積和，計算如下：\n$$ C_{n+1} \\doteq C_{n}+W_{n+1} $$\n其中，$C_0 \\doteq 0$。\nOff-policy Monte Carlo Control    以下是 off-policy MC control 基於 GPI 和 weighted importance sampling，用來估計 $\\pi_$ 和 $q_$。\n 目標策略：根據 $Q$ 的 greedy policy，是一個 $q_\\pi$ 的估計值。 行為策略：可以是任意的，不過為了確保 $\\pi$ 收斂到最佳策略，必須對每一組 state 和 action 取得無限多的 returns。  可以選擇將 $b$ 設定為 $\\varepsilon$-soft 來達到這個條件。    *Discounting-aware Importance Sampling    考慮一種情況：當 episodes 很長且 $\\gamma \\ll 1$，例如 100 steps 和 $\\gamma = 0$。\n時間點 $t = 0$ 的時候，return 會是 $G_0 = R_1$，但 importance sampling ratio 會是 100 個乘積：\n$$ \\frac{\\pi\\left(A_{0} \\mid S_{0}\\right)}{b\\left(A_{0} \\mid S_{0}\\right)} \\frac{\\pi\\left(A_{1} \\mid S_{1}\\right)}{b\\left(A_{1} \\mid S_{1}\\right)} \\cdots \\frac{\\pi\\left(A_{99} \\mid S_{99}\\right)}{b\\left(A_{99} \\mid S_{99}\\right)} $$\n在 ordinary importance sampling 的情況下，return 會被這整個乘積放大，但實際需要的只有第一個，也就是 $\\frac{\\pi\\left(A_{0} \\mid S_{0}\\right)}{b\\left(A_{0} \\mid S_{0}\\right)}$，其他都是不相關的。期望值會是 1 不變，但變異數會不斷增加到無限大。\nFlat partial returns:\n$$ \\bar{G}_ {t: h} \\doteq R_{t+1} + R_{t+2} + \\cdots + R_{h}, \\quad 0 \\leq t\u0026lt;h \\leq T $$\n $h$: horizon，return 所要考慮的時間長度。  完整版的 return $G_t$ 可以視為 flat partial returns 的總和：\n$$ \\begin{aligned} G_{t} \\doteq \u0026amp; R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\cdots+\\gamma^{T-t-1} R_{T} \\newline =\u0026amp;(1-\\gamma) R_{t+1} \\newline \u0026amp;+(1-\\gamma) \\gamma\\left(R_{t+1}+R_{t+2}\\right) \\newline \u0026amp;+(1-\\gamma) \\gamma^{2}\\left(R_{t+1}+R_{t+2}+R_{t+3}\\right) \\newline \u0026amp; \\vdots \\newline \u0026amp;+(1-\\gamma) \\gamma^{T-t-2}\\left(R_{t+1}+R_{t+2}+\\cdots+R_{T-1}\\right) \\newline \u0026amp;+\\gamma^{T-t-1}\\left(R_{t+1}+R_{t+2}+\\cdots+R_{T}\\right) \\newline =\u0026amp;(1-\\gamma) \\sum_{h=t+1}^{T-1} \\gamma^{h-t-1} \\bar{G}{t: h}+\\gamma^{T-t-1} \\bar{G}{t: T} \\end{aligned} $$\n對這個 flat partial returns 以 importance-sampling ratio 進行放大，oridinary importance sampling：\n$$ V(s) \\doteq \\frac{\\sum_{t \\in \\mathcal{T}(s)}\\left((1-\\gamma) \\sum_{h=t+1}^{T(t)-1} \\gamma^{h-t-1} \\rho_{t: h-1} \\bar{G}{t: h}+\\gamma^{T(t)-t-1} \\rho{t: T(t)-1} \\bar{G}_{t: T(t)}\\right)}{|\\mathcal{T}(s)|} $$\nWeighted importance sampling:\n$$ V(s) \\doteq \\frac{\\sum_{t \\in \\mathcal{T}(s)}\\left((1-\\gamma) \\sum_{h=t+1}^{T(t)-1} \\gamma^{h-t-1} \\rho_{t: h-1} \\bar{G}{t: h}+\\gamma^{T(t)-t-1} \\rho{t: T(t)-1} \\bar{G}{t: T(t)}\\right)}{\\sum{t \\in \\mathcal{T}(s)}\\left((1-\\gamma) \\sum_{h=t+1}^{T(t)-1} \\gamma^{h-t-1} \\rho_{t: h-1}+\\gamma^{T(t)-t-1} \\rho_{t: T(t)-1}\\right)} $$\n*Per-decision Importance Sampling    另一種方式來減少變異數，可以用在即使有 discounting ($\\gamma$) 的情況。\nOff-policy estimators (value functions) 的分子，每一項的總和本身也是個總和，如下 (5.11)：\n$$ \\begin{aligned} \\rho_{t: T-1} G_{t} \u0026amp;=\\rho_{t: T-1}\\left(R_{t+1}+\\gamma R_{t+2}+\\cdots+\\gamma^{T-t-1} R_{T}\\right) \\newline \u0026amp;=\\rho_{t: T-1} R_{t+1}+\\gamma \\rho_{t: T-1} R_{t+2}+\\cdots+\\gamma^{T-t-1} \\rho_{t: T-1} R_{T} \\end{aligned} $$\n每一項都跟期望值有關，所以能用更簡單的方式描述。上式的每項都是一個隨機的 reward 和隨機的 importance-sampling ratio 的乘積，例如：\n$$ \\rho_{t: T-1} R_{t+1}=\\frac{\\pi\\left(A_{t} \\mid S_{t}\\right)}{b\\left(A_{t} \\mid S_{t}\\right)} \\frac{\\pi\\left(A_{t+1} \\mid S_{t+1}\\right)}{b\\left(A_{t+1} \\mid S_{t+1}\\right)} \\frac{\\pi\\left(A_{t+2} \\mid S_{t+2}\\right)}{b\\left(A_{t+2} \\mid S_{t+2}\\right)} \\cdots \\frac{\\pi\\left(A_{T-1} \\mid S_{T-1}\\right)}{b\\left(A_{T-1} \\mid S_{T-1}\\right)} R_{t+1} $$\n 有人認為只有第一個和最後一個乘數 (reward) 有用，其他都是在這個 reward 之後發生的事件。 其他項的期望值是 $1$：  $$ \\mathbb{E}\\left[\\frac{\\pi\\left(A_{k} \\mid S_{k}\\right)}{b\\left(A_{k} \\mid S_{k}\\right)}\\right] \\doteq \\sum_{a} b\\left(a \\mid S_{k}\\right) \\frac{\\pi\\left(a \\mid S_{k}\\right)}{b\\left(a \\mid S_{k}\\right)}=\\sum_{a} \\pi\\left(a \\mid S_{k}\\right)=1 $$\n每個乘數在期望值上都沒有作用，也就是：\n$$ \\mathbb{E}\\left[\\rho_{t: T-1} R_{t+1}\\right]=\\mathbb{E}\\left[\\rho_{t: t} R_{t+1}\\right] $$\n如果重複這個流程，對於第 k 個 (5.11) 的子項，可以得到以下：\n$$ \\mathbb{E}\\left[\\rho_{t: T-1} R_{t+k}\\right]=\\mathbb{E}\\left[\\rho_{t: t+k-1} R_{t+k}\\right] $$\n原本的期望值可以被寫成如下：\n$$ \\mathbb{E}\\left[\\rho_{t: T-1} G_{t}\\right]=\\mathbb{E}\\left[\\tilde{G}_{t}\\right] $$\n其中，\n$$ \\tilde{G}_ {t}=\\rho_{t: t} R_{t+1}+\\gamma \\rho_{t: t+1} R_{t+2}+\\gamma^{2} \\rho_{t: t+2} R_{t+3}+\\cdots+\\gamma^{T-t-1} \\rho_{t: T-1} R_{T} $$\n這個稱為 per-decision importance sampling。\n使用這個 importance sampling 套在 ordinary importance sampling 之後，可以得到以下：\n$$ V(s) \\doteq \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\tilde{G}_{t}}{|\\mathcal{T}(s)|} $$\n這個會產生較小的變異數。\nSummary    Monte Carlo 方法被提出來，以採樣的方式來學習 value functions 和 optimal policies。對比於 DP 法，有至少三個優勢：\n 直接跟環境互動，不需要環境模型。 可以用模擬 (simulation) 或是採樣模型 (sample models) 有簡單有效的方式把 Monte Carlo 方法用來聚焦處理 所有狀態中的一個小子集 (it is easy and efficient to focus Monte Carlo methods on a small subset of the states)，在 Ch 8. 進行討論。 較少違反 Markov 性質，因為它不會基於連續狀態的 value 來更新 value。也就是沒有 bootstrap。  "},{"id":4,"href":"/RL-notes/sutton/tabular-solution-methods/dynamic-programming/","title":"Ch 4. Dynamic Programming","parent":"Part I: Tabular Solution Methods","content":"    Policy Evaluation (Prediction)  Iterative policy evaluation (迭代策略評估)   Policy Improvement  Policy improvement theorem   Policy Iteration Value Iteration Asynchronous Dynamic Programming Generalized Policy Iteration (GPI) Efficiency of Dynamic Programming Summary     Dynamic programming (DP, 動態規劃): a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP).\nDP 的限制：\n 假設一個完美的環境模型 消耗龐大的運算量  通常也會假設環境是一個 finite MDP (有限馬可夫決策過程)。\nDP 的核心觀念：使用價值函數來組織並結構化「尋找好的策略」。\nPolicy Evaluation (Prediction)    Policy evaluation: 如何為任意的策略 $\\pi$ 計算 狀態-價值函數 $v_{\\pi}$\n$$ \\begin{aligned} v_{\\pi}(s) \u0026amp; \\doteq \\mathbb{E}{\\pi}\\left[G{t} \\mid S_{t}=s\\right] \\newline \u0026amp;=\\mathbb{E}{\\pi}\\left[R{t+1}+\\gamma G_{t+1} \\mid S_{t}=s\\right] \\newline \u0026amp;=\\mathbb{E}{\\pi}\\left[R{t+1}+\\gamma v_{\\pi}\\left(S_{t+1}\\right) \\mid S_{t}=s\\right] \\newline \u0026amp;=\\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right] \\end{aligned} $$\n$v_{\\pi}$ 保證存在且唯一的條件 (任一):\n $\\gamma \u0026lt; 1$ 從策略 $\\pi$ 之下的所有狀態，最後會進入中止狀態  如果環境的動態是完全已知的，那麼 (4.4) 式是一個 $| \\mathcal{S} |$ 未知的聯立線性方程 (simultaneous linear equations) ($v_{\\pi}, s \\in \\mathcal{S}$)\n (4.4): $v_{\\pi}(s)=\\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right]$ 這種情況可以直接求解 (tedious)  Iterative policy evaluation (迭代策略評估)    迭代解法會更適用 RL 的情境。\n考慮到一個近似價值函數的序列 $v_0, v_1, v_2, \\dots,$ 每個映射 $\\mathcal{S}^+$ 到 $\\mathbb{R}$。 選定任意初始價值 $v_0$ (除了中止狀態必須給值為 0)，對所有狀態 $s \\in \\mathcal{S}$，可以利用 Bellman equation (4.4) 進行連續近似：\n$$ \\begin{aligned} v_{k+1}(s) \u0026amp; \\doteq \\mathbb{E}{\\pi}\\left[R{t+1}+\\gamma v_{k}\\left(S_{t+1}\\right) \\mid S_{t}=s\\right] \\newline \u0026amp;=\\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{k}\\left(s^{\\prime}\\right)\\right] \\end{aligned} $$\n 當 $k \\rightarrow \\infty$ 時， $\\lbrace v_k \\rbrace$ 收斂到 $v_{\\pi}$  為了產生每個連續的近似，迭代策略評估對每個狀態 $s$ 套用相同的操作：\n 利用 期望立即獎勵 (expected immediate rewards)，在被評估的策略下，沿著所有可能的單步狀態轉移 (one-step transitions)，以新的狀態價值取代舊的。  這個操作稱為 期望更新 (expected update)。\n 對所有可能的狀態計算期望值，而非採樣一個狀態。  Iterative Policy Evaluation, for estimating $V \\approx v_{\\pi}$\nInput $\\pi$, the policy to be evaluated\nAlgorithm parameter:\n a small threshold $\\theta \u0026gt; 0$ determining accuracy of estimation  Initialize $V(s)$, for all $s \\in \\mathcal{S}^{+}$, arbitrarily except that $V( terminal )=0$\n Loop: \u0026hellip; $\\Delta \\leftarrow 0$ \u0026hellip; Loop for each $s \\in \\mathcal{S}:$ \u0026hellip; \u0026hellip; $v \\leftarrow V(s)$ \u0026hellip; \u0026hellip; $V(s) \\leftarrow \\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma V\\left(s^{\\prime}\\right)\\right]$ \u0026hellip; \u0026hellip; $\\Delta \\leftarrow \\max (\\Delta,|v-V(s)|)$ until $\\Delta\u0026lt;\\theta$   Policy Improvement    計算 價值函數 是為了找到更好的 策略。\n假設我們已經決定了 價值函數 $v_{\\pi}$ 對任意確定性的策略 $\\pi$。\n對某個狀態 $s$ 我們可能想知道是否我們應該改變策略來確定性的選擇一個動作 $a \\neq \\pi(s)$。\n我們想知道 $v_{\\pi}(s)$ 有多好，但改變策略可能會變得更好或更壞。\n一種方式是：考慮在狀態 $s$ 之下選擇一個動作 $a$，經由目前的策略 $\\pi$。這種情況下，價值為：\n$$ \\begin{aligned}q_{\\pi}(s, a) \u0026amp; \\doteq \\mathbb{E}\\left[R_{t+1}+\\gamma v_{\\pi}\\left(S_{t+1}\\right) \\mid S_{t}=s, A_{t}=a\\right] \\newline \u0026amp;=\\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right]\\end{aligned} $$\n這個價值比 $v_{\\pi}(s)$ 大或小？如果比較大，代表在狀態 $s$ 下，選擇動作 $a$ 會有較大的價值。\nPolicy improvement theorem    Let $\\pi$ and $\\pi^\\prime$ be any pair of deterministic policies such that, for all $s \\in \\mathcal{S}$, (4.7):\n$$q_{\\pi}\\left(s, \\pi^{\\prime}(s)\\right) \\geq v_{\\pi}(s)$$\nThen the policy $\\pi^\\prime$ must be as good as, or better than, $\\pi$. That is, it must obtain greater or equal expected return from all states $s \\in \\mathcal{S}$, (4.8):\n$$v_{\\pi^{\\prime}}(s) \\geq v_{\\pi}(s)$$\nMoreover, if there is strict inequality of (4.7) at any state, then there must be strict inequality of (4.8) at that state.\n$$\\begin{aligned}v_{\\pi}(s) \u0026amp; \\leq q_{\\pi}\\left(s, \\pi^{\\prime}(s)\\right) \\newline\u0026amp;=\\mathbb{E}\\left[R_{t+1}+\\gamma v_{\\pi}\\left(S_{t+1}\\right) \\mid S_{t}=s, A_{t}=\\pi^{\\prime}(s)\\right] \\newline\u0026amp;=\\mathbb{E}{\\pi^{\\prime}}\\left[R{t+1}+\\gamma v_{\\pi}\\left(S_{t+1}\\right) \\mid S_{t}=s\\right] \\newline\u0026amp; \\leq \\mathbb{E}{\\pi^{\\prime}}\\left[R{t+1}+\\gamma q_{\\pi}\\left(S_{t+1}, \\pi^{\\prime}\\left(S_{t+1}\\right)\\right) \\mid S_{t}=s\\right] \\newline\u0026amp;=\\mathbb{E}{\\pi^{\\prime}}\\left[R{t+1}+\\gamma \\mathbb{E}\\left[R_{t+2}+\\gamma v_{\\pi}\\left(S_{t+2}\\right) \\mid S_{t+1}, A_{t+1}=\\pi^{\\prime}\\left(S_{t+1}\\right)\\right] \\mid S_{t}=s\\right] \\newline\u0026amp;=\\mathbb{E}{\\pi^{\\prime}}\\left[R{t+1}+\\gamma R_{t+2}+\\gamma^{2} v_{\\pi}\\left(S_{t+2}\\right) \\mid S_{t}=s\\right] \\newline\u0026amp; \\leq \\mathbb{E}{\\pi^{\\prime}}\\left[R{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\gamma^{3} v_{\\pi}\\left(S_{t+3}\\right) \\mid S_{t}=s\\right] \\newline\u0026amp; \\vdots \\newline\u0026amp; \\leq \\mathbb{E}{\\pi^{\\prime}}\\left[R{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\gamma^{3} R_{t+4}+\\cdots \\mid S_{t}=s\\right] \\newline\u0026amp;=v_{\\pi^{\\prime}}(s) .\\end{aligned}$$\n考慮新的 greedy 策略 $\\pi^\\prime$\n$$\\begin{aligned}\\pi^{\\prime}(s) \u0026amp; \\doteq \\underset{a}{\\arg \\max } q_{\\pi}(s, a) \\newline\u0026amp;=\\underset{a}{\\arg \\max } \\mathbb{E}\\left[R_{t+1}+\\gamma v_{\\pi}\\left(S_{t+1}\\right) \\mid S_{t}=s, A_{t}=a\\right] \\newline\u0026amp;=\\underset{a}{\\arg \\max } \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right]\\end{aligned}$$\n這個 greedy 策略執行一步後，根據最好的價值 $v_{\\pi}$ 來採取動作。\n上面討論的是確定性的策略，對於隨機性的策略可以很輕易的由此擴展。圖 4.1 是一個隨機性策略的更新例子。\nPolicy Iteration    一個策略 $\\pi$ 經由價值函數 $v_{\\pi}$ 產生更好的策略 $\\pi^\\prime$，之後進一步計算新的價值函數與更好的策略\u0026hellip;由此可以得到一個序列：\n$$ \\pi_{0} \\stackrel{\\mathrm{E}}{\\longrightarrow} v_{\\pi_{0}} \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{1} \\stackrel{\\mathrm{E}}{\\longrightarrow} v_{\\pi_{1}} \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{2} \\stackrel{\\mathrm{E}}{\\longrightarrow} \\cdots \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{} \\stackrel{\\mathrm{E}}{\\longrightarrow} v_{} $$\n其中 $\\mathrm{E}$ 表示為 evaluation，$\\mathrm{I}$ 表示為 improvement。\n由於 finite MDP 只會有有限個策略數量，這個過程必定在有限的迭代次數收斂到最佳的策略和價值函數。這個尋找最佳策略的方法稱為 policy iteration。這個方法大幅提昇 policy evaluation 的收斂速度。\nPolicy Iteration (using iterative policy evaluation) for estimating $\\pi \\approx \\pi_{*}$\n Initialization\n$V(s) \\in \\mathbb{R}$ and $\\pi(s) \\in \\mathcal{A}(s)$ arbitrarily for all $s \\in \\mathcal{S}$ Policy Evaluation  Loop: \u0026hellip; $\\Delta \\leftarrow 0$ \u0026hellip; \u0026hellip; Loop for each $s \\in \\mathcal{S}$: \u0026hellip; \u0026hellip; $v \\leftarrow V(s)$ \u0026hellip; \u0026hellip; $V(s) \\leftarrow \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, \\pi(s)\\right)\\left[r+\\gamma V\\left(s^{\\prime}\\right)\\right]$ \u0026hellip; \u0026hellip; $\\Delta \\leftarrow \\max (\\Delta,|v-V(s)|)$ until $\\Delta\u0026lt;\\theta$ (a small positive number determining the accuracy of estimation)   Policy Improvement  $policy\u0026ndash;stable$ $\\leftarrow$ true For each $s \\in \\mathcal{S}$: \u0026hellip; old-action $\\leftarrow \\pi(s)$ \u0026hellip; $\\pi(s) \\leftarrow \\arg \\max_ {a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma V\\left(s^{\\prime}\\right)\\right]$ \u0026hellip; If old-action $\\neq \\pi(s)$, then $policy\u0026ndash;stable$ $\\leftarrow$ false If $policy\u0026ndash;stable$, then \u0026hellip; stop and return $V \\approx v_{}$ and $\\pi \\approx \\pi_{} ;$ else \u0026hellip; go to 2     Value Iteration    Policy iteration 的一個缺陷是迭代涉及 policy evaluation，這需要多次的 sweep (採訪所有狀態後進行更新)，導致計算緩慢。從圖 4.1 可以得到一點線索，或許可以裁剪 policy evaluation，因為在第三次以後 policy evaluation 就不會再改變 policy。裁剪並不破壞收斂性的方式有很多種，最重要的一種方式稱為 value iteration (價值迭代) (4.10):\n$$\\begin{aligned}v_{k+1}(s) \u0026amp; \\doteq \\max {a} \\mathbb{E}\\left[R{t+1}+\\gamma v_{k}\\left(S_{t+1}\\right) \\mid S_{t}=s, A_{t}=a\\right] \\newline\u0026amp;=\\max {a} \\sum{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{k}\\left(s^{\\prime}\\right)\\right]\\end{aligned}$$\n保證存在 optimal value $v_{}$ 的條件下，$\\lbrace v_k \\rbrace$ 最後會收斂到 $v_{}$。\nValue Iteration, for estimating $\\pi \\approx \\pi_{*}$\nAlgorithm parameter: a small threshold $\\theta\u0026gt;0$ determining accuracy of estimation\nInitialize $V(s)$, for all $s \\in \\mathcal{S}^{+}$, arbitrarily except that $V(terminal)=0$\n Loop: \u0026hellip; $\\Delta \\leftarrow 0$ \u0026hellip; Loop for each $s \\in \\mathcal{S}$: \u0026hellip; \u0026hellip; $v \\leftarrow V(s)$ \u0026hellip; \u0026hellip; $V(s) \\leftarrow \\max_{a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma V\\left(s^{\\prime}\\right)\\right]$ \u0026hellip; \u0026hellip; $\\Delta \\leftarrow \\max (\\Delta,|v-V(s)|)$ until $\\Delta\u0026lt;\\theta$  Output a deterministic policy, $\\pi \\approx \\pi_{*}$, such that\n$\\pi(s)=\\arg \\max_{a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma V\\left(s^{\\prime}\\right)\\right]$\n Asynchronous Dynamic Programming    DP 法主要的缺陷是涉及 sweep 整個狀態集。如果整個狀態集非常大，就會需要昂貴運算量。非同步的 DP\nGeneralized Policy Iteration (GPI)    Policy iteration 包含兩個同時、互動的過程：\n Policy evaluation: 讓價值函數和目前的策略一致 Policy improvement: 讓 policy greedy 考慮目前的價值函數  Efficiency of Dynamic Programming    DP 在大多數的問題上並不實用，但是相較其他解決 MDPs 的方法，DP 非常有效率。在 worst case，DP 找到最佳策略的時間保證是狀態與動作數量的多項式函數，即使 (決定性的) 策略的總數是 $k^n$。\nLP (Linear Programming, 線性規劃) 也能解決 MDPs。在某些情況下，它的 worst-case 保證收斂的比 DP 更好。但是 LP 實務上可以解決的狀態數比 DP 更少 (相差大約 100 倍)。在大型問題上，只有 DP 可行。\n由於維度災難 (curse of dimensionality)，DP 被認為適用性很受限。不過在狀態數量較大的問題上，它們是本質上困難的，並不限於 DP 解法。這類問題使用 DP 都還是比直接搜尋和 LP 解法來得更好。\nSummary     Policy evaluation: 一種計算方式，迭代式的計算價值 $v_{\\pi}$ 給定一個策略 $\\pi$。 Policy improvement: 一種計算方式，用來改善策略 $\\pi$ 給定價值函數 $v_{\\pi}$。 Policy iteration Value iteration  對於有限的馬可夫決策過程，這些方法可以用來求出最佳策略和價值函數。\n expected update: 根據所有可能的狀態和它們的發生機率，來更新一個狀態的價值。 Generalized policy iteration (GPI): 一種通用的想法，兩個互動的過程圍繞著近似策略和近似價值函數。幾乎所有 RL 方法都可以被視為 GPI。 Asynchronous DP: 一種 in-place 的迭代方法，以任意的順序更新狀態。  "},{"id":5,"href":"/RL-notes/sutton/tabular-solution-methods/finite-markov-decision-processes/","title":"Ch 3. Finite Markov Decision Processes","parent":"Part I: Tabular Solution Methods","content":"    The Agent-Environment Interface Goals and Rewards Returns and Episodes  Episodic tasks Continuing tasks Discounting   Unified Notation for Episodic and Continuing Tasks Policies and Value Functions Optimal Policies and Optimal Value Functions Optimality and Approximation     這章談論的問題是 associative aspect: 在不同狀況選擇不同的動作。\nMDPs 是一個經典的連續決策的形式，動作會影響當下的獎勵、子序列的狀態，還有未來的獎勵。\n權衡:\n immediate reward (當下的獎勵) delayed reward (延遲的獎勵)  差異:\n bandit problem: 估計價值 $q_{*}(a)$ MDPs: 估計  狀態動作價值 $q_{*}(s, a)$ 狀態價值 $v_{*}(s)$ (given optimal action selections)    關鍵元素:\n mathematical structure returns value functions Bellman equations  The Agent-Environment Interface    $$S_0, A_0, R_1, S_1, A_1, R_2, \\dots$$\n在有限的 MDP, 隨機變數 $R_t$ 和 $S_t$ 有隨機機率分佈只依賴於先前的動作和狀態。\n$$p( s^{\\prime}, r \\mid s, a) \\doteq \\operatorname{Pr} \\lbrace S_{t}=s^{\\prime}, R_{t}=r \\mid S_{t-1}=s, A_{t-1}=a \\rbrace$$\nfor all $s', s \\in \\mathcal{S}, r \\in \\mathcal{R}$, and $a \\in \\mathcal{A}(s)$.\n函數 $p: \\mathcal{S} \\times \\mathcal{R} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$ 定義了 MDP 的 dynamics.\n條件機率的符號提醒我們：$p$ 對每個選擇 $s$ 和 $a$ 指定一個機率分佈，也就是:\n$$\\sum_{s^{\\prime} \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p (s^{\\prime}, r \\mid s, a ) = 1, \\text { for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)$$\n$S_t$ 和 $A_t$ 的機率只依賴於先前的狀態和動作, $S_{t-1}$ and $A_{t-1}$.\n 這是一個馬可夫性質 (Markov property)。  對於環境，我們可以計算狀態轉移機率 $p: \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$\n$$p\\left(s^{\\prime} \\mid s, a\\right) \\doteq \\operatorname{Pr}\\lbrace S_{t}=s^{\\prime} \\mid S_{t-1}=s, A_{t-1}=a\\rbrace=\\sum_{r \\in \\mathcal{R}} p\\left(s^{\\prime}, r \\mid s, a\\right)$$\n計算 狀態-動作 (state-action pairs) 的期望獎勵 (expected rewards) $r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$\n$$r(s, a) \\doteq \\mathbb{E}\\left[R_{t} \\mid S_{t-1}=s, A_{t-1}=a\\right]=\\sum_{r \\in \\mathcal{R}} r \\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime}, r \\mid s, a\\right)$$\n計算 狀態-動作-下個狀態 (state-action-next-state pairs) 的期望獎勵 $r: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}$\n$$r\\left(s, a, s^{\\prime}\\right) \\doteq \\mathbb{E}\\left[R_{t} \\mid S_{t-1}=s, A_{t-1}=a, S_{t}=s^{\\prime}\\right]=\\sum_{r \\in \\mathcal{R}} r \\frac{p\\left(s^{\\prime}, r \\mid s, a\\right)}{p\\left(s^{\\prime} \\mid s, a\\right)}$$\n通用的規則是：任何事情無法被智慧主體任意改變的，都被視為它的外部，也就成為環境的一部分。智慧主體和環境的邊界在於智慧主體的可以絕對控制的範圍。\nGoals and Rewards    Reward hypothesis:\n可以把目標視為最大化累積獎勵的期望值。\nThe reward signal is your way of communicating to the agent what you want to achieve, not how you want it to achieve.\nReturns and Episodes    We have said that the agent’s goal is to maximize the cumulative reward it receives in the long run.\n$$G_{t} \\doteq R_{t+1}+R_{t+2}+R_{t+3}+\\cdots+R_{T}$$\n Return: 獎勵序列的某個特定函數 Episodes: 當智慧主體和環境的互動自然的分成子序列的時刻 Terminal state: 每個 episode 在特別的狀態下結束，之後會重整為起始狀態。  Episodic tasks    有 episode 的任務稱作 episodic tasks。\n non-terminal states: $\\mathcal{S}$ terminal state: $\\mathcal{S}^+$ 終止時間 $T$ 是一個隨機變數，會隨著 episode 變化。  Continuing tasks    $T=\\infty$\nThus, in this book we usually use a definition of return that is slightly more complex conceptually but much simpler mathematically\nDiscounting    $$G_{t} \\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\cdots=\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}$$\n $\\gamma$: a parameter $0 \\le \\gamma \\le 1$ $\\gamma \\lt 1$: $G_t$ is a finite value as long as the reward sequence ${R_k}$ is bounded.  $R_k$ 數值有上限   $\\gamma = 0$: the agent is myopic $\\gamma \\approx 1$: the agent is farsighted  $$\\begin{aligned} G_{t} \u0026amp; \\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\gamma^{3} R_{t+4}+\\cdots \\newline \u0026amp;=R_{t+1}+\\gamma\\left(R_{t+2}+\\gamma R_{t+3}+\\gamma^{2} R_{t+4}+\\cdots\\right) \\newline \u0026amp;=R_{t+1}+\\gamma G_{t+1} \\end{aligned}$$\n即使是無限項，reward 還是有限的\ne.g. if the reward is constant $+1$\n$$G_{t}=\\sum_{k=0}^{\\infty} \\gamma^{k}=\\frac{1}{1-\\gamma}$$\nUnified Notation for Episodic and Continuing Tasks    可以藉由把 episode termination 是否進入 absorbing state 來統一兩種 tasks\n吸收狀態 (absorbing state): 狀態只會轉移到自己，並且產生 0 reward\n重新定義 return 的計算，省略了 episode 次數：\n$$G_{t} \\doteq \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_{k}$$\nPolicies and Value Functions    Policy: 映射 states 到選擇某個動作的機率\n $\\pi (a | s)$  Value functions: 給定 states 或 state-action pairs，估計 agent 的 states 有多好。\n $v_{\\pi} (s)$  $$v_{\\pi}(s) \\doteq \\mathbb{E}{\\pi}\\left[G{t} \\mid S_{t}=s\\right]=\\mathbb{E}{\\pi}\\left[\\sum{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\mid S_{t}=s\\right], \\text { for all } s \\in \\mathcal{S}$$\n the state-action-value function for policy $\\pi$  $$q_{\\pi}(s, a) \\doteq \\mathbb{E}{\\pi}\\left[G{t} \\mid S_{t}=s, A_{t}=a\\right]=\\mathbb{E}{\\pi}\\left[\\sum{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\mid S_{t}=s, A_{t}=a\\right]$$\n the state-value function for policy $\\pi$  $$\\begin{aligned} v_{\\pi}(s) \u0026amp; \\doteq \\mathbb{E}{\\pi}\\left[G{t} \\mid S_{t}=s\\right] \\newline \u0026amp;=\\mathbb{E}{\\pi}\\left[R{t+1}+\\gamma G_{t+1} \\mid S_{t}=s\\right] \\newline \u0026amp;=\\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}} \\sum_{r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma \\mathbb{E}{\\pi}\\left[G{t+1} \\mid S_{t+1}=s^{\\prime}\\right]\\right] \\newline \u0026amp;=\\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right], \\quad \\text { for all } s \\in \\mathcal{S}, \\end{aligned}$$\nMonte Carlo method: 採樣的次數越多 (趨近於無限)，價值估計就會越準 (收斂)。\n一個使用 RL 和 動態規劃的價值函數性質：符合遞迴關係式。\nBellman equation:\n$$\\begin{aligned} v_{\\pi}(s) \u0026amp; \\doteq \\mathbb{E}{\\pi}\\left[G{t} \\mid S_{t}=s\\right] \\newline \u0026amp;=\\mathbb{E}{\\pi}\\left[R{t+1}+\\gamma G_{t+1} \\mid S_{t}=s\\right] \\newline \u0026amp;=\\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}} \\sum_{r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma \\mathbb{E}{\\pi}\\left[G{t+1} \\mid S_{t+1}=s^{\\prime}\\right]\\right] \\newline \u0026amp;=\\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right], \\quad \\text { for all } s \\in \\mathcal{S}, \\end{aligned}$$\n$s^\\prime$: 下個狀態\n 空心圓代表一個 state 實心圓代表一個 state-action pair  Optimal Policies and Optimal Value Functions    Optimal policy: $\\pi \\ge \\pi^\\prime$ if and only if $v_\\pi (s) \\ge v_{\\pi^\\prime} (s)$ for all $s \\in \\mathcal{S}$\n如果超過一個：就把所有的 optimal policies 定義為 $\\pi_{*}$\nOptimal state-value function:\n$$\\begin{array}{l} v_{*}(s) \\doteq \\max_{\\pi}{v_{\\pi}(s)} \\newline \\text { for all } s \\in \\mathcal{S} . \\end{array}$$\nOptimal action-value function\n$$q_{*}(s, a) \\doteq \\max_{\\pi} {q_{\\pi}(s, a)},$$\n可以用 $v_{*}$ 重寫如下：\n$$q_{}(s, a)=\\mathbb{E}\\left[R_{t+1}+\\gamma v_{}\\left(S_{t+1}\\right) \\mid S_{t}=s, A_{t}=a\\right]$$\nOptimality and Approximation    我們定義了 optimal value functions 和 optimal policies。不過實務上難以使用，因為計算量太大了。即使我們有個精準估計環境的模型，通常也不可能透過解出 Bellman optimality equation 計算一個 optimal policy。\n記憶體大小是重要的條件。對於狀態空間小的，可以求出近似解，這種案例稱為 tabular case。實務上，多數的問題涉及更大的狀態，這類的必須要用某種參數化的函數 (parameterized function representation)。\n"},{"id":6,"href":"/RL-notes/sutton/tabular-solution-methods/","title":"Part I: Tabular Solution Methods","parent":"Sutton","content":"  表格式解法 (Tabular Solution Method) 為 RL 最簡單的形式：當所有的狀態和動作數量少到可以用多維陣列來表示價值函數。這些方法通常可以找到精確解 (exact solution)，也就是精確的 價值函數 和 策略 的極值。\n第一章是 RL 的特殊形式：只有一個狀態，稱為 吃角子老虎機問題 (bandit problems)。\n第二章是更通用的問題形式：有限馬可夫決策過程，主要概念包含 貝爾曼方程 (Bellman equations；又稱為 動態規劃方程) 和 價值函數。\n第三、四、五章描述三個基本類型的方法來解決 有限馬可夫決策 問題：動態規劃 (dynamic programming)、蒙地卡羅法 (Monte Carlo methods)、時序差分學習法 (temporal difference learning)。每個方法各有優劣：\n 動態規劃：需要完整且精確的環境模型 蒙地卡羅法：不需要完整且精確的環境模型，但是不適合逐步的增量計算。  增量計算 (incremental computation): 是一個軟體功能，當只有一小塊資料改變的時候，只會對產生變化的部分進行計算和更新，以節省計算時間。(is a software feature which, whenever a piece of data changes, attempts to save time by only recomputing those outputs which depend on the changed data.)   時序差分學習法：不需要完整的環境模型，也完全支援增量計算，但是更複雜而難以分析。  第六、七章描述這三種方法如何結合各自的優點。第六章描述如何用 多步拔靴法 (自助法；自助抽樣法；multi-step bootstrapping methods) 結合 蒙地卡羅法 和 時間差分學習法。第七章描述如何以 時序差分學習法 結合 模型學習 (model learning) 和 規劃法 (例如動態規劃) 來解決通用的表格式 RL 問題。\n Multi-Armed Bandits  "},{"id":7,"href":"/RL-notes/sutton/tabular-solution-methods/multi-armed-bandits/","title":"Ch 2. Multi-Armed Bandits","parent":"Part I: Tabular Solution Methods","content":"    A k-armed Bandit Problem Action-value Methods  Sample-average method Greedy $\\varepsilon$-greedy   The 10-armed Testbed Incremental Implementation Tracking a Nonstationary Problem  可變長度的步長 (step-size)   Optimistic Initial Value Upper-Confidence-Bound Action Selection Gradient Bandit Algorithms Associative Search (Contextual Bandits) Summary     A k-armed Bandit Problem    k 臂吃角子老虎機問題：你需要從 k 個不同 選項 或是 動作 中做出選擇，每個選擇都會帶來獎勵，獎勵多寡來自固定的機率分佈。你的目標是要在經過一段時間後得到最大的累積獎勵值。\n在這個問題中，每個動作會有期望或平均的獎勵值，讓我們稱之為該動作的 價值。我們把在時間點 $t$ 選擇的動作表示為 $A_t$，對應的獎勵表示為 $R_t$。那麼：任意動作 $a$ 的價值 $q_{*}(a)$ 是選擇該動作的期望值：\n$$q_{*}(a) \\doteq \\mathbb{E}[R_t | A_t = a].$$\n我們把 在時間點 $t$ 選擇動作 $a$ 的估計價值表示為 $Q_t(a)$，我們想要此估計值要接近 $q_{*}(a)$。\nAction-value Methods    估計動作的價值，作為根據來做決策，這類的方法通稱為 action-value methods。\nSample-average method    一個自然的方式是計算平均得到多少的獎勵來當作估計值：\n$$Q_t(a) \\doteq \\dfrac{\\text{sum of rewards when } a \\text{ taken prior to }t}{\\text{number of times } a \\text{ taken prior to }t} = \\dfrac{\\sum_{i=1}^{t-1}{R_i \\cdot \\mathbb{1}{A_i=a}}}{\\sum{i=1}^{t-1}{\\mathbb{1}_{A_i=a}}}$$\n其中 $\\mathbb{1}_{predicate}$ 表示 $predicate$ 為真時設定為 $1$，否則為 $0$。\n 當分母趨近於 $0$ 的時候，把 $Q_{t}(a)$ 設定為預設值 (例如 $0$)。 當分母趨近於無限大時，根據大數法則，$Q_t(a)$ 會收斂於 $q_*(a)$。  Greedy    最簡單的方法是選擇 最大的估計價值，如下：\n$$A_t \\doteq \\arg \\max_{a} Q_t (a)$$\n這個方法完全的利用已知的資訊，不採樣任何較差的動作來進行任何探索。\n$\\varepsilon$-greedy    一種替代方案是大部分時間都用貪心法，以機率為 $\\varepsilon$ 採樣其他的動作來進行少量的探索。\nThe 10-armed Testbed    Figure 2.1 為 $k=10$ 的實例，每個都是 $\\mathcal{N}(0,1)$ 的常態分佈。\n$\\varepsilon$-greedy 比 greedy 還要看情況決定\n 如果獎勵的變異數是 10 而非 1，那麼要花費更多的探索才會找到最佳解。 如果獎勵的變異數是 0，那麼 greedy 可能會找到最佳解而不需要再探索。 如果吃角子老虎機是確定性 (deterministic) 但不固定的 (nonstationarity)，例如：會隨著時間改變，那麼 $\\varepsilon$-greedy 還是比較好。  Incremental Implementation    以上都是以 平均獎勵值 來估計 價值。以下討論如何以更有效率的方式計算。\n設 $R_{i}$ 為當前動作中第 $i$ 個選擇接收的獎勵值，$Q_n$ 為這個動作第 $n-1$ 次選擇後的估計價值，可以簡寫如下：\n$$Q_n \\doteq \\dfrac{R_1 + R_2 + \\ldots + R_{n-1}}{n-1}$$\n計算複雜度會隨著次數越來越多次以後變得更多。可以改寫為遞迴式：\n$$ \\begin{aligned} Q_{n+1} \u0026amp;= \\frac{1}{n} \\sum_{i=1}^{n} R_{i}\\newline \u0026amp;= \\frac{1}{n}\\left(R_{n}+\\sum_{i=1}^{n-1} R_{i}\\right)\\newline \u0026amp;=\\frac{1}{n}\\left(R_{n}+(n-1) \\frac{1}{n-1} \\sum_{i=1}^{n-1} R_{i}\\right)\\newline \u0026amp;=\\frac{1}{n}\\left(R_{n}+(n-1) Q_{n}\\right)\\newline \u0026amp;=\\frac{1}{n}\\left(R_{n}+n Q_{n}-Q_{n}\\right)\\newline \u0026amp;=Q_{n}+\\frac{1}{n}\\left[R_{n}-Q_{n}\\right],\\newline \\end{aligned} $$\n通用形式如下：\n$$\\text{NewEstimate} \\leftarrow \\text{OldEstimate} + \\text{StepSize}[\\text{Target} - \\text{OldEstimate}]$$\n其中 $\\text{Target} - \\text{OldEstimate}$ 為 誤差，$\\text{StepSize}$ 為 步長，通常以符號 $\\alpha$ 或 $\\alpha_{t}(a)$ 表示。\n增量法的完整演算法如下：\nTracking a Nonstationary Problem    以上方法適用於固定的 (stationary) 吃角子老虎機問題，也就是獲得的 獎勵值 不會隨著時間改變。為了計算不固定 (nonstationary) 的問題，通常會為每一步給個 權重值，可以經由 $\\text{StepSize}$ $\\alpha$ 給定。\n例如：原增量法的計算式如下：\n$$Q_{n+1} \\doteq Q_{n}+\\alpha\\left[R_{n}-Q_{n}\\right]$$\n其中 $\\alpha \\in (0, 1]$ 為常數。那麼展開後的 價值 為過去 獎勵值 的 加權平均 (weighted average)，結果計算如下 (2.6)：\n$$\\begin{aligned} Q_{n+1} \u0026amp;=Q_{n}+\\alpha\\left[R_{n}-Q_{n}\\right] \\newline \u0026amp;=\\alpha R_{n}+(1-\\alpha) Q_{n} \\newline \u0026amp;=\\alpha R_{n}+(1-\\alpha)\\left[\\alpha R_{n-1}+(1-\\alpha) Q_{n-1}\\right] \\newline \u0026amp;=\\alpha R_{n}+(1-\\alpha) \\alpha R_{n-1}+(1-\\alpha)^{2} Q_{n-1} \\newline \u0026amp;=\\alpha R_{n}+(1-\\alpha) \\alpha R_{n-1}+(1-\\alpha)^{2} \\alpha R_{n-2}+\\newline \u0026amp; \\quad \\cdots+(1-\\alpha)^{n-1} \\alpha R_{1}+(1-\\alpha)^{n} Q_{1} \\newline \u0026amp;=(1-\\alpha)^{n} Q_{1}+\\sum_{i=1}^{n} \\alpha(1-\\alpha)^{n-i} R_{i} \\end{aligned}$$\n稱為加權平均是因為 $(1-\\alpha)^{n}+\\sum_{i=1}^{n} \\alpha(1-\\alpha)^{n-i}=1$。\n $(1 - \\alpha) \u0026lt; 1$: 所以給予過去探索的獎勵值 $R_i$ 權重會越來越小 當 $(1 - \\alpha) \u0026lt; 0$: 稱為 exponential recency-weighted average 當 $(1 - \\alpha) = 0$: 所有權重都會給最近的獎勵值。  可變長度的步長 (step-size)    設 在第 $n$ 個選擇了動作 $a$ 之後的步長 (step-size) 為 $\\alpha_n (a)$\n $\\alpha_n (a) = \\dfrac{1}{n}$: 採樣平均法 (sample-average method)，根據大數法則 (the law of large numbers) 保證收斂到真實的動作價值。 $\\sum_{n=1}^\\infty \\alpha_n (a) = \\infty \\text{ and } \\sum_{n=1}^\\infty \\alpha_n^2 (a) \u0026lt; \\infty$: 根據隨機近似理論 (stochastic approximation theory) 符合條件保證收斂到機率為 $1$。  第一個條件保證步長總和大到可以克服任意初始條件和隨機波動。 第二個條件保證最後步長足夠小到可以收斂。    問題：\n 雖然保證收斂，但是收斂速度緩慢，或是要經過調整才能得到良好的收斂率。 理論上可行，實務上很少使用。  Optimistic Initial Value    以上的方法都依賴於初始的動作價值 (initial action-value) $Q_1(a)$。在統計學裡，這稱為 偏差 (biased by their initial estimates)。\n 在採樣平均法中，偏差會在所有動作至少採樣過一次以後消失。 在所有 $\\alpha$ 為常數的方法，根據 2.6 式，偏差會隨著時間削減但永久存在。  壞處：初始值成為一種使用者設定的參數，除非把它們設定為 $0$。 好處：以一種簡單的方式提供某種先驗的知識，告訴我們預期可以得到什麼程度的獎勵值。    初始動作價值也可以用來 鼓勵探索。假設不把初始值設定為 $0$，在 10-armed testbed 中把所有都設定成 $+5$，而 $q_*(a)$ 在先前設定為平均為 $0$ 變異數為 $1$ 的常態分佈。\n 這種情況下，初始的估計值都是 $+5$ 是樂觀的 (optimistic)，這種樂觀會鼓勵 動作價值法 去探索。 不論選擇哪個動作，獎勵值都會少於估計值，學習器 (learner) 會對獎勵值失望 (disappointed)。 所有動作在嘗試幾次以後收斂。  下圖為 10-armed testbed 比較兩種設定 ($\\alpha = 0.1$)：\n greedy: $Q_1(a) = +5$ $\\varepsilon$-greedy: $Q_1(a) = 0$  它是一個簡單的技巧，但是不適用於不固定問題 (nonstationary problems)。\n 當任務改變時，需要開啟新的探索，這個方法就會無效。  Upper-Confidence-Bound Action Selection    因為不確定 動作價值 (action-value) 估計值的準確度，所以需要 探索。問題是如何選擇可能最好的動作？以下提供一個有效的方式：\n$$A_{t} \\doteq \\underset{a}{\\arg \\max }\\left[Q_{t}(a)+c \\sqrt{\\frac{\\ln t}{N_{t}(a)}}\\right],$$\n $N_t(a)$ 表示動作 $a$ 隨著時間 $t$ 經過後採樣的次數。 $c \u0026gt; 0$ 用來控制探索的程度。 平方根內的值用來測量動作 $a$ 的不確定性 (uncertainty) 或 變異數 (variance)。  下圖為 10-armed testbed 比較兩種設定：\n UCB: $c = 2$ $\\varepsilon$-greedy: $\\varepsilon = 0.1$  解決更通用的 RL 問題的困難：\n 處理不固定問題 (nonstationary problems) 方法更複雜。 處理更大的狀態空間 (state spaces)。  Gradient Bandit Algorithms    先前的作法是估計動作價值之後根據這些估計值來選擇動作。\n這節的作法是學習一個動作的 偏好 (preference)，表示為 $H_t (a) \\in \\mathbb{R}$。\n 偏好跟獎勵值無關。 偏好是相對的。如果對所有偏好都增加 $1000$，根據 soft-max 將不影響計算結果：  $$ \\operatorname{Pr}\\{A_{t}=a \\} \\doteq \\frac{e^{H_{t}(a)}}{\\sum_{b=1}^{k} e^{H_{t}(b)}} \\doteq \\pi_{t}(a) $$\n使用 stochastic gradient ascent 來更新偏好值：\n$$\\begin{aligned} H_{t+1}\\left(A_{t}\\right) \u0026amp; \\doteq H_{t}\\left(A_{t}\\right)+\\alpha\\left(R_{t}-\\bar{R}{t}\\right)\\left(1-\\pi{t}\\left(A_{t}\\right)\\right), \u0026amp; \u0026amp; \\text { and } \\newline H_{t+1}(a) \u0026amp; \\doteq H_{t}(a)-\\alpha\\left(R_{t}-\\bar{R}{t}\\right) \\pi{t}(a), \u0026amp; \u0026amp; \\text { for all } a \\neq A_{t}, \\end{aligned}$$\n $\\alpha \u0026gt; 0$: 步長 (step-size) 參數。 $\\bar{R}_t \\in \\mathbb{R}$: 平均但不包含時間點 $t$ 的獎勵值，通常當作比較基準 (baseline)。  如果獎勵比基準大，未來選擇動作 $A_t$ 的機率會增加；否則就降低。   原本應該要計算的梯度: $\\dfrac{\\partial \\pi_t(x)}{\\partial H_t(a)} = \\pi_t(x)(\\mathbb{1}_{a=x} - \\pi_t(a))$ (證明略)  下圖為 10-armed testbed $\\mathcal{N}(4,1)$ 比較兩種設定：\n 包含 baseline 不含 baseline ($\\bar{R}_t = 0$)  Associative Search (Contextual Bandits)    目前為止考慮的都是非結合式的任務 (nonassociative tasks)，不需要在不同情況下結合不同的動作。更通用的 RL 任務是學習一個策略 (policy)：從一個狀態映射到一個最好的動作選項 (a mapping from situations to the actions that are best in those situations)。\n舉例：\n 假設有幾個不同的 k-armed 吃角子老虎機任務。 每一步都是隨機的。 假設機器是可以辨識的，例如當他改變動作價值 (action values) 時你會知道顏色。 目標是要學習策略來結合每個任務，根據你看到的顏色，選擇當前任務最好的動作。  在文獻中也稱為 contextual bandits。\nSummary     k-armed Bandit Problem  $\\varepsilon$-greedy 隨機性的探索不同動作 greedy with optimistic initialization 利用初始值來當作探索策略 UCB 確定性的選擇，並且計算採樣過的動作的不確定性來取得平衡 Gradient bandit algorithm 估計動作偏好 (action preference) 而非動作價值    下圖為各個方法在控制不同參數過後平均取得的獎勵值：\n"},{"id":8,"href":"/RL-notes/sutton/introduction/","title":"Ch 1. Introduction","parent":"Sutton","content":"    Reinforcement Learning (增強式學習) Elements of Reinforcement Learning  Policy (策略) Reward signal (獎勵訊號) Value function (價值函數) Model (模型)   Limitations and Scope An Extended Example: Tic-Tac-Toe (井字遊戲)  Minimax 動態規劃 演化式方法 使用 RL 並配合價值函數       Reinforcement Learning (增強式學習)    RL 的目標：學習怎麼在給定狀態下，輸出可以得到最大的獎勵 (reward) 的動作 (action)。\nRL 最特別的性質：\n trial-and-error search delayed reward  Reinforcement learning 和其他 -ing 結尾的主題類似 (e.g. machine learning)，同時是個問題也是解決問題的方法。區分問題和解法在 RL 非常重要，搞不清楚時常會造成困惑。\nRL 借用動態系統理論 (dynamical systems theroy) 的馬可夫決策過程 (Markov decision processes) 的觀念來形式化。基本觀念：一個 學習代理 (learning agent) 會隨著 時間 與 環境 (environment) 互動來達成一個 目標 (goal)。學習代理必須可以：\n 感測環境的 狀態 (state) 採取 動作 (action) 來影響環境的狀態  馬可夫決策過程以三個觀點提供最簡化的形式涵蓋這個問題：\n sensation action goal  機器學習三大類：\n supervised learning (監督式學習) unsupervised learning (非監督式學習) 增強式學習  不同之處：\n RL 與 supervised learning 的不同：（略） RL 與 unsupervised learning 的不同：（略）  RL 必須在 exploration (探索) 和 exploitation (利用) 之間取捨：\n 利用：為了要獲得大量的獎勵，RL 代理必須偏好選擇「過去嘗試過最好的」動作 探索：為了做到這件事情，必須探索沒有嘗試過的動作  Elements of Reinforcement Learning    RL 的基本組成：\n the agent (智慧主體，主動進行動作並影響環境狀態的主體) the environment (環境) RL 系統的子元素:  a policy (策略) a reward signal (獎勵訊號) a value function (價值函數) (optional) a model of the environment (環境的模型)    Policy (策略)    策略定義了智慧主體的行為。在給定一個時間點，智慧主體從環境接收狀態，並選擇動作來改變環境。而策略是狀態到動作的映射函數。\n 可能是簡單的函數或是查表，也可能是會涉及複雜計算的演算法 可能是隨機性的，提供採取每個動作的機率  Reward signal (獎勵訊號)    獎勵訊號定義了 RL 問題的目標。在每個時間點，環境會傳送單一數值的訊號給 RL 智慧主體，稱為 獎勵。獎勵給學習代理定義事件的好壞。\n 可能是一個隨機性的函數，根據環境的狀態和採取的動作  Value function (價值函數)    一個狀態的 價值 是智慧主體預期未來會在這個狀態下取得多少獎勵的總和。\n 以最大的價值而非最大的獎勵來採取動作。 獎勵是立即的回饋，價值函數是長期的 獎勵是主要的，價值函數是次要的。  沒有獎勵就沒有價值，估計價值的唯一目的是獲得更多的獎勵。   決定價值比獎勵更難  獎勵通常可以直接由環境取得 價值必須估計、以及來自一個生命週期觀測的結果來重新估計    Model (模型)    模型 用來模擬環境的行為。例如：給定狀態和動作，模型要預測下個狀態和下個獎勵\n模型是用來 planning (規劃)\n model-based model-free  trial-and-error    Limitations and Scope     極度依賴狀態：作為輸入給策略、價值函數和模型，以及來自模型的輸出。  本書探討的問題是假設狀態已經被良好的處理過，可以直接使用。 為了專注在討論決策問題，不考慮如何設計狀態的訊號的問題。   大部分 RL 的方法都圍繞在如何估計價值函數，但這非 RL 的必要條件。不涉及估計價值函數的方法的例子：  基因演算法 (genetic algorithms) 基因規劃 (genetic programming) 模擬退火法 (simulated annealing) 以上是演化式 (evolutionary) 方法，在生命週期中不學習，而是在下個世代產生具備能力的個體。 什麼情況下演化式方法會有優勢：  如果策略空間很小，或是容易被找到、有足夠多的時間搜尋 當無法從環境中感測出完整的狀態     本書專注在環境互動中學習的方法，不包含演化式方法。  An Extended Example: Tic-Tac-Toe (井字遊戲)    井字遊戲是個簡單的問題，但沒有辦法由經典的演算法來適當的解決。以下舉幾個例子來說明：\n 使用 minimax 使用動態規劃 使用演化式方法 使用 RL 並配合價值函數  Minimax     這個方法假設了對手的遊戲策略 讓玩家無法到達一個「可能會輸掉、但實際上對手可能會失誤而因此勝利」的狀態。  動態規劃     需要關於對手的完整規格，包含在每個狀態下會以何種機率採取行動。通常這種資訊是不會先驗的 (a prior) 獲得，大部分實務也不會有。 有一種方式是學習模型來模擬對手的行為，再根據模型來計算動態規劃求出最佳解。最後，這個方法和某些 RL 的方法並無不同。  演化式方法     直接搜尋所有可能的策略，找出一個有高機率獲勝的方法。 此處的策略是一個規則來告訴玩家要下哪一步，對遊戲所有可能狀態 — 每個可能的 O 或 X 的設定 (configuration) 對每一個策略，藉由大量的對戰來估計勝率。 藉由估計值來決定下一次的策略。 可能會用的經典演化式演算法：  爬坡演算法 (hill-climbing algorithm)，會連續的生成和估計策略 基因演算法類    使用 RL 並配合價值函數    步驟如下：\n 設定數值表 (table of numbers)，每一個代表這場遊戲中可能的狀態，每一個數值是當前狀態下勝率的最新估計值。 把這個估計值當作這個狀態的價值，整個表就是學習到的價值函數。 對於所有三個連一直線的狀態的勝率是 1 (或是 0，被對手成功連線時) 。 所有其他狀態的勝率值都初始化為 0.5。 接下來開始對戰很多次。 選擇要下的點，大部分情況下可以使用貪心法挑選最大的價值，也就是最高的勝率。 少部份的情況下選擇其他沒有下過的落點，這稱之為 探索，讓我們可以看過沒見過的狀態。 在遊戲過程中會更新價值函數，使估計的更準確。方式是在每一步過後回補 (back up) 前一步的狀態的價值。 假設當前狀態是 $S_{t}$，經過一個貪心選擇過後下一步是 $S_{t+1}$，這時會更新 $S_{t}$ 的價值函數，標記為 $V(S_{t})$。可以被列式如下：  $V(S_{t}) \\leftarrow V(S_{t}) + \\alpha [ V(S_{t+1}) - V(S_{t})]$  $\\alpha$: 步長參數 (step-size parameter) 這是一個 temporal-difference 學習法的例子         每個實心點代表狀態，實線是狀態轉移的路徑，虛線是可以轉移的路徑 (輪到自己時有多個動作可以選擇) ，* 代表最大價值的狀態。 (Richard S. Sutton and Andrew G. Barto on Reinforcement Learning: An Introduction)   這個方法在這個任務上可以做得很好，因為：\n 步長參數在經過一段時間後適當的減少，會收斂到在給定每個狀態下真實的勝率值。 每一步都是根據對手的落子下的最佳解。  這個例子突顯了 RL 的關鍵特色：\n 強調在與環境互動中學習 目標明確 不只根據當前的狀態，也會考慮後面的發展  "},{"id":9,"href":"/RL-notes/","title":"Reinforcement Learning Notes","parent":"","content":"學習資源     Reinforcement Learning: An Introduction (Richard S. Sutton and Andrew G. Barto)  http://incompleteideas.net/book/the-book.html   OpenAI  https://openai.com/    "},{"id":10,"href":"/RL-notes/categories/","title":"Categories","parent":"Reinforcement Learning Notes","content":""},{"id":11,"href":"/RL-notes/tags/","title":"Tags","parent":"Reinforcement Learning Notes","content":""}]